{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eFWpWFOjah-E",
        "KLiqs1_7cw-z",
        "Hhs2X2m4d5fG",
        "nzpV_03krHAG",
        "G_BsimALuLck",
        "rMxTUbGwtxlF",
        "ov7yeMsLuq6i",
        "XcGPdylHu-4Z",
        "mqKc6XQ7ZMnZ",
        "joGAJoFU47Ha",
        "YGnaOB0rWvgI",
        "tNXOUX_Z8cpz",
        "1Ig8J0pKvE4A",
        "kUJ3e-OavKLS",
        "Go3y5bAXtxll",
        "OFHlO0-6o0U_"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ThlHoGZaXLm"
      },
      "source": [
        "# **Neural Network with Numpy**\n",
        "*(Author: Axel ROCHEL)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNLzYSumeZ33"
      },
      "source": [
        "# **Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFWpWFOjah-E"
      },
      "source": [
        "\n",
        "## **Importations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRKbF9UFaSm3"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLiqs1_7cw-z"
      },
      "source": [
        "## Exceptions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKudLP3ycwPY"
      },
      "source": [
        "class WrongInput(Exception):\n",
        "    \"\"\"Exception raised when the array which feeds the layer is incorrect.\"\"\"\n",
        "    pass\n",
        "\n",
        "class NotImplementedError(Exception):\n",
        "    \"\"\"Exception raised when a not implemented feature is required.\"\"\"\n",
        "    pass\n",
        "\n",
        "class AlreadyBuiltError(Exception):\n",
        "    \"\"\"Exception raised when build function is called a second time.\"\"\"\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E38-nS-1es4g"
      },
      "source": [
        "## **Activation functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w21zMqrZeyOs"
      },
      "source": [
        "class Activation():\n",
        "    \"\"\"Base class for activation functions.\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Linear(Activation):\n",
        "    \"\"\"\n",
        "    Class defining the activation function : x -> x\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Apply the function defined by the class to the input.\n",
        "        \"\"\"\n",
        "        return x\n",
        "    \n",
        "    def derivative(self, x):\n",
        "        \"\"\"\n",
        "        Apply the derivative of the function defined by the class to the input.\n",
        "        \"\"\"\n",
        "        return (x * 0 + 1)[..., np.newaxis] * np.eye(x.shape[1])\n",
        "\n",
        "\n",
        "class Relu(Activation):\n",
        "    \"\"\"\n",
        "    Class defining the activation function : x -> max(x, 0)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Apply the function defined by the class to the input.\n",
        "        \"\"\"\n",
        "        return np.maximum(x, 0)\n",
        "    \n",
        "    def derivative(self, x):\n",
        "        \"\"\"\n",
        "        Apply the derivative of the function defined by the class to the input.\n",
        "        \"\"\"\n",
        "        return ((x > 0) * 1)[..., np.newaxis] * np.eye(x.shape[1]) # Batch of diagonal matrix\n",
        "\n",
        "\n",
        "class Softmax(Activation):\n",
        "    \"\"\"\n",
        "    Class defining the softmax activation function.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Apply the function defined by the class to the input.\n",
        "\n",
        "        Notes:\n",
        "            - The maximum is used to avoid to reach too high numbers with exp while giving same result.\n",
        "            - [:, np.newaxis] convert 1D-array in 2D-array to make operation with 2d-arrays.\n",
        "        \"\"\"\n",
        "        return np.exp(x - np.max(x, axis=1)[:, np.newaxis]) / np.sum(np.exp(x - np.max(x, axis=1)[:, np.newaxis]), axis=1)[:, np.newaxis] \n",
        "    \n",
        "    def derivative(self, x):\n",
        "        \"\"\"\n",
        "        Apply the derivative of the function defined by the class to the input.\n",
        "\n",
        "        Returns:\n",
        "            - matrice M with M[i][k] = self(x)[k] * ( (i==k)*1 - self(x[i]))\n",
        "        \"\"\"\n",
        "        s = self(x)\n",
        "        return (- s[..., np.newaxis] * s[:, np.newaxis, :]) + np.eye(s.shape[1]) * s[..., np.newaxis] # @ instead of * may be better"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOikskf9eO0V"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_fHuCHkeOB-"
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"Base class for layers.\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    \"\"\"\n",
        "    Layer for fully connected networks.\n",
        "\n",
        "    Attributes:\n",
        "        - nb_neurons (int): Number of neurons of the layer.\n",
        "        - activation (Activation): Activation function applied after linear combination (default=Linear).\n",
        "        - name (string): Name given to the layer (default='Dense').\n",
        "    \"\"\"\n",
        "    def __init__(self, nb_neurons, activation=Linear(), name='Dense'):\n",
        "        self.type_name = 'Dense'\n",
        "        self.nb = nb_neurons\n",
        "        self.activation = activation\n",
        "        self.name = name\n",
        "        self.input_dim = 1\n",
        "        self.z = None\n",
        "        self.out = None\n",
        "\n",
        "    def backward(self, dloss_dact, previous_act, weights):\n",
        "        \"\"\"\n",
        "        Compute a gradient step on the layer given its weights, the activation on the previous layer and the loss derivative w.r.t current activation.\n",
        "\n",
        "        Parameters:\n",
        "            - dloss_dact (np.array): loss derivative w.r.t activation of the following layer.\n",
        "            - previous_act (np.array): output of the previous layer.\n",
        "            - weights (np.array): weights of the layer.\n",
        "        \n",
        "        Returns:\n",
        "            - dloss_db (np.array): loss derivative w.r.t biases of the layer.\n",
        "            - dloss_dw (np.array): loss derivative w.r.t weights of the layer.\n",
        "            - dloss_dact (np.array): loss derivative w.r.t activation of the current layer.\n",
        "        \"\"\"\n",
        "        ## Compute gradient ##\n",
        "        # dloss/dw_jk(L) = dloss/da_j(L) * da_j(L)/dz_j(L) * dz_j/dw_jk(L)\n",
        "        # or, (dz_j/dw_jk(L) = a_k(L-1)) so dloss/dw(L) = act(L) * (dloss/da(L) * da(L)/dz(L))\n",
        "        dact_dz = self.activation.derivative(self.z) # shape = (b_s, nb_neurons, nb_neurons)\n",
        "\n",
        "        dloss_db = np.squeeze(dact_dz @ dloss_dact[:,:,None])  # shape = (b_s, nb_neurons, nb_neurons) @ (bs, nb_neurons, 1) = (b_s, nb_neurons)      \n",
        "        dloss_dw = previous_act[:,:,np.newaxis] @ dloss_db[:,np.newaxis,:] # shape = (bs, dim_in, 1) @ (b_s, 1, nb_neurons) = (b_s, dim_in, nb_neurons,)                     \n",
        "\n",
        "        ## Precomputation for previous layer ##\n",
        "        # dloss/da_k(L-1) = SOMME[j=0,n[(dz_j(L)/da_k(L-1) * da_j(L)/dz_j(L) *  dloss/da_j(L))\n",
        "        # i.e. dloss/da(L-1) = dloss/da(L) * da(L)/dz(L) * dz(L)/da(L-1)\n",
        "        dloss_dact = dloss_db @ weights.T # shape = (bs, nb_neurons) @ (dim_in, nb_neurons).T = (bs, dim_in)\n",
        "\n",
        "        return dloss_db, dloss_dw, dloss_dact\n",
        "    \n",
        "    def compute_biases_shape(self):\n",
        "        \"\"\"\n",
        "        Compute the shape of the biases array of the layer.\n",
        "        \"\"\"\n",
        "        return (self.nb,)\n",
        "\n",
        "    def compute_output_shape(self, input_shape=None):\n",
        "        \"\"\"\n",
        "        Compute the shape of the output of the layer.\n",
        "\n",
        "        Parameters:\n",
        "            - input_shape: shape of the features which will feed the layer (useless here, default=None).\n",
        "        \"\"\"\n",
        "        return (self.nb,)\n",
        "    \n",
        "    def compute_weights_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        Compute the shape of the weights array induced by the previous layer.\n",
        "\n",
        "        Parameters:\n",
        "            - input_shape: shape of the features which will feed the layer.\n",
        "        \"\"\"\n",
        "        if len(input_shape) != 1:   \n",
        "            return (np.prod(input_shape), self.nb)\n",
        "        return (*input_shape, self.nb) # inverted because used in a batch\n",
        "    \n",
        "    def forward(self, x, weights, biases):\n",
        "        \"\"\"\n",
        "        Compute the activation of the layer given an input and some weights.\n",
        "\n",
        "        Parameters:\n",
        "            - x (np.array): batch of input vectors.\n",
        "            - weights (np.array): weights used for the forward step.\n",
        "            - biases (np.array): biases used for the forward step.\n",
        "        \n",
        "        Returns:\n",
        "            - out (np.array): the computed output.\n",
        "        \"\"\"\n",
        "        if len(x.shape) != 2:\n",
        "            self.z = np.reshape(x, (x.shape[0], -1)) @ weights + biases\n",
        "        else:\n",
        "            self.z = x @ weights + biases\n",
        "        self.out = self.activation(self.z)\n",
        "        return self.out\n",
        "    \n",
        "    def init_weights(self, input_shape):\n",
        "        \"\"\"\n",
        "        Compute random initial weights for the layer.\n",
        "\n",
        "        Parameters:\n",
        "            - input_shape: shape of the features which will feed the layer.\n",
        "\n",
        "        Returns:\n",
        "            - init_weights (np.array): Array of the initialized weights.\n",
        "            - init_biases (np.array): Array of the initialized biases.\n",
        "        \"\"\"\n",
        "        weights_shape = self.compute_weights_shape(input_shape)\n",
        "        biases_shape = self.compute_biases_shape()\n",
        "\n",
        "        std = 1 / np.sqrt(2 * np.prod(input_shape)) # standard deviation\n",
        "\n",
        "        init_weights = np.random.randn(*weights_shape) * std\n",
        "        init_biases = np.random.randn(*biases_shape) * std\n",
        "\n",
        "        return init_weights, init_biases\n",
        "\n",
        "\n",
        "class Conv2D(Layer):\n",
        "    \"\"\"\n",
        "    Layer for convolution applied to 2-dimensionnal inputs. (Padding not implemented yet) \n",
        "\n",
        "    Attributes:\n",
        "        - nb_features (int): number of features computed by the layers.\n",
        "        - kernel_size (int or tuple): shape of the kernel window with the following format: (height, width), height = width if int given.\n",
        "        - activation (Activation): Activation function applied after linear combination. (default=Linear).\n",
        "        - stride (int or tuple): stride applied during the convolution (default=(1, 1)).\n",
        "        - name (string): Name given to the layer (default='Conv2D').\n",
        "    \"\"\"\n",
        "    def __init__(self, nb_features, kernel_size, activation=Linear(), stride=(1,1), name='Conv2D'):\n",
        "        self.type_name = 'Conv2D'\n",
        "        self.nb = nb_features\n",
        "        self.activation = activation\n",
        "        self.kernel_size = kernel_size\n",
        "        if isinstance(kernel_size, int):\n",
        "            self.kernel_size = (kernel_size, kernel_size)       \n",
        "        self.stride = stride\n",
        "        if isinstance(stride, int):\n",
        "            self.stride = (stride, stride)\n",
        "        self.name = name\n",
        "        self.input_dim = 3\n",
        "        self.indices = None\n",
        "        self.indices_bis = None\n",
        "    \n",
        "    def backward(self, dloss_dact, previous_act, weights):\n",
        "        \"\"\"\n",
        "        Compute a gradient step on the layer given its weights, the activation on the previous layer and the loss derivative w.r.t current activation.\n",
        "\n",
        "        Parameters:\n",
        "            - dloss_dact (np.array): loss derivative w.r.t activation of the following layer.\n",
        "            - previous_act (np.array): output of the previous layer.\n",
        "            - weights (np.array): weight of the layer.\n",
        "        \n",
        "        Returns:\n",
        "            - dloss_db (np.array): loss derivative w.r.t biases of the layer.\n",
        "            - dloss_dw (np.array): loss derivative w.r.t weights of the layer.\n",
        "            - dloss_dact (np.array): loss derivative w.r.t activation of the current layer.\n",
        "        \"\"\"\n",
        "        ## 1st layer case ##\n",
        "        if len(previous_act.shape) != 2:\n",
        "            previous_act = np.reshape(previous_act, (previous_act.shape[0], -1))   \n",
        "        \n",
        "        ## Parameters ##\n",
        "        c_in, h_in, w_in = self.input_shape\n",
        "        nb, _, k0, k1, = self.weight_shape\n",
        "        nb, h_out, w_out = self.output_shape\n",
        "        \n",
        "        ## Weights computation ##   \n",
        "        if self.W is None:\n",
        "            W = np.zeros((w_out*h_out*nb, w_in*h_in*c_in))\n",
        "            W[self.indices, self.indices_bis] += np.reshape(weights, (-1,))[:, np.newaxis]\n",
        "        else:\n",
        "            W = self.W\n",
        "            self.W = None\n",
        "        \n",
        "        ## Compute gradient ##\n",
        "        # dloss/dw_jk(L) = dloss/da_j(L) * da_j(L)/dz_j(L) * dz_j/dw_jk(L)\n",
        "        # or, (dz_j/dw_jk(L) = a_k(L-1)) donc dloss/dw(L) = act(L) * (dloss/da(L) * da(L)/dz(L)) \n",
        "        dact_dz = self.activation.derivative(self.z) # shape = (b_s, nb_neurons, nb_neurons)\n",
        "\n",
        "        dloss_db = np.squeeze(dact_dz @ dloss_dact[:,:,None])  # shape = (b_s, nb_neurons, nb_neurons) @ (bs, nb_neurons, 1) = (b_s, nb_neurons)\n",
        "        dloss_dw = previous_act[:, :, np.newaxis] @ dloss_db[:, np.newaxis,:] # shape = (bs, dim_in, 1) @ (b_s, 1, nb_neurons) = (b_s, dim_in, nb_neurons)                     \n",
        "\n",
        "        ## Precomputation for previous layer ##\n",
        "        # dloss/da_k(L-1) = SOMME[j=0,n[(dz_j(L)/da_k(L-1) * da_j(L)/dz_j(L) *  dloss/da_j(L))\n",
        "        # i.e. dloss/da(L-1) = dloss/da(L) * da(L)/dz(L) * dz(L)/da(L-1)\n",
        "        dloss_dact = dloss_db @ W # shape = (bs, nb_neurons) @ (dim_in, nb_neurons).T = (bs, dim_in)\n",
        "        \n",
        "        ## original shape\n",
        "        bs = self.z.shape[0]\n",
        "        \n",
        "        dloss_db = np.sum(np.reshape(dloss_db, (bs, self.nb, -1)), axis=2)\n",
        "        dloss_dweights = np.sum(dloss_dw[:, self.indices_bis, self.indices], axis=2)\n",
        "        dloss_dweights = np.reshape(dloss_dweights, (bs, nb, c_in, k1, k0))        \n",
        "\n",
        "        return dloss_db, dloss_dweights, dloss_dact\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Set the parameters essential for forward and backward step (to compute convolution with matrix multiplication).\n",
        "        \"\"\"\n",
        "        if len(input_shape) == 2:\n",
        "            input_shape = (1, *input_shape)\n",
        "        if len(input_shape) != 3:\n",
        "            raise WrongInput('the dimension of the input is incorrect. Given : {} ; Expected : {} or {}.'.format(len(input_shape), 2, 3))\n",
        "        h_out = (input_shape[1] - self.kernel_size[0]) // self.stride[0] + 1\n",
        "        w_out = (input_shape[2] - self.kernel_size[1]) // self.stride[1] + 1\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = (self.nb, h_out, w_out)\n",
        "        \n",
        "        c_in, h_in, w_in  = self.input_shape\n",
        "        nb, _, k0, k1, = self.weight_shape\n",
        "        \n",
        "        offset = []\n",
        "        for i in range(h_out):\n",
        "            for j in range(w_out):\n",
        "                offset += [j * self.stride[1] + i * (w_in) * self.stride[0]]\n",
        "        offset = np.array(offset)\n",
        "             \n",
        "        indices = np.zeros((self.nb, w_out * h_out)) # repeat k0 * k1 fois\n",
        "        indices_bis = np.zeros((self.nb * c_in * k0 * k1, w_out * h_out))\n",
        "        for n_filter in range(self.nb):\n",
        "            indices[n_filter] = np.arange(0, w_out * h_out) + n_filter*w_out*h_out\n",
        "            for k_filter in range(c_in):\n",
        "                for i_filter in range(k0):\n",
        "                    for j_filter in range(k1):\n",
        "                        indices_bis[j_filter + k1*i_filter + k0*k1*k_filter + k0*k1*c_in*n_filter] = j_filter + i_filter * w_in + k_filter * h_in * w_in + offset\n",
        "\n",
        "        self.indices = np.repeat(indices, c_in*k0*k1, axis=0).astype(int)\n",
        "        self.indices_bis = indices_bis.astype(int)\n",
        "    \n",
        "    def compute_biases_shape(self):\n",
        "        \"\"\"\n",
        "        Compute the shape of the biases array of the layer.\n",
        "        \"\"\"\n",
        "        return (self.nb,)\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        Compute the shape of the output of the layer and build the layer.\n",
        "\n",
        "        Parameters:\n",
        "            - input_shape: shape of the features which will feed the layer.\n",
        "        \"\"\"\n",
        "        self.build(input_shape)\n",
        "\n",
        "        return self.output_shape\n",
        "    \n",
        "    def compute_weights_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        Compute the shape of the weights array induced by the previous layer.\n",
        "\n",
        "        Parameters:\n",
        "            - input_shape: shape of the features which will feed the layer.\n",
        "        \"\"\"\n",
        "        if len(input_shape) not in (2, 3):\n",
        "            raise WrongInput('the dimension of the input is incorrect. Given : {} ; Expected : {} or {}.'.format(len(input_shape), 2, 3))\n",
        "        if len(input_shape) == 2:\n",
        "            c_in = 1\n",
        "        else:\n",
        "            c_in = input_shape[0]\n",
        "        self.weight_shape = (self.nb, c_in, self.kernel_size[0], self.kernel_size[1])\n",
        "        return self.weight_shape\n",
        "    \n",
        "    def forward(self, x, weights, biases):\n",
        "        \"\"\"\n",
        "        Compute the activation of the layer given an input and some weights.\n",
        "\n",
        "        Parameters:\n",
        "            - x (np.array): batch of input vectors.\n",
        "            - weights (np.array): weights used for the forward step.\n",
        "            - biases (np.array): biases used for the forward step.\n",
        "        \n",
        "        Returns:\n",
        "            - out (np.array): the computed output.\n",
        "        \"\"\"\n",
        "        if weights.shape != self.weight_shape:\n",
        "            raise ValueError(\"Bad weight shape. Given : {} ; Expected : {}.\".format(weights.shape, self.weight_shape))\n",
        "        if biases.shape != (self.nb,):\n",
        "            raise ValueError(\"Bad biases shape. Given : {} ; Expected : {}.\".format(biases.shape, (self.nb,)))\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[:, np.newaxis,...]\n",
        "                             \n",
        "        ## Parameters ##\n",
        "        b_s, c_in, h_in, w_in = x.shape[0], *self.input_shape\n",
        "        nb, c_in, k0, k1,  = self.weight_shape\n",
        "        _, h_out, w_out = self.output_shape\n",
        "        \n",
        "        ## Weights computation ##      \n",
        "        W = np.zeros((w_out*h_out*nb, w_in*h_in*c_in))\n",
        "        W[self.indices, self.indices_bis] += np.reshape(weights, (-1,))[:, None]\n",
        "        self.W = W # for backward\n",
        "        \n",
        "        B = np.repeat(biases, h_out * w_out)\n",
        "        \n",
        "        ## Output computation ##\n",
        "        reshaped_x = np.reshape(x, (x.shape[0], -1)) # (bs, reshape_in)\n",
        "        self.z = reshaped_x @ W.T + B\n",
        "        self.out = self.activation(self.z)\n",
        "        \n",
        "        return np.reshape(self.out, (b_s, nb, h_out, w_out))\n",
        "    \n",
        "    def init_weights(self, input_shape):\n",
        "        \"\"\"\n",
        "        Compute random initial weights for the layer.\n",
        "\n",
        "        Parameters:\n",
        "            - input_shape: shape of the features which will feed the layer.\n",
        "\n",
        "        Returns:\n",
        "            - init_weights (np.array): Array of the initialized weights.\n",
        "            - init_biases (np.array): Array of the initialized biases.\n",
        "        \"\"\"\n",
        "        weights_shape = self.compute_weights_shape(input_shape)\n",
        "        biases_shape = self.compute_biases_shape()\n",
        "\n",
        "        if len(input_shape) == 2:\n",
        "            c_in = 1\n",
        "        else:\n",
        "            c_in = input_shape[0]\n",
        "\n",
        "        std = 1 / np.sqrt(2 * c_in * self.kernel_size[0] * self.kernel_size[1]) # standard deviation\n",
        "\n",
        "        init_weights = np.random.randn(*weights_shape) * std\n",
        "        init_biases = np.random.randn(*biases_shape) * std\n",
        "\n",
        "        return init_weights, init_biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imwn1puH0X4Z"
      },
      "source": [
        "## **Loss functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_zlKeAj0Xma"
      },
      "source": [
        "class Loss():\n",
        "    \"\"\"Base class for loss functions.\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class MSE(Loss):\n",
        "    \"\"\"\n",
        "    Class of the Mean Squared Error.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def __call__(self, pred, target):\n",
        "        \"\"\"\n",
        "        Apply the loss function to the prediction.\n",
        "\n",
        "        Parameters:\n",
        "            - pred (np.array): array corresponding to the output of a neural network.\n",
        "            - target (np.array): array corresponding to the ideal output of the neural network.\n",
        "\n",
        "        Returns:\n",
        "            - losses (np.array with shape=(pred.shape[0],)): array containing the losses of each sample.\n",
        "        \"\"\"\n",
        "        return np.mean(np.square(pred - target), axis=1)\n",
        "    \n",
        "    def compute_accuracy(self, pred, targets):\n",
        "        \"\"\"\n",
        "        Compute the accuracy of the model on given data.\n",
        "\n",
        "        Parameters:\n",
        "            - pred (np.array): array corresponding to the output of a neural network.\n",
        "            - target (np.array): array corresponding to the ideal output of the neural network.\n",
        "         \n",
        "         Returns:\n",
        "            - accuracy (float): ratio between good and bad predictions according to the targets.\n",
        "        \"\"\"\n",
        "        return np.sum(np.argmax(pred, axis=1)==np.argmax(targets, axis=1)) / pred.shape[0]\n",
        " \n",
        "    def derivative(self, pred, target):\n",
        "        \"\"\"\n",
        "        Compute the derivative of the loss w.r.t to the prediction.\n",
        "\n",
        "        Parameters:\n",
        "            - pred (np.array): array corresponding to the output of a neural network.\n",
        "            - target (np.array): array corresponding to the ideal output of the neural network.\n",
        "        \n",
        "        Returns:\n",
        "            - dloss_dpred (np.array with the same shape as pred): array such as dloss_dpred[i][j] is the derivative of the ith prediction of the batch w.r.t the jth output neuron.\n",
        "        \"\"\"\n",
        "        return 2 * (pred - target) / pred.shape[1]\n",
        "    \n",
        "\n",
        "class CategoricalCrossentropy(Loss):\n",
        "    \"\"\"\n",
        "    Class of the crossentropy applied to one-hot vectors.\n",
        "\n",
        "    Attributes:\n",
        "        - epsilon (float): small constant preventing numerical instability (default=1e-7).\n",
        "    \"\"\"\n",
        "    def __init__(self, epsilon=1e-7):\n",
        "        self.epsilon = epsilon\n",
        "    \n",
        "    def __call__(self, pred, target):\n",
        "        \"\"\"\n",
        "        Apply the loss function to the prediction.\n",
        "\n",
        "        Parameters:\n",
        "            - pred (np.array): array corresponding to the output of a neural network.\n",
        "            - target (np.array): array corresponding to the ideal output of the neural network.\n",
        "        \n",
        "        Returns:\n",
        "            - losses (np.array with shape=(pred.shape[0],)): array containing the losses of each sample.\n",
        "        \"\"\"\n",
        "        #return - SOMMEi=1,n(traget[i] * log(pred[i]))\n",
        "        return - np.sum(target * np.log(np.clip(pred, self.epsilon, None)), axis=1)\n",
        "\n",
        "    def compute_accuracy(self, pred, targets):\n",
        "        \"\"\"\n",
        "        Compute the accuracy of the model on given data.\n",
        "\n",
        "        Parameters:\n",
        "            - pred (np.array): array corresponding to the output of a neural network.\n",
        "            - target (np.array): array corresponding to the ideal output of the neural network.\n",
        "         \n",
        "         Returns:\n",
        "            - accuracy (float): ratio between good and bad predictions according to the targets.\n",
        "        \"\"\"\n",
        "        return np.sum(np.argmax(pred, axis=1)==np.argmax(targets, axis=1)) / pred.shape[0]\n",
        "\n",
        "    def derivative(self, pred, target):\n",
        "        \"\"\"\n",
        "        Compute the derivative of the loss w.r.t to the prediction.\n",
        "\n",
        "        Parameters:\n",
        "            - pred (np.array): array corresponding to the output of a neural network.\n",
        "            - target (np.array): array corresponding to the ideal output of the neural network.\n",
        "        \n",
        "        Returns:\n",
        "            - dloss_dpred (np.array with the same shape as pred): array such as dloss_dpred[i][j] is the derivative of the ith prediction of the batch w.r.t the jth output neuron.\n",
        "        \"\"\"\n",
        "        return - target / np.clip(pred, self.epsilon, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5QEbDuVHZZE"
      },
      "source": [
        "## **Optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqBP1h66HXcl"
      },
      "source": [
        "class Optimizer():\n",
        "    \"\"\"Base class for the optimizers.\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SGD(Optimizer):\n",
        "    \"\"\"\n",
        "    Class of the Stochastic gradient descent.\n",
        "\n",
        "    Attributes:\n",
        "        - learning_rate (float): Factor reducing the gradient step (default=1e-2).\n",
        "        - momentum (float): hyperparameter >= 0 that accelerates gradient descent in the relevant direction and dampens oscillations (default=0).\n",
        "        - nesterov (boolean): either nesterov momentum is applied or not.\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False):\n",
        "        assert (momentum >= 0.0), 'momentum should be positive or null'\n",
        "        self.lr = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.delta_w = None # last gradient step for weights\n",
        "        self.delta_b = None # last gradient step for biases\n",
        "        self.nesterov = nesterov\n",
        "\n",
        "    def update_weights(self, weights, biases, dloss_dw, dloss_db):\n",
        "        \"\"\"\n",
        "        Perform a gradient step by computing the new values of parameters.\n",
        "\n",
        "        Parameters:\n",
        "            - weights (array, shape=(batch size, _)): Weights of the model before gradient step.\n",
        "            - biases  (array, shape=(batch size, _)): Biases of the model before gradient step.\n",
        "            - dloss_dw  (array, shape=(batch size, _)): Derivatives of the loss with regard to the different weights of the model.\n",
        "            - dloss_db (array, shape=(batch size, _)): Derivatives of the loss with regard to the different biases of the model.\n",
        "\n",
        "        Returns:\n",
        "            - (new_weight, new biases): parameters values after gradient steps.\n",
        "        \"\"\"\n",
        "        if self.delta_w is None:\n",
        "            self.delta_w = [0 for i in range(len(weights))]\n",
        "            self.delta_b = [0 for i in range(len(weights))]\n",
        "\n",
        "        new_weight = []\n",
        "        new_biases = []\n",
        "        delta_w = []\n",
        "        delta_b = []\n",
        "        for i in range(len(weights)):\n",
        "            delta_w.append(self.momentum * self.delta_w[i] - self.lr * np.mean(dloss_dw[i], axis=0))\n",
        "            delta_b.append(self.momentum * self.delta_b[i] - self.lr * np.mean(dloss_db[i], axis=0))\n",
        "            if self.nesterov:\n",
        "                new_weight.append(weights[i] + self.momentum * delta_w[i] - self.lr * np.mean(dloss_dw[i], axis=0))\n",
        "                new_biases.append(biases[i] + self.momentum * delta_b[i] - self.lr * np.mean(dloss_db[i], axis=0))\n",
        "            else:\n",
        "                new_weight.append(weights[i] + delta_w[i])\n",
        "                new_biases.append(biases[i] + delta_b[i])\n",
        "        self.delta_w, self.delta_b = delta_w, delta_b\n",
        "        return new_weight, new_biases\n",
        "\n",
        "\n",
        "class Adam(Optimizer):\n",
        "    \"\"\"\n",
        "    Class of the Adaptative Moment estimation.\n",
        "\n",
        "    Attributes:\n",
        "        - learning_rate (float): Factor reducing the gradient step (default=1e-3).\n",
        "        - beta_1 (float): exponential decay rate for the 1st moment estimates (default=0.9).\n",
        "        - beta_2 (float): exponential decay rate for the 2nd moment estimates (default=0.999).\n",
        "        - epsilon (float): small constant preventing numerical instability (default=1e-7).\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-7):\n",
        "        self.lr = learning_rate\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "        self.eps = epsilon\n",
        "        self.ms_w = None\n",
        "        self.ms_b = None\n",
        "        self.vs_w = None\n",
        "        self.vs_b = None\n",
        "        self.nb_it = 1\n",
        "    \n",
        "    def update_weights(self, weights, biases, dloss_dw, dloss_db):\n",
        "        \"\"\"\n",
        "        Perform a gradient step by computing the new values of parameters.\n",
        "\n",
        "        Parameters:\n",
        "            - weights (array, shape=(batch size, _)): Weights of the model before gradient step.\n",
        "            - biases  (array, shape=(batch size, _)): Biases of the model before gradient step.\n",
        "            - dloss_dw  (array, shape=(batch size, _)): Derivatives of the loss with regard to the different weights of the model.\n",
        "            - dloss_db (array, shape=(batch size, _)): Derivatives of the loss with regard to the different biases of the model.\n",
        "\n",
        "        Returns:\n",
        "            - (new_weight, new biases): parameters values after gradient steps.\n",
        "        \"\"\"\n",
        "        if self.ms_w is None:\n",
        "            self.ms_w = [0 for i in range(len(weights))]\n",
        "            self.ms_b = [0 for i in range(len(weights))]\n",
        "            self.vs_w = [0 for i in range(len(weights))]\n",
        "            self.vs_b = [0 for i in range(len(weights))]\n",
        "\n",
        "        new_weight = []\n",
        "        new_biases = []\n",
        "        ms_w = []\n",
        "        ms_b = []\n",
        "        vs_w = []\n",
        "        vs_b = []\n",
        "        for i in range(len(weights)):\n",
        "            grad_w = np.mean(dloss_dw[i], axis=0)\n",
        "            grad_b = np.mean(dloss_db[i], axis=0)\n",
        "\n",
        "            ms_w.append(self.beta_1 * self.ms_w[i] + (1 - self.beta_1) * grad_w)\n",
        "            ms_b.append(self.beta_1 * self.ms_b[i] + (1 - self.beta_1) * grad_b)\n",
        "\n",
        "            vs_w.append(self.beta_2 * self.vs_w[i] + (1 - self.beta_2) * np.square(grad_w))\n",
        "            vs_b.append(self.beta_2 * self.vs_b[i] + (1 - self.beta_2) * np.square(grad_b))\n",
        "\n",
        "            new_weight.append(weights[i] - (self.lr / (1 - self.beta_1 ** self.nb_it) * ms_w[i] / (self.eps + np.sqrt(vs_w[i] / (1 - self.beta_2 ** self.nb_it)))))\n",
        "            new_biases.append(biases[i] - (self.lr / (1 - self.beta_1 ** self.nb_it) * ms_b[i] / (self.eps + np.sqrt(vs_b[i] / (1 - self.beta_2 ** self.nb_it)))))\n",
        "\n",
        "        self.ms_w, self.ms_b, self.vs_w, self.vs_b = ms_w, ms_b, vs_w, vs_b\n",
        "        self.nb_it += 1\n",
        "        return new_weight, new_biases\n",
        "\n",
        "\n",
        "class RMSProp(Optimizer):\n",
        "    \"\"\"\n",
        "    Class of the Root Mean Squared Propagation.\n",
        "\n",
        "    Attributes:\n",
        "        - learning_rate (float): Factor reducing the gradient step (default=1e-3).\n",
        "        - momentum (float): hyperparameter >= 0 that accelerates gradient descent in the relevant direction and dampens oscillations (default=0).\n",
        "        - rho (float >= 0): Discounting factor for the history/coming gradient (default=0.9).\n",
        "        - epsilon (float): small constant preventing numerical instability (default=1e-7).\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=1e-3, rho=0.9, momentum=0.0, epsilon=1e-7):\n",
        "        assert (momentum >= 0.0), 'momentum should be positive or null'\n",
        "        self.lr = learning_rate\n",
        "        self.rho = rho\n",
        "        self.momentum = momentum\n",
        "        self.eps = epsilon\n",
        "        self.sqr_w = None\n",
        "        self.sqr_b = None\n",
        "    \n",
        "    def update_weights(self, weights, biases, dloss_dw, dloss_db):\n",
        "        \"\"\"\n",
        "        Perform a gradient step by computing the new values of parameters.\n",
        "\n",
        "        Parameters:\n",
        "            - weights (array, shape=(batch size, _)): Weights of the model before gradient step.\n",
        "            - biases  (array, shape=(batch size, _)): Biases of the model before gradient step.\n",
        "            - dloss_dw  (array, shape=(batch size, _)): Derivatives of the loss with regard to the different weights of the model.\n",
        "            - dloss_db (array, shape=(batch size, _)): Derivatives of the loss with regard to the different biases of the model.\n",
        "\n",
        "        Returns:\n",
        "            - (new_weight, new biases): parameters values after gradient steps.\n",
        "        \"\"\"\n",
        "        if self.sqr_w is None:\n",
        "            self.sqr_w = [0 for i in range(len(weights))]\n",
        "            self.sqr_b = [0 for i in range(len(weights))]\n",
        "\n",
        "        new_weight = []\n",
        "        new_biases = []\n",
        "        sqr_w = []\n",
        "        sqr_b = []\n",
        "        for i in range(len(weights)):\n",
        "            grad_w = np.mean(dloss_dw[i], axis=0)\n",
        "            grad_b = np.mean(dloss_db[i], axis=0)\n",
        "\n",
        "            sqr_w.append(self.rho * self.sqr_w[i] + (1 - self.rho) * np.square(grad_w))\n",
        "            sqr_b.append(self.rho * self.sqr_b[i] + (1 - self.rho) * np.square(grad_b))\n",
        "\n",
        "            new_weight.append(weights[i] - self.lr * grad_w / (np.sqrt(sqr_w[i]) + self.eps))\n",
        "            new_biases.append(biases[i] - self.lr * grad_b / (np.sqrt(sqr_b[i]) + self.eps))\n",
        "\n",
        "        self.sqr_w, self.sqr_b = sqr_w, sqr_b\n",
        "        return new_weight, new_biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcKkTNJwalR3"
      },
      "source": [
        "## **Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVfpROnxagLO"
      },
      "source": [
        "class Model():\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.built = False\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "    \n",
        "    def add(self, layer):\n",
        "        \"\"\"\n",
        "        Add a new layer to the graph model.\n",
        "\n",
        "        Parameters:\n",
        "            - layer (Layer): the layer added.\n",
        "        \"\"\"\n",
        "        self.layers.append(layer)\n",
        "    \n",
        "    def build(self, input_dim, optimizer, loss):\n",
        "        \"\"\"\n",
        "        Initialize weights and trainings parameters.\n",
        "\n",
        "        Parameters:\n",
        "            - input_dim (tuple): shape of each sample which will feed the model.\n",
        "            - optimizer (Optimizer): Optimizer which will be used during training.\n",
        "            - loss (Loss): function which will be decreased during training.\n",
        "        \n",
        "        Raises:\n",
        "            - AlreadyBuiltError: Model already built.\n",
        "        \"\"\"\n",
        "        if self.built:\n",
        "            raise AlreadyBuiltError(\"Model already built.\")\n",
        "        self.input_dim = input_dim\n",
        "        for i in range(len(self.layers)):\n",
        "            init_weights, init_biases = self.layers[i].init_weights(input_dim)\n",
        "            self.weights.append(init_weights)\n",
        "            self.biases.append(init_biases)\n",
        "            input_dim = self.layers[i].compute_output_shape(input_dim)\n",
        "        self.loss = loss\n",
        "        self.opt = optimizer\n",
        "        self.dloss_dw = [None for i in range(len(self.layers))]\n",
        "        self.dloss_db = [None for i in range(len(self.layers))]\n",
        "        self.built = True\n",
        "\n",
        "    def evaluate(self, data, target):\n",
        "        \"\"\"\n",
        "        Compute loss and accuracy of the model on given data.\n",
        "\n",
        "        Parameters:\n",
        "            - data (np.array): array containing inputs whose shape are equals to self.input_dim.\n",
        "            - target (np.array): array corresponding to the ideal output of the neural network given the input data.\n",
        "        \n",
        "        Returns:\n",
        "            - loss, accuracy.\n",
        "        \"\"\"\n",
        "        return np.mean(self.loss(self(data), target)), self.loss.compute_accuracy(self(data), target)\n",
        "    \n",
        "    def fit(self, data, targets, epochs=1, validation_split=None, batch_size=32, shuffle=True, verbose='training'):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "\n",
        "        Parameters:\n",
        "            - data (np.array): Input of the neural network.\n",
        "            - targets: Output that should be returned by the model according to the input data.\n",
        "            - epochs (int): number of epochs. (default=1)\n",
        "            - validation_split (float or None): ratio between the number of validation data and training data. None is equivalent to 0.0 (default=None)\n",
        "            - batch_size: number of data evaluated between each gradient step. (default=32)\n",
        "            - shuffle (boolean): if True, data are shuffle at each epoch. (default=True)\n",
        "            - verbose: Define the information displayed in the terminal (default: 'training' ; use 'debug to see all \n",
        " intermediate results).\n",
        "\n",
        "        Return:\n",
        "            - history (dict): recorded loss and accuracy of the training.\n",
        "        \"\"\"\n",
        "        if validation_split==None:\n",
        "            data_train, targets_train = data, targets\n",
        "            hist = {'acc':[], 'loss':[]}\n",
        "        else:\n",
        "            data_train, data_valid, targets_train, targets_valid = train_test_split(data, targets, test_size=validation_split, random_state=31, stratify=targets)\n",
        "            hist = {'acc':[], 'loss':[], 'val_acc':[], 'val_loss':[]}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print('Epoch {} / {}'.format(epoch+1, epochs))\n",
        "\n",
        "            batch_index = np.arange(0, len(data_train))\n",
        "            if shuffle:\n",
        "                np.random.shuffle(batch_index) # the last batch is passed even if it is incomplete\n",
        "            current_index = 0\n",
        "            batch_total = (len(data_train)-1) // batch_size + 1\n",
        "            current_batch_nb = 0\n",
        "            while current_index < len(data_train):\n",
        "                if current_index+batch_size > len(data_train):\n",
        "                    indexes = batch_index[current_index:len(data_train)]\n",
        "                else:\n",
        "                    indexes = batch_index[current_index:current_index+batch_size]\n",
        "\n",
        "                data_batch = data_train[indexes]\n",
        "                targets_batch = targets_train[indexes]\n",
        "            \n",
        "                pred_batch = self(data_batch)\n",
        "                loss = np.mean(self.loss(pred_batch, targets_batch))\n",
        "                current_batch_nb += 1                              \n",
        "                                    \n",
        "                print(\"\\r{}/{} [\".format(current_batch_nb, batch_total) + (20*current_batch_nb//batch_total-1) * \"=\" + \">\" + (20-20*current_batch_nb//batch_total) * \".\" + \"]\", end=\"\")\n",
        "                print(\" loss: \", round(loss, 3), end=\"\")\n",
        "\n",
        "                ## Backpropagation ##\n",
        "                dloss_dact = self.loss.derivative(pred_batch, targets_batch) #  shape = (bs, nb_neurons)\n",
        "                for i in range(len(self.layers) - 1, -1, -1):\n",
        "                    previous_act = self.layers[i-1].out\n",
        "                    if i == 0:\n",
        "                        previous_act = data_batch\n",
        "                    self.dloss_db[i], self.dloss_dw[i], dloss_dact = self.layers[i].backward(dloss_dact, previous_act, self.weights[i])\n",
        "                self.weights, self.biases = self.opt.update_weights(self.weights, self.biases, self.dloss_dw, self.dloss_db)\n",
        "                current_index += batch_size\n",
        "\n",
        "                ## Debug\n",
        "                if verbose == 'debug':\n",
        "                    print('\\ndloss_dw\\n', self.dloss_dw)\n",
        "                    print('\\ndloss_db\\n', self.dloss_db)\n",
        "\n",
        "            ## Evaluate ##\n",
        "            print(\"\\r{}/{} [\".format(current_batch_nb, batch_total) + (20*current_batch_nb//batch_total-1) * \"=\" + \">\" + (20-20*current_batch_nb//batch_total) * \".\" + \"]\", end=\"\")\n",
        "\n",
        "            loss, acc = self.evaluate(data_train, targets_train)\n",
        "            hist['loss'].append(loss)\n",
        "            hist['acc'].append(acc)\n",
        "            if validation_split==None:\n",
        "                print(\" loss: {} ; acc: {}\".format(loss, acc))\n",
        "            else:\n",
        "                val_loss, val_acc = self.evaluate(data_valid, targets_valid)       \n",
        "                print(\" loss: {} ; acc: {} ; val_loss: {} ; val_acc: {}\".format(loss, acc, val_loss, val_acc))\n",
        "                hist['val_loss'].append(val_loss)\n",
        "                hist['val_acc'].append(val_acc)\n",
        "\n",
        "        return hist\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply a forward step on the given input.\n",
        "\n",
        "        Parameters:\n",
        "            - x (np.array): array containing inputs which shape are equals to self.input_dim.\n",
        "        \n",
        "        Return:\n",
        "            - out (np.array): The output of the forward pass.\n",
        "        \n",
        "        Raises:\n",
        "            - WrongInput: The dimension of the inputs aren't equal to the input dimension of the model.\n",
        "        \"\"\"\n",
        "        if x.shape != (x.shape[0], *self.input_dim):\n",
        "            raise WrongInput(\"The dimension of the inputs aren't equal to the input dimension of the model. (Given: {}, Expected:{})\".format(x[0].shape, self.input_dim))\n",
        "        \n",
        "        out = x\n",
        "        for i in range(len(self.layers)):\n",
        "            out = self.layers[i].forward(out, self.weights[i], self.biases[i])\n",
        "        return out\n",
        "    \n",
        "    def summary(self):\n",
        "        \"\"\"\n",
        "        Prints a summary of the network.\n",
        "        \"\"\"\n",
        "        if self.built:\n",
        "            print(\"layer | input shape | output shape | nb_param |\")\n",
        "            input_dim = self.input_dim\n",
        "            for i in range(len(self.layers)):\n",
        "                print('-' * 47)\n",
        "                print(self.layers[i].type_name, end=(6-len(self.layers[i].type_name))*\" \" + \"| \")\n",
        "                \n",
        "                nb_space_in = 12 - len(str(input_dim))\n",
        "                print(input_dim, end=nb_space_in*\" \" + \"| \")\n",
        "                \n",
        "                output_dim = self.layers[i].compute_output_shape(input_dim)\n",
        "                nb_space_out = 13 - len(str(output_dim))\n",
        "                print(output_dim, end=nb_space_out*\" \" + \"| \")\n",
        "                \n",
        "                nb_param = np.prod(self.weights[i].shape) + np.prod(self.biases[i].shape)\n",
        "                print(nb_param, end= (8-int(np.log10(nb_param))) * \" \" + \"|\\n\")\n",
        "                input_dim = output_dim\n",
        "        \n",
        "        else:\n",
        "            print('MODEL UNBUILT')\n",
        "            print(\"layer    |\")\n",
        "            for i in range(len(self.layers)):\n",
        "                print('-' * 10)\n",
        "                nb_char = len(self.layers[i].type_name)\n",
        "                print(self.layers[i].type_name, end=(9-nb_char)*\" \" + \"|\\n\")\n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhs2X2m4d5fG"
      },
      "source": [
        "# **Tests**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzpV_03krHAG"
      },
      "source": [
        "## Unit Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_BsimALuLck"
      },
      "source": [
        "### Fully connected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFF3A_CNntB1"
      },
      "source": [
        "Init & forward tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpiABiu4f4C7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c49e92-1d81-4d4b-c78a-bef8ea9b98f7"
      },
      "source": [
        "inputs = np.array([[1, -1, 1],\n",
        "                  [2, 1, 4],\n",
        "                  [-1, 2, 1],\n",
        "                  [2, 2, 1],\n",
        "                  [4, 1, -4]])\n",
        "\n",
        "target = np.array([[0, 0, 1, 0],\n",
        "                   [0, 1, 0, 0],\n",
        "                   [1, 0, 0, 0],\n",
        "                   [1, 0, 0, 0],\n",
        "                   [0, 0, 1, 0]])\n",
        "\n",
        "print('#### BUILDING TESTS ####')\n",
        "model = Model()\n",
        "model.add(Dense(2, activation=Relu()))\n",
        "model.add(Dense(4, activation=Softmax()))\n",
        "\n",
        "model.summary()\n",
        "model.build(inputs[0].shape, SGD(), MSE())\n",
        "model.summary()\n",
        "\n",
        "print('weights :\\n', model.weights)\n",
        "print('biases :\\n', model.biases)\n",
        "\n",
        "\n",
        "\n",
        "print('\\n\\n#### FORWARD TESTS ####')\n",
        "model.weights = [np.array([[0, 1],\n",
        "                          [2, 1],\n",
        "                          [-1, 0.5]]),\n",
        "                np.array([[1, -1, 2, 1],\n",
        "                          [0, -2, -1, 0.5]])]\n",
        "\n",
        "model.biases = [np.array([1, 0]), np.array([0.5, -1, 0, 2])]\n",
        "print('weights :\\n', model.weights)\n",
        "print('biases :\\n', model.biases)\n",
        "\n",
        "output = model(inputs)\n",
        "\n",
        "\n",
        "for i in range(len(model.layers)):\n",
        "    print('\\nLayer', i+1)\n",
        "    print('z:', model.layers[i].z)\n",
        "    print('activation:', model.layers[i].out)\n",
        "\n",
        "print('\\noutput :\\n', output)\n",
        "print('\\n loss:', model.loss(output, target))\n",
        "print('\\n eval:', model.evaluate(inputs, target))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n### BACKWARD TESTS ###\\n\")\n",
        "\n",
        "model.fit(inputs, target, epochs=1, verbose='debug')\n",
        "\n",
        "print(model.biases)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#### BUILDING TESTS ####\n",
            "MODEL UNBUILT\n",
            "layer    |\n",
            "----------\n",
            "Dense    |\n",
            "----------\n",
            "Dense    |\n",
            "\n",
            "\n",
            "layer | input shape | output shape | nb_param |\n",
            "-----------------------------------------------\n",
            "Dense | (3,)        | (2,)         | 8        |\n",
            "-----------------------------------------------\n",
            "Dense | (2,)        | (4,)         | 12       |\n",
            "\n",
            "\n",
            "weights :\n",
            " [array([[-0.24594327, -0.70327267],\n",
            "       [ 0.25816348, -0.3770528 ],\n",
            "       [ 0.19506191, -0.16303806]]), array([[ 0.1791997 ,  1.53668955, -0.32253881, -0.69005981],\n",
            "       [-0.70885682, -0.39474291, -0.49277723,  0.42043044]])]\n",
            "biases :\n",
            " [array([ 0.59425682, -0.01436838]), array([0.00626433, 0.63498125, 0.19963605, 0.29689209])]\n",
            "\n",
            "\n",
            "#### FORWARD TESTS ####\n",
            "weights :\n",
            " [array([[ 0. ,  1. ],\n",
            "       [ 2. ,  1. ],\n",
            "       [-1. ,  0.5]]), array([[ 1. , -1. ,  2. ,  1. ],\n",
            "       [ 0. , -2. , -1. ,  0.5]])]\n",
            "biases :\n",
            " [array([1, 0]), array([ 0.5, -1. ,  0. ,  2. ])]\n",
            "\n",
            "Layer 1\n",
            "z: [[-2.   0.5]\n",
            " [-1.   5. ]\n",
            " [ 4.   1.5]\n",
            " [ 4.   4.5]\n",
            " [ 7.   3. ]]\n",
            "activation: [[0.  0.5]\n",
            " [0.  5. ]\n",
            " [4.  1.5]\n",
            " [4.  4.5]\n",
            " [7.  3. ]]\n",
            "\n",
            "Layer 2\n",
            "z: [[  0.5   -2.    -0.5    2.25]\n",
            " [  0.5  -11.    -5.     4.5 ]\n",
            " [  4.5   -8.     6.5    6.75]\n",
            " [  4.5  -14.     3.5    8.25]\n",
            " [  7.5  -14.    11.    10.5 ]]\n",
            "activation: [[1.38800845e-01 1.13934671e-02 5.10619771e-02 7.98743711e-01]\n",
            " [1.79848847e-02 1.82188565e-07 7.35001131e-05 9.81941433e-01]\n",
            " [5.59384366e-02 2.08463152e-07 4.13332246e-01 5.30729109e-01]\n",
            " [2.27847725e-02 2.10473189e-10 8.38204936e-03 9.68833178e-01]\n",
            " [1.84498479e-02 8.48518721e-12 6.10975051e-01 3.70575101e-01]]\n",
            "\n",
            "output :\n",
            " [[1.38800845e-01 1.13934671e-02 5.10619771e-02 7.98743711e-01]\n",
            " [1.79848847e-02 1.82188565e-07 7.35001131e-05 9.81941433e-01]\n",
            " [5.59384366e-02 2.08463152e-07 4.13332246e-01 5.30729109e-01]\n",
            " [2.27847725e-02 2.10473189e-10 8.38204936e-03 9.68833178e-01]\n",
            " [1.84498479e-02 8.48518721e-12 6.10975051e-01 3.70575101e-01]]\n",
            "\n",
            " loss: [0.38946759 0.49113302 0.33594229 0.4734144  0.07225168]\n",
            "\n",
            " eval: (0.3524417957911345, 0.2)\n",
            "\n",
            "\n",
            "\n",
            "### BACKWARD TESTS ###\n",
            "\n",
            "Epoch 1 / 1\n",
            "\r1/1 [===================>] loss:  0.352\n",
            "dloss_dw\n",
            " [array([[[-0.35315178,  0.52754196],\n",
            "        [-0.08828795,  0.13188549],\n",
            "        [ 0.35315178, -0.52754196]],\n",
            "\n",
            "       [[ 0.        ,  0.08448467],\n",
            "        [ 0.        , -0.08448467],\n",
            "        [ 0.        ,  0.08448467]],\n",
            "\n",
            "       [[ 0.        ,  0.008619  ],\n",
            "        [ 0.        ,  0.0043095 ],\n",
            "        [ 0.        ,  0.017238  ]],\n",
            "\n",
            "       [[-0.00761141,  0.03299039],\n",
            "        [-0.00761141,  0.03299039],\n",
            "        [-0.0038057 ,  0.01649519]],\n",
            "\n",
            "       [[-0.00281584, -0.01456856],\n",
            "        [ 0.00563167,  0.02913713],\n",
            "        [ 0.00281584,  0.01456856]]]), array([[[ 7.65001343e-03,  2.97035709e-12, -6.18015620e-01,\n",
            "          6.10365606e-01],\n",
            "        [ 3.27857718e-03,  1.27301018e-12, -2.64863837e-01,\n",
            "          2.61585260e-01]],\n",
            "\n",
            "       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
            "          0.00000000e+00],\n",
            "        [-1.63136625e-02, -1.70200991e-03, -1.98869853e-02,\n",
            "          3.79026577e-02]],\n",
            "\n",
            "       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
            "          0.00000000e+00],\n",
            "        [-4.25588634e-02, -8.94788201e-07, -1.77219569e-04,\n",
            "          4.27369777e-02]],\n",
            "\n",
            "       [[-8.62931144e-02, -3.85773091e-10, -1.52228127e-02,\n",
            "          1.01515927e-01],\n",
            "        [-9.70797537e-02, -4.33994727e-10, -1.71256643e-02,\n",
            "          1.14205418e-01]],\n",
            "\n",
            "       [[-1.50336693e-01, -1.66648527e-07,  1.12630069e-02,\n",
            "          1.39073853e-01],\n",
            "        [-5.63762598e-02, -6.24931978e-08,  4.22362758e-03,\n",
            "          5.21526947e-02]]])]\n",
            "\n",
            "dloss_db\n",
            " [array([[-0.08828795,  0.13188549],\n",
            "       [ 0.        ,  0.08448467],\n",
            "       [ 0.        ,  0.0043095 ],\n",
            "       [-0.0038057 ,  0.01649519],\n",
            "       [ 0.00281584,  0.01456856]]), array([[ 1.09285906e-03,  4.24336728e-13, -8.82879457e-02,\n",
            "         8.71950866e-02],\n",
            "       [-3.26273250e-02, -3.40401981e-03, -3.97739707e-02,\n",
            "         7.58053155e-02],\n",
            "       [-8.51177268e-03, -1.78957640e-07, -3.54439139e-05,\n",
            "         8.54739555e-03],\n",
            "       [-2.15732786e-02, -9.64432727e-11, -3.80570317e-03,\n",
            "         2.53789819e-02],\n",
            "       [-3.75841732e-02, -4.16621319e-08,  2.81575172e-03,\n",
            "         3.47684631e-02]])]\n",
            "\r1/1 [===================>] loss: 0.3514382749071635 ; acc: 0.2\n",
            "[array([ 1.00017856e+00, -5.03486828e-04]), array([ 5.00198407e-01, -9.99993192e-01,  2.58174623e-04,  1.99953661e+00])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5-Eak41rsgd"
      },
      "source": [
        "Test training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwyVp8bPruhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14be18e-17b7-425f-d8b3-a80f843efbae"
      },
      "source": [
        "input = np.array([[1, -1, 1],\n",
        "                  [2, 1, 4],\n",
        "                  [-1, 2, 1],\n",
        "                  [2, 2, 1],\n",
        "                  [4, 1, -4]])\n",
        "\n",
        "target = np.array([[0, 0, 1, 0],\n",
        "                   [0, 1, 0, 0],\n",
        "                   [1, 0, 0, 0],\n",
        "                   [1, 0, 0, 0],\n",
        "                   [0, 0, 1, 0]])\n",
        "\n",
        "model = Model()\n",
        "model.add(Dense(2, activation=Relu()))\n",
        "model.add(Dense(4, activation=Softmax()))\n",
        "\n",
        "model.build(input[0].shape, SGD(), MSE())\n",
        "\n",
        "\n",
        "model.weights = [np.array([[0, 1],\n",
        "                          [2, 1],\n",
        "                          [-1, 0.5]]),\n",
        "                np.array([[1, -1, 2, 1],\n",
        "                          [0, -2, -1, 0.5]])]\n",
        "\n",
        "model.biases = [np.array([1, 0]), np.array([0.5, -1, 0, 2])]\n",
        "\n",
        "hist = model.fit(input, target, epochs=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 1000\n",
            "\r1/1 [===================>] loss:  0.352\r1/1 [===================>] loss: 0.3514382749071635 ; acc: 0.2\n",
            "Epoch 2 / 1000\n",
            "\r1/1 [===================>] loss:  0.351\r1/1 [===================>] loss: 0.35049454076503417 ; acc: 0.2\n",
            "Epoch 3 / 1000\n",
            "\r1/1 [===================>] loss:  0.35\r1/1 [===================>] loss: 0.3496061809779165 ; acc: 0.2\n",
            "Epoch 4 / 1000\n",
            "\r1/1 [===================>] loss:  0.35\r1/1 [===================>] loss: 0.3487689873483114 ; acc: 0.2\n",
            "Epoch 5 / 1000\n",
            "\r1/1 [===================>] loss:  0.349\r1/1 [===================>] loss: 0.34797898293637464 ; acc: 0.2\n",
            "Epoch 6 / 1000\n",
            "\r1/1 [===================>] loss:  0.348\r1/1 [===================>] loss: 0.34723243673394905 ; acc: 0.2\n",
            "Epoch 7 / 1000\n",
            "\r1/1 [===================>] loss:  0.347\r1/1 [===================>] loss: 0.3465258688657562 ; acc: 0.2\n",
            "Epoch 8 / 1000\n",
            "\r1/1 [===================>] loss:  0.347\r1/1 [===================>] loss: 0.34585604877061066 ; acc: 0.2\n",
            "Epoch 9 / 1000\n",
            "\r1/1 [===================>] loss:  0.346\r1/1 [===================>] loss: 0.34521998836364043 ; acc: 0.2\n",
            "Epoch 10 / 1000\n",
            "\r1/1 [===================>] loss:  0.345\r1/1 [===================>] loss: 0.34461493177165553 ; acc: 0.2\n",
            "Epoch 11 / 1000\n",
            "\r1/1 [===================>] loss:  0.345\r1/1 [===================>] loss: 0.34403834287992574 ; acc: 0.2\n",
            "Epoch 12 / 1000\n",
            "\r1/1 [===================>] loss:  0.344\r1/1 [===================>] loss: 0.3434878916323284 ; acc: 0.2\n",
            "Epoch 13 / 1000\n",
            "\r1/1 [===================>] loss:  0.343\r1/1 [===================>] loss: 0.34296143978515925 ; acc: 0.2\n",
            "Epoch 14 / 1000\n",
            "\r1/1 [===================>] loss:  0.343\r1/1 [===================>] loss: 0.342457026622076 ; acc: 0.2\n",
            "Epoch 15 / 1000\n",
            "\r1/1 [===================>] loss:  0.342\r1/1 [===================>] loss: 0.3419728549866875 ; acc: 0.2\n",
            "Epoch 16 / 1000\n",
            "\r1/1 [===================>] loss:  0.342\r1/1 [===================>] loss: 0.34150727787319557 ; acc: 0.2\n",
            "Epoch 17 / 1000\n",
            "\r1/1 [===================>] loss:  0.342\r1/1 [===================>] loss: 0.34105878572769227 ; acc: 0.2\n",
            "Epoch 18 / 1000\n",
            "\r1/1 [===================>] loss:  0.341\r1/1 [===================>] loss: 0.3406259945474716 ; acc: 0.2\n",
            "Epoch 19 / 1000\n",
            "\r1/1 [===================>] loss:  0.341\r1/1 [===================>] loss: 0.3402076348181654 ; acc: 0.2\n",
            "Epoch 20 / 1000\n",
            "\r1/1 [===================>] loss:  0.34\r1/1 [===================>] loss: 0.33980254129463316 ; acc: 0.2\n",
            "Epoch 21 / 1000\n",
            "\r1/1 [===================>] loss:  0.34\r1/1 [===================>] loss: 0.33940964360807657 ; acc: 0.2\n",
            "Epoch 22 / 1000\n",
            "\r1/1 [===================>] loss:  0.339\r1/1 [===================>] loss: 0.3390279576662361 ; acc: 0.2\n",
            "Epoch 23 / 1000\n",
            "\r1/1 [===================>] loss:  0.339\r1/1 [===================>] loss: 0.3386565778037346 ; acc: 0.2\n",
            "Epoch 24 / 1000\n",
            "\r1/1 [===================>] loss:  0.339\r1/1 [===================>] loss: 0.33829466963409355 ; acc: 0.2\n",
            "Epoch 25 / 1000\n",
            "\r1/1 [===================>] loss:  0.338\r1/1 [===================>] loss: 0.3379414635524693 ; acc: 0.2\n",
            "Epoch 26 / 1000\n",
            "\r1/1 [===================>] loss:  0.338\r1/1 [===================>] loss: 0.33759624883782563 ; acc: 0.2\n",
            "Epoch 27 / 1000\n",
            "\r1/1 [===================>] loss:  0.338\r1/1 [===================>] loss: 0.3372583683044027 ; acc: 0.2\n",
            "Epoch 28 / 1000\n",
            "\r1/1 [===================>] loss:  0.337\r1/1 [===================>] loss: 0.336927213454446 ; acc: 0.2\n",
            "Epoch 29 / 1000\n",
            "\r1/1 [===================>] loss:  0.337\r1/1 [===================>] loss: 0.33660222008686475 ; acc: 0.2\n",
            "Epoch 30 / 1000\n",
            "\r1/1 [===================>] loss:  0.337\r1/1 [===================>] loss: 0.33628286431951715 ; acc: 0.2\n",
            "Epoch 31 / 1000\n",
            "\r1/1 [===================>] loss:  0.336\r1/1 [===================>] loss: 0.3359686589859927 ; acc: 0.2\n",
            "Epoch 32 / 1000\n",
            "\r1/1 [===================>] loss:  0.336\r1/1 [===================>] loss: 0.335659150370938 ; acc: 0.2\n",
            "Epoch 33 / 1000\n",
            "\r1/1 [===================>] loss:  0.336\r1/1 [===================>] loss: 0.3353539152510648 ; acc: 0.2\n",
            "Epoch 34 / 1000\n",
            "\r1/1 [===================>] loss:  0.335\r1/1 [===================>] loss: 0.335052558211936 ; acc: 0.2\n",
            "Epoch 35 / 1000\n",
            "\r1/1 [===================>] loss:  0.335\r1/1 [===================>] loss: 0.3347547092134019 ; acc: 0.2\n",
            "Epoch 36 / 1000\n",
            "\r1/1 [===================>] loss:  0.335\r1/1 [===================>] loss: 0.3344600213791436 ; acc: 0.2\n",
            "Epoch 37 / 1000\n",
            "\r1/1 [===================>] loss:  0.334\r1/1 [===================>] loss: 0.3341681689881663 ; acc: 0.2\n",
            "Epoch 38 / 1000\n",
            "\r1/1 [===================>] loss:  0.334\r1/1 [===================>] loss: 0.33387884564826653 ; acc: 0.2\n",
            "Epoch 39 / 1000\n",
            "\r1/1 [===================>] loss:  0.334\r1/1 [===================>] loss: 0.33359176263348983 ; acc: 0.2\n",
            "Epoch 40 / 1000\n",
            "\r1/1 [===================>] loss:  0.334\r1/1 [===================>] loss: 0.33330664736940174 ; acc: 0.2\n",
            "Epoch 41 / 1000\n",
            "\r1/1 [===================>] loss:  0.333\r1/1 [===================>] loss: 0.3330232420516251 ; acc: 0.2\n",
            "Epoch 42 / 1000\n",
            "\r1/1 [===================>] loss:  0.333\r1/1 [===================>] loss: 0.33274130238457544 ; acc: 0.2\n",
            "Epoch 43 / 1000\n",
            "\r1/1 [===================>] loss:  0.333\r1/1 [===================>] loss: 0.33246059642864906 ; acc: 0.2\n",
            "Epoch 44 / 1000\n",
            "\r1/1 [===================>] loss:  0.332\r1/1 [===================>] loss: 0.33218090354531205 ; acc: 0.2\n",
            "Epoch 45 / 1000\n",
            "\r1/1 [===================>] loss:  0.332\r1/1 [===================>] loss: 0.3319020134306074 ; acc: 0.2\n",
            "Epoch 46 / 1000\n",
            "\r1/1 [===================>] loss:  0.332\r1/1 [===================>] loss: 0.3316237252285603 ; acc: 0.2\n",
            "Epoch 47 / 1000\n",
            "\r1/1 [===================>] loss:  0.332\r1/1 [===================>] loss: 0.3313458467168174 ; acc: 0.2\n",
            "Epoch 48 / 1000\n",
            "\r1/1 [===================>] loss:  0.331\r1/1 [===================>] loss: 0.3310681935576284 ; acc: 0.2\n",
            "Epoch 49 / 1000\n",
            "\r1/1 [===================>] loss:  0.331\r1/1 [===================>] loss: 0.33079058860796906 ; acc: 0.2\n",
            "Epoch 50 / 1000\n",
            "\r1/1 [===================>] loss:  0.331\r1/1 [===================>] loss: 0.3305128612832203 ; acc: 0.2\n",
            "Epoch 51 / 1000\n",
            "\r1/1 [===================>] loss:  0.331\r1/1 [===================>] loss: 0.3302348469693712 ; acc: 0.2\n",
            "Epoch 52 / 1000\n",
            "\r1/1 [===================>] loss:  0.33\r1/1 [===================>] loss: 0.3299563864792114 ; acc: 0.2\n",
            "Epoch 53 / 1000\n",
            "\r1/1 [===================>] loss:  0.33\r1/1 [===================>] loss: 0.3296773255484191 ; acc: 0.2\n",
            "Epoch 54 / 1000\n",
            "\r1/1 [===================>] loss:  0.33\r1/1 [===================>] loss: 0.3293975143678506 ; acc: 0.2\n",
            "Epoch 55 / 1000\n",
            "\r1/1 [===================>] loss:  0.329\r1/1 [===================>] loss: 0.3291168071486911 ; acc: 0.2\n",
            "Epoch 56 / 1000\n",
            "\r1/1 [===================>] loss:  0.329\r1/1 [===================>] loss: 0.3288350617174512 ; acc: 0.2\n",
            "Epoch 57 / 1000\n",
            "\r1/1 [===================>] loss:  0.329\r1/1 [===================>] loss: 0.3285521391380731 ; acc: 0.2\n",
            "Epoch 58 / 1000\n",
            "\r1/1 [===================>] loss:  0.329\r1/1 [===================>] loss: 0.328267903358676 ; acc: 0.2\n",
            "Epoch 59 / 1000\n",
            "\r1/1 [===================>] loss:  0.328\r1/1 [===================>] loss: 0.3279822208806956 ; acc: 0.2\n",
            "Epoch 60 / 1000\n",
            "\r1/1 [===================>] loss:  0.328\r1/1 [===================>] loss: 0.32769496044838486 ; acc: 0.2\n",
            "Epoch 61 / 1000\n",
            "\r1/1 [===================>] loss:  0.328\r1/1 [===================>] loss: 0.327405992756829 ; acc: 0.2\n",
            "Epoch 62 / 1000\n",
            "\r1/1 [===================>] loss:  0.327\r1/1 [===================>] loss: 0.32711519017679697 ; acc: 0.2\n",
            "Epoch 63 / 1000\n",
            "\r1/1 [===================>] loss:  0.327\r1/1 [===================>] loss: 0.32682242649490545 ; acc: 0.2\n",
            "Epoch 64 / 1000\n",
            "\r1/1 [===================>] loss:  0.327\r1/1 [===================>] loss: 0.32652757666770665 ; acc: 0.2\n",
            "Epoch 65 / 1000\n",
            "\r1/1 [===================>] loss:  0.327\r1/1 [===================>] loss: 0.32623051658843905 ; acc: 0.2\n",
            "Epoch 66 / 1000\n",
            "\r1/1 [===================>] loss:  0.326\r1/1 [===================>] loss: 0.3259311228652909 ; acc: 0.2\n",
            "Epoch 67 / 1000\n",
            "\r1/1 [===================>] loss:  0.326\r1/1 [===================>] loss: 0.3256292726101287 ; acc: 0.2\n",
            "Epoch 68 / 1000\n",
            "\r1/1 [===================>] loss:  0.326\r1/1 [===================>] loss: 0.32532484323673844 ; acc: 0.2\n",
            "Epoch 69 / 1000\n",
            "\r1/1 [===================>] loss:  0.325\r1/1 [===================>] loss: 0.3250177122677092 ; acc: 0.2\n",
            "Epoch 70 / 1000\n",
            "\r1/1 [===================>] loss:  0.325\r1/1 [===================>] loss: 0.32470775714916833 ; acc: 0.2\n",
            "Epoch 71 / 1000\n",
            "\r1/1 [===================>] loss:  0.325\r1/1 [===================>] loss: 0.32439485507264776 ; acc: 0.2\n",
            "Epoch 72 / 1000\n",
            "\r1/1 [===================>] loss:  0.324\r1/1 [===================>] loss: 0.32407888280342584 ; acc: 0.2\n",
            "Epoch 73 / 1000\n",
            "\r1/1 [===================>] loss:  0.324\r1/1 [===================>] loss: 0.3237597165147499 ; acc: 0.2\n",
            "Epoch 74 / 1000\n",
            "\r1/1 [===================>] loss:  0.324\r1/1 [===================>] loss: 0.3234372316274001 ; acc: 0.2\n",
            "Epoch 75 / 1000\n",
            "\r1/1 [===================>] loss:  0.323\r1/1 [===================>] loss: 0.32311130265410637 ; acc: 0.2\n",
            "Epoch 76 / 1000\n",
            "\r1/1 [===================>] loss:  0.323\r1/1 [===================>] loss: 0.3227818030483786 ; acc: 0.2\n",
            "Epoch 77 / 1000\n",
            "\r1/1 [===================>] loss:  0.323\r1/1 [===================>] loss: 0.3224486050573577 ; acc: 0.2\n",
            "Epoch 78 / 1000\n",
            "\r1/1 [===================>] loss:  0.322\r1/1 [===================>] loss: 0.32211157957833286 ; acc: 0.2\n",
            "Epoch 79 / 1000\n",
            "\r1/1 [===================>] loss:  0.322\r1/1 [===================>] loss: 0.32177059601861846 ; acc: 0.2\n",
            "Epoch 80 / 1000\n",
            "\r1/1 [===================>] loss:  0.322\r1/1 [===================>] loss: 0.3214255221585153 ; acc: 0.2\n",
            "Epoch 81 / 1000\n",
            "\r1/1 [===================>] loss:  0.321\r1/1 [===================>] loss: 0.32107622401712466 ; acc: 0.2\n",
            "Epoch 82 / 1000\n",
            "\r1/1 [===================>] loss:  0.321\r1/1 [===================>] loss: 0.32072256572081853 ; acc: 0.2\n",
            "Epoch 83 / 1000\n",
            "\r1/1 [===================>] loss:  0.321\r1/1 [===================>] loss: 0.32036440937420174 ; acc: 0.2\n",
            "Epoch 84 / 1000\n",
            "\r1/1 [===================>] loss:  0.32\r1/1 [===================>] loss: 0.3200016149334438 ; acc: 0.2\n",
            "Epoch 85 / 1000\n",
            "\r1/1 [===================>] loss:  0.32\r1/1 [===================>] loss: 0.31963404008188967 ; acc: 0.2\n",
            "Epoch 86 / 1000\n",
            "\r1/1 [===================>] loss:  0.32\r1/1 [===================>] loss: 0.31926154010789365 ; acc: 0.2\n",
            "Epoch 87 / 1000\n",
            "\r1/1 [===================>] loss:  0.319\r1/1 [===================>] loss: 0.3188839677848609 ; acc: 0.2\n",
            "Epoch 88 / 1000\n",
            "\r1/1 [===================>] loss:  0.319\r1/1 [===================>] loss: 0.3185011732535141 ; acc: 0.2\n",
            "Epoch 89 / 1000\n",
            "\r1/1 [===================>] loss:  0.319\r1/1 [===================>] loss: 0.31811300390644337 ; acc: 0.2\n",
            "Epoch 90 / 1000\n",
            "\r1/1 [===================>] loss:  0.318\r1/1 [===================>] loss: 0.31771930427503575 ; acc: 0.2\n",
            "Epoch 91 / 1000\n",
            "\r1/1 [===================>] loss:  0.318\r1/1 [===================>] loss: 0.31731991591892345 ; acc: 0.2\n",
            "Epoch 92 / 1000\n",
            "\r1/1 [===================>] loss:  0.317\r1/1 [===================>] loss: 0.3169146773181305 ; acc: 0.2\n",
            "Epoch 93 / 1000\n",
            "\r1/1 [===================>] loss:  0.317\r1/1 [===================>] loss: 0.3165034237681442 ; acc: 0.2\n",
            "Epoch 94 / 1000\n",
            "\r1/1 [===================>] loss:  0.317\r1/1 [===================>] loss: 0.3160859872781877 ; acc: 0.2\n",
            "Epoch 95 / 1000\n",
            "\r1/1 [===================>] loss:  0.316\r1/1 [===================>] loss: 0.3156621964730145 ; acc: 0.2\n",
            "Epoch 96 / 1000\n",
            "\r1/1 [===================>] loss:  0.316\r1/1 [===================>] loss: 0.31523187649860657 ; acc: 0.2\n",
            "Epoch 97 / 1000\n",
            "\r1/1 [===================>] loss:  0.315\r1/1 [===================>] loss: 0.3147948489322102 ; acc: 0.2\n",
            "Epoch 98 / 1000\n",
            "\r1/1 [===================>] loss:  0.315\r1/1 [===================>] loss: 0.3143509316972041 ; acc: 0.2\n",
            "Epoch 99 / 1000\n",
            "\r1/1 [===================>] loss:  0.314\r1/1 [===================>] loss: 0.31389993898336227 ; acc: 0.2\n",
            "Epoch 100 / 1000\n",
            "\r1/1 [===================>] loss:  0.314\r1/1 [===================>] loss: 0.31344168117314053 ; acc: 0.2\n",
            "Epoch 101 / 1000\n",
            "\r1/1 [===================>] loss:  0.313\r1/1 [===================>] loss: 0.31297596477468764 ; acc: 0.2\n",
            "Epoch 102 / 1000\n",
            "\r1/1 [===================>] loss:  0.313\r1/1 [===================>] loss: 0.31250259236236516 ; acc: 0.2\n",
            "Epoch 103 / 1000\n",
            "\r1/1 [===================>] loss:  0.313\r1/1 [===================>] loss: 0.3120213625256365 ; acc: 0.2\n",
            "Epoch 104 / 1000\n",
            "\r1/1 [===================>] loss:  0.312\r1/1 [===================>] loss: 0.31153206982728127 ; acc: 0.2\n",
            "Epoch 105 / 1000\n",
            "\r1/1 [===================>] loss:  0.312\r1/1 [===================>] loss: 0.3110345047719789 ; acc: 0.2\n",
            "Epoch 106 / 1000\n",
            "\r1/1 [===================>] loss:  0.311\r1/1 [===================>] loss: 0.3105284537864092 ; acc: 0.2\n",
            "Epoch 107 / 1000\n",
            "\r1/1 [===================>] loss:  0.311\r1/1 [===================>] loss: 0.3100136992121211 ; acc: 0.2\n",
            "Epoch 108 / 1000\n",
            "\r1/1 [===================>] loss:  0.31\r1/1 [===================>] loss: 0.30949001931253284 ; acc: 0.2\n",
            "Epoch 109 / 1000\n",
            "\r1/1 [===================>] loss:  0.309\r1/1 [===================>] loss: 0.3089571882955428 ; acc: 0.2\n",
            "Epoch 110 / 1000\n",
            "\r1/1 [===================>] loss:  0.309\r1/1 [===================>] loss: 0.3084149763533557 ; acc: 0.2\n",
            "Epoch 111 / 1000\n",
            "\r1/1 [===================>] loss:  0.308\r1/1 [===================>] loss: 0.3078631497212527 ; acc: 0.2\n",
            "Epoch 112 / 1000\n",
            "\r1/1 [===================>] loss:  0.308\r1/1 [===================>] loss: 0.30730147075717384 ; acc: 0.2\n",
            "Epoch 113 / 1000\n",
            "\r1/1 [===================>] loss:  0.307\r1/1 [===================>] loss: 0.30672969804411243 ; acc: 0.2\n",
            "Epoch 114 / 1000\n",
            "\r1/1 [===================>] loss:  0.307\r1/1 [===================>] loss: 0.3061475865174707 ; acc: 0.2\n",
            "Epoch 115 / 1000\n",
            "\r1/1 [===================>] loss:  0.306\r1/1 [===================>] loss: 0.3055548876196653 ; acc: 0.2\n",
            "Epoch 116 / 1000\n",
            "\r1/1 [===================>] loss:  0.306\r1/1 [===================>] loss: 0.3049513494844259 ; acc: 0.2\n",
            "Epoch 117 / 1000\n",
            "\r1/1 [===================>] loss:  0.305\r1/1 [===================>] loss: 0.30433671715337235 ; acc: 0.2\n",
            "Epoch 118 / 1000\n",
            "\r1/1 [===================>] loss:  0.304\r1/1 [===================>] loss: 0.30371073282760996 ; acc: 0.2\n",
            "Epoch 119 / 1000\n",
            "\r1/1 [===================>] loss:  0.304\r1/1 [===================>] loss: 0.30307313615722625 ; acc: 0.2\n",
            "Epoch 120 / 1000\n",
            "\r1/1 [===================>] loss:  0.303\r1/1 [===================>] loss: 0.30242366457171116 ; acc: 0.2\n",
            "Epoch 121 / 1000\n",
            "\r1/1 [===================>] loss:  0.302\r1/1 [===================>] loss: 0.3017620536544603 ; acc: 0.2\n",
            "Epoch 122 / 1000\n",
            "\r1/1 [===================>] loss:  0.302\r1/1 [===================>] loss: 0.3010880375646404 ; acc: 0.2\n",
            "Epoch 123 / 1000\n",
            "\r1/1 [===================>] loss:  0.301\r1/1 [===================>] loss: 0.30040134950980374 ; acc: 0.2\n",
            "Epoch 124 / 1000\n",
            "\r1/1 [===================>] loss:  0.3\r1/1 [===================>] loss: 0.29970172227273045 ; acc: 0.2\n",
            "Epoch 125 / 1000\n",
            "\r1/1 [===================>] loss:  0.3\r1/1 [===================>] loss: 0.29898888879604 ; acc: 0.2\n",
            "Epoch 126 / 1000\n",
            "\r1/1 [===================>] loss:  0.299\r1/1 [===================>] loss: 0.29826258282815254 ; acc: 0.2\n",
            "Epoch 127 / 1000\n",
            "\r1/1 [===================>] loss:  0.298\r1/1 [===================>] loss: 0.2975225396341807 ; acc: 0.2\n",
            "Epoch 128 / 1000\n",
            "\r1/1 [===================>] loss:  0.298\r1/1 [===================>] loss: 0.2967684967752947 ; acc: 0.2\n",
            "Epoch 129 / 1000\n",
            "\r1/1 [===================>] loss:  0.297\r1/1 [===================>] loss: 0.2960001949600127 ; acc: 0.2\n",
            "Epoch 130 / 1000\n",
            "\r1/1 [===================>] loss:  0.296\r1/1 [===================>] loss: 0.29521737897072264 ; acc: 0.2\n",
            "Epoch 131 / 1000\n",
            "\r1/1 [===================>] loss:  0.295\r1/1 [===================>] loss: 0.2944197986685341 ; acc: 0.2\n",
            "Epoch 132 / 1000\n",
            "\r1/1 [===================>] loss:  0.294\r1/1 [===================>] loss: 0.2936072100792658 ; acc: 0.2\n",
            "Epoch 133 / 1000\n",
            "\r1/1 [===================>] loss:  0.294\r1/1 [===================>] loss: 0.29277937656301034 ; acc: 0.2\n",
            "Epoch 134 / 1000\n",
            "\r1/1 [===================>] loss:  0.293\r1/1 [===================>] loss: 0.2919360700692498 ; acc: 0.2\n",
            "Epoch 135 / 1000\n",
            "\r1/1 [===================>] loss:  0.292\r1/1 [===================>] loss: 0.2910770724789292 ; acc: 0.2\n",
            "Epoch 136 / 1000\n",
            "\r1/1 [===================>] loss:  0.291\r1/1 [===================>] loss: 0.29020217703421164 ; acc: 0.2\n",
            "Epoch 137 / 1000\n",
            "\r1/1 [===================>] loss:  0.29\r1/1 [===================>] loss: 0.28931118985583376 ; acc: 0.2\n",
            "Epoch 138 / 1000\n",
            "\r1/1 [===================>] loss:  0.289\r1/1 [===================>] loss: 0.28840393154704513 ; acc: 0.2\n",
            "Epoch 139 / 1000\n",
            "\r1/1 [===================>] loss:  0.288\r1/1 [===================>] loss: 0.2874802388820365 ; acc: 0.2\n",
            "Epoch 140 / 1000\n",
            "\r1/1 [===================>] loss:  0.287\r1/1 [===================>] loss: 0.2865399665755445 ; acc: 0.2\n",
            "Epoch 141 / 1000\n",
            "\r1/1 [===================>] loss:  0.287\r1/1 [===================>] loss: 0.28558298912894753 ; acc: 0.2\n",
            "Epoch 142 / 1000\n",
            "\r1/1 [===================>] loss:  0.286\r1/1 [===================>] loss: 0.28460920274665574 ; acc: 0.2\n",
            "Epoch 143 / 1000\n",
            "\r1/1 [===================>] loss:  0.285\r1/1 [===================>] loss: 0.28361852731492976 ; acc: 0.2\n",
            "Epoch 144 / 1000\n",
            "\r1/1 [===================>] loss:  0.284\r1/1 [===================>] loss: 0.28261090843346354 ; acc: 0.2\n",
            "Epoch 145 / 1000\n",
            "\r1/1 [===================>] loss:  0.283\r1/1 [===================>] loss: 0.2815863194881385 ; acc: 0.2\n",
            "Epoch 146 / 1000\n",
            "\r1/1 [===================>] loss:  0.282\r1/1 [===================>] loss: 0.2805447637513182 ; acc: 0.2\n",
            "Epoch 147 / 1000\n",
            "1/1 [===================>] loss: 0.27948627649393165 ; acc: 0.2\n",
            "Epoch 148 / 1000\n",
            "1/1 [===================>] loss: 0.2784109270914164 ; acc: 0.2\n",
            "Epoch 149 / 1000\n",
            "1/1 [===================>] loss: 0.2773188211034032 ; acc: 0.2\n",
            "Epoch 150 / 1000\n",
            "1/1 [===================>] loss: 0.27621010230485993 ; acc: 0.2\n",
            "Epoch 151 / 1000\n",
            "1/1 [===================>] loss: 0.27508495464433735 ; acc: 0.2\n",
            "Epoch 152 / 1000\n",
            "1/1 [===================>] loss: 0.2739436041030242 ; acc: 0.2\n",
            "Epoch 153 / 1000\n",
            "1/1 [===================>] loss: 0.2727863204265946 ; acc: 0.2\n",
            "Epoch 154 / 1000\n",
            "1/1 [===================>] loss: 0.2716134187003958 ; acc: 0.2\n",
            "Epoch 155 / 1000\n",
            "1/1 [===================>] loss: 0.2704252607374401 ; acc: 0.2\n",
            "Epoch 156 / 1000\n",
            "1/1 [===================>] loss: 0.2692222562480295 ; acc: 0.2\n",
            "Epoch 157 / 1000\n",
            "1/1 [===================>] loss: 0.26800486375971017 ; acc: 0.2\n",
            "Epoch 158 / 1000\n",
            "1/1 [===================>] loss: 0.26677359125672484 ; acc: 0.2\n",
            "Epoch 159 / 1000\n",
            "1/1 [===================>] loss: 0.2655289965092442 ; acc: 0.2\n",
            "Epoch 160 / 1000\n",
            "1/1 [===================>] loss: 0.26427168706449267 ; acc: 0.2\n",
            "Epoch 161 / 1000\n",
            "1/1 [===================>] loss: 0.2630023198744642 ; acc: 0.2\n",
            "Epoch 162 / 1000\n",
            "1/1 [===================>] loss: 0.2617216005382768 ; acc: 0.2\n",
            "Epoch 163 / 1000\n",
            "1/1 [===================>] loss: 0.26043028214134234 ; acc: 0.2\n",
            "Epoch 164 / 1000\n",
            "1/1 [===================>] loss: 0.25912916367839933 ; acc: 0.2\n",
            "Epoch 165 / 1000\n",
            "1/1 [===================>] loss: 0.2578190880530241 ; acc: 0.2\n",
            "Epoch 166 / 1000\n",
            "1/1 [===================>] loss: 0.25650093965240545 ; acc: 0.2\n",
            "Epoch 167 / 1000\n",
            "1/1 [===================>] loss: 0.2551756415028458 ; acc: 0.2\n",
            "Epoch 168 / 1000\n",
            "1/1 [===================>] loss: 0.2538441520184792 ; acc: 0.2\n",
            "Epoch 169 / 1000\n",
            "1/1 [===================>] loss: 0.25250746136292224 ; acc: 0.2\n",
            "Epoch 170 / 1000\n",
            "1/1 [===================>] loss: 0.25116658745080955 ; acc: 0.2\n",
            "Epoch 171 / 1000\n",
            "1/1 [===================>] loss: 0.2498225716232067 ; acc: 0.2\n",
            "Epoch 172 / 1000\n",
            "1/1 [===================>] loss: 0.248476474037545 ; acc: 0.2\n",
            "Epoch 173 / 1000\n",
            "1/1 [===================>] loss: 0.24712936881876696 ; acc: 0.2\n",
            "Epoch 174 / 1000\n",
            "1/1 [===================>] loss: 0.24578233902362348 ; acc: 0.2\n",
            "Epoch 175 / 1000\n",
            "1/1 [===================>] loss: 0.24443647147433664 ; acc: 0.2\n",
            "Epoch 176 / 1000\n",
            "1/1 [===================>] loss: 0.24309285152099863 ; acc: 0.2\n",
            "Epoch 177 / 1000\n",
            "1/1 [===================>] loss: 0.2417525577939846 ; acc: 0.2\n",
            "Epoch 178 / 1000\n",
            "1/1 [===================>] loss: 0.24041665700826073 ; acc: 0.2\n",
            "Epoch 179 / 1000\n",
            "1/1 [===================>] loss: 0.2390861988807175 ; acc: 0.2\n",
            "Epoch 180 / 1000\n",
            "1/1 [===================>] loss: 0.23776221121958474 ; acc: 0.2\n",
            "Epoch 181 / 1000\n",
            "1/1 [===================>] loss: 0.2364456952416401 ; acc: 0.2\n",
            "Epoch 182 / 1000\n",
            "1/1 [===================>] loss: 0.23513762116841783 ; acc: 0.2\n",
            "Epoch 183 / 1000\n",
            "1/1 [===================>] loss: 0.23383892414710156 ; acc: 0.2\n",
            "Epoch 184 / 1000\n",
            "1/1 [===================>] loss: 0.23255050053541967 ; acc: 0.2\n",
            "Epoch 185 / 1000\n",
            "1/1 [===================>] loss: 0.23127320458285522 ; acc: 0.2\n",
            "Epoch 186 / 1000\n",
            "1/1 [===================>] loss: 0.23000784553304418 ; acc: 0.2\n",
            "Epoch 187 / 1000\n",
            "1/1 [===================>] loss: 0.22875518516459117 ; acc: 0.2\n",
            "Epoch 188 / 1000\n",
            "1/1 [===================>] loss: 0.22751593577989224 ; acc: 0.2\n",
            "Epoch 189 / 1000\n",
            "1/1 [===================>] loss: 0.22629075864412046 ; acc: 0.2\n",
            "Epoch 190 / 1000\n",
            "1/1 [===================>] loss: 0.2250802628694939 ; acc: 0.2\n",
            "Epoch 191 / 1000\n",
            "1/1 [===================>] loss: 0.22388500473344722 ; acc: 0.2\n",
            "Epoch 192 / 1000\n",
            "1/1 [===================>] loss: 0.2227054874135186 ; acc: 0.4\n",
            "Epoch 193 / 1000\n",
            "1/1 [===================>] loss: 0.22154216111671415 ; acc: 0.4\n",
            "Epoch 194 / 1000\n",
            "1/1 [===================>] loss: 0.2203954235769019 ; acc: 0.4\n",
            "Epoch 195 / 1000\n",
            "1/1 [===================>] loss: 0.21926562089043994 ; acc: 0.4\n",
            "Epoch 196 / 1000\n",
            "1/1 [===================>] loss: 0.2181530486577588 ; acc: 0.4\n",
            "Epoch 197 / 1000\n",
            "1/1 [===================>] loss: 0.21705795339697515 ; acc: 0.4\n",
            "Epoch 198 / 1000\n",
            "1/1 [===================>] loss: 0.21598053419475854 ; acc: 0.4\n",
            "Epoch 199 / 1000\n",
            "1/1 [===================>] loss: 0.2149209445595444 ; acc: 0.4\n",
            "Epoch 200 / 1000\n",
            "1/1 [===================>] loss: 0.21387929444270215 ; acc: 0.4\n",
            "Epoch 201 / 1000\n",
            "1/1 [===================>] loss: 0.21285565239434168 ; acc: 0.4\n",
            "Epoch 202 / 1000\n",
            "1/1 [===================>] loss: 0.2118500478219798 ; acc: 0.4\n",
            "Epoch 203 / 1000\n",
            "1/1 [===================>] loss: 0.21086247332220265 ; acc: 0.4\n",
            "Epoch 204 / 1000\n",
            "1/1 [===================>] loss: 0.2098928870576517 ; acc: 0.4\n",
            "Epoch 205 / 1000\n",
            "1/1 [===================>] loss: 0.2089412151540555 ; acc: 0.4\n",
            "Epoch 206 / 1000\n",
            "1/1 [===================>] loss: 0.20800735409454257 ; acc: 0.4\n",
            "Epoch 207 / 1000\n",
            "1/1 [===================>] loss: 0.20709117309103148 ; acc: 0.4\n",
            "Epoch 208 / 1000\n",
            "1/1 [===================>] loss: 0.20619251641505226 ; acc: 0.4\n",
            "Epoch 209 / 1000\n",
            "1/1 [===================>] loss: 0.205311205672845 ; acc: 0.4\n",
            "Epoch 210 / 1000\n",
            "1/1 [===================>] loss: 0.20444704201197933 ; acc: 0.4\n",
            "Epoch 211 / 1000\n",
            "1/1 [===================>] loss: 0.2035998082490012 ; acc: 0.4\n",
            "Epoch 212 / 1000\n",
            "1/1 [===================>] loss: 0.20276927090972227 ; acc: 0.4\n",
            "Epoch 213 / 1000\n",
            "1/1 [===================>] loss: 0.2019551821757052 ; acc: 0.4\n",
            "Epoch 214 / 1000\n",
            "1/1 [===================>] loss: 0.20115728173225933 ; acc: 0.4\n",
            "Epoch 215 / 1000\n",
            "1/1 [===================>] loss: 0.20037529851483246 ; acc: 0.4\n",
            "Epoch 216 / 1000\n",
            "1/1 [===================>] loss: 0.1996089523520848 ; acc: 0.4\n",
            "Epoch 217 / 1000\n",
            "1/1 [===================>] loss: 0.19885795550514346 ; acc: 0.4\n",
            "Epoch 218 / 1000\n",
            "1/1 [===================>] loss: 0.19812201410358943 ; acc: 0.4\n",
            "Epoch 219 / 1000\n",
            "1/1 [===================>] loss: 0.19740082947961518 ; acc: 0.4\n",
            "Epoch 220 / 1000\n",
            "1/1 [===================>] loss: 0.19669409940254015 ; acc: 0.4\n",
            "Epoch 221 / 1000\n",
            "1/1 [===================>] loss: 0.19600151921647477 ; acc: 0.4\n",
            "Epoch 222 / 1000\n",
            "1/1 [===================>] loss: 0.19532278288441662 ; acc: 0.4\n",
            "Epoch 223 / 1000\n",
            "1/1 [===================>] loss: 0.194657583942437 ; acc: 0.4\n",
            "Epoch 224 / 1000\n",
            "1/1 [===================>] loss: 0.1940056163679003 ; acc: 0.4\n",
            "Epoch 225 / 1000\n",
            "1/1 [===================>] loss: 0.19336657536585722 ; acc: 0.4\n",
            "Epoch 226 / 1000\n",
            "1/1 [===================>] loss: 0.1927401580778765 ; acc: 0.4\n",
            "Epoch 227 / 1000\n",
            "1/1 [===================>] loss: 0.19212606421764378 ; acc: 0.6\n",
            "Epoch 228 / 1000\n",
            "1/1 [===================>] loss: 0.19152399663766412 ; acc: 0.6\n",
            "Epoch 229 / 1000\n",
            "1/1 [===================>] loss: 0.19093366183137297 ; acc: 0.6\n",
            "Epoch 230 / 1000\n",
            "1/1 [===================>] loss: 0.19035477037488654 ; acc: 0.6\n",
            "Epoch 231 / 1000\n",
            "1/1 [===================>] loss: 0.18978703731252372 ; acc: 0.6\n",
            "Epoch 232 / 1000\n",
            "1/1 [===================>] loss: 0.18923018249010862 ; acc: 0.6\n",
            "Epoch 233 / 1000\n",
            "1/1 [===================>] loss: 0.18868393083991958 ; acc: 0.6\n",
            "Epoch 234 / 1000\n",
            "1/1 [===================>] loss: 0.18814801262099567 ; acc: 0.6\n",
            "Epoch 235 / 1000\n",
            "1/1 [===================>] loss: 0.18762216361834944 ; acc: 0.6\n",
            "Epoch 236 / 1000\n",
            "1/1 [===================>] loss: 0.18710612530445941 ; acc: 0.6\n",
            "Epoch 237 / 1000\n",
            "1/1 [===================>] loss: 0.18659964496624687 ; acc: 0.6\n",
            "Epoch 238 / 1000\n",
            "1/1 [===================>] loss: 0.18610247580056177 ; acc: 0.6\n",
            "Epoch 239 / 1000\n",
            "1/1 [===================>] loss: 0.18561437698103136 ; acc: 0.6\n",
            "Epoch 240 / 1000\n",
            "1/1 [===================>] loss: 0.18513511369895194 ; acc: 0.6\n",
            "Epoch 241 / 1000\n",
            "1/1 [===================>] loss: 0.18466445718073704 ; acc: 0.6\n",
            "Epoch 242 / 1000\n",
            "1/1 [===================>] loss: 0.18420218468426947 ; acc: 0.6\n",
            "Epoch 243 / 1000\n",
            "1/1 [===================>] loss: 0.18374807947635102 ; acc: 0.6\n",
            "Epoch 244 / 1000\n",
            "1/1 [===================>] loss: 0.18330193079328616 ; acc: 0.6\n",
            "Epoch 245 / 1000\n",
            "1/1 [===================>] loss: 0.18286353378649506 ; acc: 0.6\n",
            "Epoch 246 / 1000\n",
            "1/1 [===================>] loss: 0.18243268945490895 ; acc: 0.6\n",
            "Epoch 247 / 1000\n",
            "1/1 [===================>] loss: 0.18200920456577055 ; acc: 0.6\n",
            "Epoch 248 / 1000\n",
            "1/1 [===================>] loss: 0.18159289156533467 ; acc: 0.6\n",
            "Epoch 249 / 1000\n",
            "1/1 [===================>] loss: 0.18118356848084763 ; acc: 0.6\n",
            "Epoch 250 / 1000\n",
            "1/1 [===================>] loss: 0.18078105881507012 ; acc: 0.6\n",
            "Epoch 251 / 1000\n",
            "1/1 [===================>] loss: 0.1803851914345028 ; acc: 0.6\n",
            "Epoch 252 / 1000\n",
            "1/1 [===================>] loss: 0.17999580045237656 ; acc: 0.6\n",
            "Epoch 253 / 1000\n",
            "1/1 [===================>] loss: 0.17961272510737303 ; acc: 0.6\n",
            "Epoch 254 / 1000\n",
            "1/1 [===================>] loss: 0.17923580963895766 ; acc: 0.6\n",
            "Epoch 255 / 1000\n",
            "1/1 [===================>] loss: 0.17886490316012302 ; acc: 0.6\n",
            "Epoch 256 / 1000\n",
            "1/1 [===================>] loss: 0.17849985952826644 ; acc: 0.6\n",
            "Epoch 257 / 1000\n",
            "1/1 [===================>] loss: 0.17814053721485454 ; acc: 0.6\n",
            "Epoch 258 / 1000\n",
            "1/1 [===================>] loss: 0.17778679917446097 ; acc: 0.6\n",
            "Epoch 259 / 1000\n",
            "1/1 [===================>] loss: 0.17743851271370537 ; acc: 0.6\n",
            "Epoch 260 / 1000\n",
            "1/1 [===================>] loss: 0.177095549360561 ; acc: 0.6\n",
            "Epoch 261 / 1000\n",
            "1/1 [===================>] loss: 0.17675778473445053 ; acc: 0.6\n",
            "Epoch 262 / 1000\n",
            "1/1 [===================>] loss: 0.17642509841749882 ; acc: 0.6\n",
            "Epoch 263 / 1000\n",
            "1/1 [===================>] loss: 0.17609737382726784 ; acc: 0.6\n",
            "Epoch 264 / 1000\n",
            "1/1 [===================>] loss: 0.17577449809125772 ; acc: 0.6\n",
            "Epoch 265 / 1000\n",
            "1/1 [===================>] loss: 0.1754563619234223 ; acc: 0.6\n",
            "Epoch 266 / 1000\n",
            "1/1 [===================>] loss: 0.17514285950290928 ; acc: 0.6\n",
            "Epoch 267 / 1000\n",
            "1/1 [===================>] loss: 0.17483388835520838 ; acc: 0.6\n",
            "Epoch 268 / 1000\n",
            "1/1 [===================>] loss: 0.1745293492358581 ; acc: 0.6\n",
            "Epoch 269 / 1000\n",
            "1/1 [===================>] loss: 0.17422914601683853 ; acc: 0.6\n",
            "Epoch 270 / 1000\n",
            "1/1 [===================>] loss: 0.1739331855757519 ; acc: 0.6\n",
            "Epoch 271 / 1000\n",
            "1/1 [===================>] loss: 0.17364137768787213 ; acc: 0.6\n",
            "Epoch 272 / 1000\n",
            "1/1 [===================>] loss: 0.17335363492112454 ; acc: 0.6\n",
            "Epoch 273 / 1000\n",
            "1/1 [===================>] loss: 0.1730698725340396 ; acc: 0.6\n",
            "Epoch 274 / 1000\n",
            "1/1 [===================>] loss: 0.17279000837670927 ; acc: 0.6\n",
            "Epoch 275 / 1000\n",
            "1/1 [===================>] loss: 0.17251396279475834 ; acc: 0.6\n",
            "Epoch 276 / 1000\n",
            "1/1 [===================>] loss: 0.17224165853633383 ; acc: 0.6\n",
            "Epoch 277 / 1000\n",
            "1/1 [===================>] loss: 0.17197302066210052 ; acc: 0.6\n",
            "Epoch 278 / 1000\n",
            "1/1 [===================>] loss: 0.1717079764582239 ; acc: 0.6\n",
            "Epoch 279 / 1000\n",
            "1/1 [===================>] loss: 0.17144645535231076 ; acc: 0.6\n",
            "Epoch 280 / 1000\n",
            "1/1 [===================>] loss: 0.17118838883227155 ; acc: 0.6\n",
            "Epoch 281 / 1000\n",
            "1/1 [===================>] loss: 0.17093371036806057 ; acc: 0.6\n",
            "Epoch 282 / 1000\n",
            "1/1 [===================>] loss: 0.17068235533624532 ; acc: 0.6\n",
            "Epoch 283 / 1000\n",
            "1/1 [===================>] loss: 0.17043426094735092 ; acc: 0.6\n",
            "Epoch 284 / 1000\n",
            "1/1 [===================>] loss: 0.1701893661759213 ; acc: 0.6\n",
            "Epoch 285 / 1000\n",
            "1/1 [===================>] loss: 0.16994761169323555 ; acc: 0.6\n",
            "Epoch 286 / 1000\n",
            "1/1 [===================>] loss: 0.16970893980261442 ; acc: 0.6\n",
            "Epoch 287 / 1000\n",
            "1/1 [===================>] loss: 0.1694732943772507 ; acc: 0.6\n",
            "Epoch 288 / 1000\n",
            "1/1 [===================>] loss: 0.16924062080049396 ; acc: 0.6\n",
            "Epoch 289 / 1000\n",
            "1/1 [===================>] loss: 0.1690108659085205 ; acc: 0.6\n",
            "Epoch 290 / 1000\n",
            "1/1 [===================>] loss: 0.1687839779353165 ; acc: 0.6\n",
            "Epoch 291 / 1000\n",
            "1/1 [===================>] loss: 0.16855990645990404 ; acc: 0.6\n",
            "Epoch 292 / 1000\n",
            "1/1 [===================>] loss: 0.16833860235573722 ; acc: 0.6\n",
            "Epoch 293 / 1000\n",
            "1/1 [===================>] loss: 0.16812001774219698 ; acc: 0.6\n",
            "Epoch 294 / 1000\n",
            "1/1 [===================>] loss: 0.16790410593811442 ; acc: 0.6\n",
            "Epoch 295 / 1000\n",
            "1/1 [===================>] loss: 0.16769082141725003 ; acc: 0.6\n",
            "Epoch 296 / 1000\n",
            "1/1 [===================>] loss: 0.16748011976566043 ; acc: 0.6\n",
            "Epoch 297 / 1000\n",
            "1/1 [===================>] loss: 0.16727195764088326 ; acc: 0.6\n",
            "Epoch 298 / 1000\n",
            "1/1 [===================>] loss: 0.16706629273287207 ; acc: 0.6\n",
            "Epoch 299 / 1000\n",
            "1/1 [===================>] loss: 0.16686308372661468 ; acc: 0.6\n",
            "Epoch 300 / 1000\n",
            "1/1 [===================>] loss: 0.16666229026637056 ; acc: 0.6\n",
            "Epoch 301 / 1000\n",
            "1/1 [===================>] loss: 0.16646387292146203 ; acc: 0.6\n",
            "Epoch 302 / 1000\n",
            "1/1 [===================>] loss: 0.16626779315355855 ; acc: 0.6\n",
            "Epoch 303 / 1000\n",
            "1/1 [===================>] loss: 0.16607401328539229 ; acc: 0.6\n",
            "Epoch 304 / 1000\n",
            "1/1 [===================>] loss: 0.16588249647084624 ; acc: 0.6\n",
            "Epoch 305 / 1000\n",
            "1/1 [===================>] loss: 0.16569320666635834 ; acc: 0.6\n",
            "Epoch 306 / 1000\n",
            "1/1 [===================>] loss: 0.16550610860358467 ; acc: 0.6\n",
            "Epoch 307 / 1000\n",
            "1/1 [===================>] loss: 0.16532116776326866 ; acc: 0.6\n",
            "Epoch 308 / 1000\n",
            "1/1 [===================>] loss: 0.16513835035026386 ; acc: 0.6\n",
            "Epoch 309 / 1000\n",
            "1/1 [===================>] loss: 0.16495762326966015 ; acc: 0.6\n",
            "Epoch 310 / 1000\n",
            "1/1 [===================>] loss: 0.16477895410396373 ; acc: 0.6\n",
            "Epoch 311 / 1000\n",
            "1/1 [===================>] loss: 0.1646023110912851 ; acc: 0.6\n",
            "Epoch 312 / 1000\n",
            "1/1 [===================>] loss: 0.16442766310448878 ; acc: 0.6\n",
            "Epoch 313 / 1000\n",
            "1/1 [===================>] loss: 0.16425497963126157 ; acc: 0.6\n",
            "Epoch 314 / 1000\n",
            "1/1 [===================>] loss: 0.16408423075505713 ; acc: 0.6\n",
            "Epoch 315 / 1000\n",
            "1/1 [===================>] loss: 0.16391538713687667 ; acc: 0.6\n",
            "Epoch 316 / 1000\n",
            "1/1 [===================>] loss: 0.16374841999784626 ; acc: 0.6\n",
            "Epoch 317 / 1000\n",
            "1/1 [===================>] loss: 0.16358330110255526 ; acc: 0.6\n",
            "Epoch 318 / 1000\n",
            "1/1 [===================>] loss: 0.16342000274311758 ; acc: 0.6\n",
            "Epoch 319 / 1000\n",
            "1/1 [===================>] loss: 0.16325849772392395 ; acc: 0.6\n",
            "Epoch 320 / 1000\n",
            "1/1 [===================>] loss: 0.16309875934705043 ; acc: 0.6\n",
            "Epoch 321 / 1000\n",
            "1/1 [===================>] loss: 0.16294076139829333 ; acc: 0.6\n",
            "Epoch 322 / 1000\n",
            "1/1 [===================>] loss: 0.162784478133799 ; acc: 0.6\n",
            "Epoch 323 / 1000\n",
            "1/1 [===================>] loss: 0.1626298842672611 ; acc: 0.6\n",
            "Epoch 324 / 1000\n",
            "1/1 [===================>] loss: 0.16247695495765652 ; acc: 0.6\n",
            "Epoch 325 / 1000\n",
            "1/1 [===================>] loss: 0.162325665797495 ; acc: 0.6\n",
            "Epoch 326 / 1000\n",
            "1/1 [===================>] loss: 0.1621759928015561 ; acc: 0.6\n",
            "Epoch 327 / 1000\n",
            "1/1 [===================>] loss: 0.1620279123960906 ; acc: 0.6\n",
            "Epoch 328 / 1000\n",
            "1/1 [===================>] loss: 0.16188140140846313 ; acc: 0.6\n",
            "Epoch 329 / 1000\n",
            "1/1 [===================>] loss: 0.16173643705721366 ; acc: 0.6\n",
            "Epoch 330 / 1000\n",
            "1/1 [===================>] loss: 0.16159299694251894 ; acc: 0.6\n",
            "Epoch 331 / 1000\n",
            "1/1 [===================>] loss: 0.16145105903703116 ; acc: 0.6\n",
            "Epoch 332 / 1000\n",
            "1/1 [===================>] loss: 0.1613106016770784 ; acc: 0.6\n",
            "Epoch 333 / 1000\n",
            "1/1 [===================>] loss: 0.16117160355420593 ; acc: 0.6\n",
            "Epoch 334 / 1000\n",
            "1/1 [===================>] loss: 0.16103404370704394 ; acc: 0.6\n",
            "Epoch 335 / 1000\n",
            "1/1 [===================>] loss: 0.1608979015134833 ; acc: 0.6\n",
            "Epoch 336 / 1000\n",
            "1/1 [===================>] loss: 0.16076315668314545 ; acc: 0.6\n",
            "Epoch 337 / 1000\n",
            "1/1 [===================>] loss: 0.16062978925013102 ; acc: 0.6\n",
            "Epoch 338 / 1000\n",
            "1/1 [===================>] loss: 0.16049777956603384 ; acc: 0.6\n",
            "Epoch 339 / 1000\n",
            "1/1 [===================>] loss: 0.16036710829320638 ; acc: 0.6\n",
            "Epoch 340 / 1000\n",
            "1/1 [===================>] loss: 0.1602377563982649 ; acc: 0.6\n",
            "Epoch 341 / 1000\n",
            "1/1 [===================>] loss: 0.16010970514582196 ; acc: 0.6\n",
            "Epoch 342 / 1000\n",
            "1/1 [===================>] loss: 0.15998293609243497 ; acc: 0.6\n",
            "Epoch 343 / 1000\n",
            "1/1 [===================>] loss: 0.15985743108076064 ; acc: 0.6\n",
            "Epoch 344 / 1000\n",
            "1/1 [===================>] loss: 0.15973317223390412 ; acc: 0.6\n",
            "Epoch 345 / 1000\n",
            "1/1 [===================>] loss: 0.15961014194995454 ; acc: 0.6\n",
            "Epoch 346 / 1000\n",
            "1/1 [===================>] loss: 0.15948832289669687 ; acc: 0.6\n",
            "Epoch 347 / 1000\n",
            "1/1 [===================>] loss: 0.1593676980064915 ; acc: 0.6\n",
            "Epoch 348 / 1000\n",
            "1/1 [===================>] loss: 0.1592482504713146 ; acc: 0.6\n",
            "Epoch 349 / 1000\n",
            "1/1 [===================>] loss: 0.15912996373794913 ; acc: 0.6\n",
            "Epoch 350 / 1000\n",
            "1/1 [===================>] loss: 0.1590128215033222 ; acc: 0.6\n",
            "Epoch 351 / 1000\n",
            "1/1 [===================>] loss: 0.1588968077099788 ; acc: 0.6\n",
            "Epoch 352 / 1000\n",
            "1/1 [===================>] loss: 0.1587819065416883 ; acc: 0.6\n",
            "Epoch 353 / 1000\n",
            "1/1 [===================>] loss: 0.15866810241917445 ; acc: 0.6\n",
            "Epoch 354 / 1000\n",
            "1/1 [===================>] loss: 0.15855537999596617 ; acc: 0.6\n",
            "Epoch 355 / 1000\n",
            "1/1 [===================>] loss: 0.15844372415436064 ; acc: 0.6\n",
            "Epoch 356 / 1000\n",
            "1/1 [===================>] loss: 0.15833312000149574 ; acc: 0.6\n",
            "Epoch 357 / 1000\n",
            "1/1 [===================>] loss: 0.1582235528655259 ; acc: 0.6\n",
            "Epoch 358 / 1000\n",
            "1/1 [===================>] loss: 0.15811500829189595 ; acc: 0.6\n",
            "Epoch 359 / 1000\n",
            "1/1 [===================>] loss: 0.1580074720397104 ; acc: 0.6\n",
            "Epoch 360 / 1000\n",
            "1/1 [===================>] loss: 0.157900930078192 ; acc: 0.6\n",
            "Epoch 361 / 1000\n",
            "1/1 [===================>] loss: 0.15779536858322724 ; acc: 0.6\n",
            "Epoch 362 / 1000\n",
            "1/1 [===================>] loss: 0.1576907739339939 ; acc: 0.6\n",
            "Epoch 363 / 1000\n",
            "1/1 [===================>] loss: 0.15758713270966743 ; acc: 0.6\n",
            "Epoch 364 / 1000\n",
            "1/1 [===================>] loss: 0.15748443168620382 ; acc: 0.6\n",
            "Epoch 365 / 1000\n",
            "1/1 [===================>] loss: 0.15738265783319397 ; acc: 0.6\n",
            "Epoch 366 / 1000\n",
            "1/1 [===================>] loss: 0.15728179831078795 ; acc: 0.6\n",
            "Epoch 367 / 1000\n",
            "1/1 [===================>] loss: 0.15718184046668623 ; acc: 0.6\n",
            "Epoch 368 / 1000\n",
            "1/1 [===================>] loss: 0.1570827718331948 ; acc: 0.6\n",
            "Epoch 369 / 1000\n",
            "1/1 [===================>] loss: 0.15698458012434163 ; acc: 0.6\n",
            "Epoch 370 / 1000\n",
            "1/1 [===================>] loss: 0.1568872532330528 ; acc: 0.6\n",
            "Epoch 371 / 1000\n",
            "1/1 [===================>] loss: 0.15679077922838505 ; acc: 0.6\n",
            "Epoch 372 / 1000\n",
            "1/1 [===================>] loss: 0.15669514635281376 ; acc: 0.6\n",
            "Epoch 373 / 1000\n",
            "1/1 [===================>] loss: 0.15660034301957354 ; acc: 0.6\n",
            "Epoch 374 / 1000\n",
            "1/1 [===================>] loss: 0.15650635781004954 ; acc: 0.6\n",
            "Epoch 375 / 1000\n",
            "1/1 [===================>] loss: 0.15641317947121855 ; acc: 0.6\n",
            "Epoch 376 / 1000\n",
            "1/1 [===================>] loss: 0.15632079691313733 ; acc: 0.6\n",
            "Epoch 377 / 1000\n",
            "1/1 [===================>] loss: 0.1562291992064767 ; acc: 0.6\n",
            "Epoch 378 / 1000\n",
            "1/1 [===================>] loss: 0.1561383755801004 ; acc: 0.6\n",
            "Epoch 379 / 1000\n",
            "1/1 [===================>] loss: 0.15604831541868708 ; acc: 0.6\n",
            "Epoch 380 / 1000\n",
            "1/1 [===================>] loss: 0.15595900826039322 ; acc: 0.6\n",
            "Epoch 381 / 1000\n",
            "1/1 [===================>] loss: 0.15587044379455758 ; acc: 0.6\n",
            "Epoch 382 / 1000\n",
            "1/1 [===================>] loss: 0.15578261185944395 ; acc: 0.6\n",
            "Epoch 383 / 1000\n",
            "1/1 [===================>] loss: 0.15569550244002245 ; acc: 0.6\n",
            "Epoch 384 / 1000\n",
            "1/1 [===================>] loss: 0.1556091056657876 ; acc: 0.6\n",
            "Epoch 385 / 1000\n",
            "1/1 [===================>] loss: 0.1555234118086122 ; acc: 0.6\n",
            "Epoch 386 / 1000\n",
            "1/1 [===================>] loss: 0.15543841128063723 ; acc: 0.6\n",
            "Epoch 387 / 1000\n",
            "1/1 [===================>] loss: 0.15535409463219405 ; acc: 0.6\n",
            "Epoch 388 / 1000\n",
            "1/1 [===================>] loss: 0.15527045254976143 ; acc: 0.6\n",
            "Epoch 389 / 1000\n",
            "1/1 [===================>] loss: 0.15518747585395415 ; acc: 0.6\n",
            "Epoch 390 / 1000\n",
            "1/1 [===================>] loss: 0.15510515549754325 ; acc: 0.6\n",
            "Epoch 391 / 1000\n",
            "1/1 [===================>] loss: 0.15502348256350734 ; acc: 0.6\n",
            "Epoch 392 / 1000\n",
            "1/1 [===================>] loss: 0.15494244826311382 ; acc: 0.6\n",
            "Epoch 393 / 1000\n",
            "1/1 [===================>] loss: 0.15486204393402966 ; acc: 0.6\n",
            "Epoch 394 / 1000\n",
            "1/1 [===================>] loss: 0.15478226103846093 ; acc: 0.6\n",
            "Epoch 395 / 1000\n",
            "1/1 [===================>] loss: 0.1547030911613204 ; acc: 0.6\n",
            "Epoch 396 / 1000\n",
            "1/1 [===================>] loss: 0.15462452600842286 ; acc: 0.6\n",
            "Epoch 397 / 1000\n",
            "1/1 [===================>] loss: 0.15454655740470746 ; acc: 0.6\n",
            "Epoch 398 / 1000\n",
            "1/1 [===================>] loss: 0.15446917729248635 ; acc: 0.6\n",
            "Epoch 399 / 1000\n",
            "1/1 [===================>] loss: 0.1543923777297193 ; acc: 0.6\n",
            "Epoch 400 / 1000\n",
            "1/1 [===================>] loss: 0.1543161508883143 ; acc: 0.6\n",
            "Epoch 401 / 1000\n",
            "1/1 [===================>] loss: 0.15424048905245202 ; acc: 0.6\n",
            "Epoch 402 / 1000\n",
            "1/1 [===================>] loss: 0.15416538461693624 ; acc: 0.6\n",
            "Epoch 403 / 1000\n",
            "1/1 [===================>] loss: 0.1540908300855667 ; acc: 0.6\n",
            "Epoch 404 / 1000\n",
            "1/1 [===================>] loss: 0.15401681806953754 ; acc: 0.6\n",
            "Epoch 405 / 1000\n",
            "1/1 [===================>] loss: 0.15394334128585702 ; acc: 0.6\n",
            "Epoch 406 / 1000\n",
            "1/1 [===================>] loss: 0.15387039255579174 ; acc: 0.6\n",
            "Epoch 407 / 1000\n",
            "1/1 [===================>] loss: 0.15379796480333233 ; acc: 0.6\n",
            "Epoch 408 / 1000\n",
            "1/1 [===================>] loss: 0.15372605105368162 ; acc: 0.6\n",
            "Epoch 409 / 1000\n",
            "1/1 [===================>] loss: 0.1536546444317645 ; acc: 0.6\n",
            "Epoch 410 / 1000\n",
            "1/1 [===================>] loss: 0.15358373816075957 ; acc: 0.6\n",
            "Epoch 411 / 1000\n",
            "1/1 [===================>] loss: 0.15351332556065095 ; acc: 0.6\n",
            "Epoch 412 / 1000\n",
            "1/1 [===================>] loss: 0.15344340004680201 ; acc: 0.6\n",
            "Epoch 413 / 1000\n",
            "1/1 [===================>] loss: 0.1533739551285488 ; acc: 0.6\n",
            "Epoch 414 / 1000\n",
            "1/1 [===================>] loss: 0.15330498440781365 ; acc: 0.6\n",
            "Epoch 415 / 1000\n",
            "1/1 [===================>] loss: 0.15323648157773898 ; acc: 0.6\n",
            "Epoch 416 / 1000\n",
            "1/1 [===================>] loss: 0.1531684404213406 ; acc: 0.6\n",
            "Epoch 417 / 1000\n",
            "1/1 [===================>] loss: 0.15310085481017993 ; acc: 0.6\n",
            "Epoch 418 / 1000\n",
            "1/1 [===================>] loss: 0.15303371870305554 ; acc: 0.6\n",
            "Epoch 419 / 1000\n",
            "1/1 [===================>] loss: 0.1529670261447136 ; acc: 0.6\n",
            "Epoch 420 / 1000\n",
            "1/1 [===================>] loss: 0.1529007712645761 ; acc: 0.6\n",
            "Epoch 421 / 1000\n",
            "1/1 [===================>] loss: 0.1528349482754883 ; acc: 0.6\n",
            "Epoch 422 / 1000\n",
            "1/1 [===================>] loss: 0.1527695514724832 ; acc: 0.6\n",
            "Epoch 423 / 1000\n",
            "1/1 [===================>] loss: 0.15270457523156428 ; acc: 0.6\n",
            "Epoch 424 / 1000\n",
            "1/1 [===================>] loss: 0.15264001400850558 ; acc: 0.6\n",
            "Epoch 425 / 1000\n",
            "1/1 [===================>] loss: 0.152575862337669 ; acc: 0.6\n",
            "Epoch 426 / 1000\n",
            "1/1 [===================>] loss: 0.15251211483083882 ; acc: 0.6\n",
            "Epoch 427 / 1000\n",
            "1/1 [===================>] loss: 0.1524487661760723 ; acc: 0.6\n",
            "Epoch 428 / 1000\n",
            "1/1 [===================>] loss: 0.15238581113656796 ; acc: 0.6\n",
            "Epoch 429 / 1000\n",
            "1/1 [===================>] loss: 0.152323244549549 ; acc: 0.6\n",
            "Epoch 430 / 1000\n",
            "1/1 [===================>] loss: 0.15226106132516398 ; acc: 0.6\n",
            "Epoch 431 / 1000\n",
            "1/1 [===================>] loss: 0.15219925644540244 ; acc: 0.6\n",
            "Epoch 432 / 1000\n",
            "1/1 [===================>] loss: 0.15213782496302672 ; acc: 0.6\n",
            "Epoch 433 / 1000\n",
            "1/1 [===================>] loss: 0.15207676200051917 ; acc: 0.6\n",
            "Epoch 434 / 1000\n",
            "1/1 [===================>] loss: 0.15201606274904456 ; acc: 0.6\n",
            "Epoch 435 / 1000\n",
            "1/1 [===================>] loss: 0.15195572246742786 ; acc: 0.6\n",
            "Epoch 436 / 1000\n",
            "1/1 [===================>] loss: 0.15189573648114632 ; acc: 0.6\n",
            "Epoch 437 / 1000\n",
            "1/1 [===================>] loss: 0.1518361001813368 ; acc: 0.6\n",
            "Epoch 438 / 1000\n",
            "1/1 [===================>] loss: 0.15177680902381735 ; acc: 0.6\n",
            "Epoch 439 / 1000\n",
            "1/1 [===================>] loss: 0.15171785852812303 ; acc: 0.6\n",
            "Epoch 440 / 1000\n",
            "1/1 [===================>] loss: 0.15165924427655592 ; acc: 0.6\n",
            "Epoch 441 / 1000\n",
            "1/1 [===================>] loss: 0.15160096191324904 ; acc: 0.6\n",
            "Epoch 442 / 1000\n",
            "1/1 [===================>] loss: 0.15154300714324404 ; acc: 0.6\n",
            "Epoch 443 / 1000\n",
            "1/1 [===================>] loss: 0.15148537573158222 ; acc: 0.6\n",
            "Epoch 444 / 1000\n",
            "1/1 [===================>] loss: 0.1514280635024093 ; acc: 0.6\n",
            "Epoch 445 / 1000\n",
            "1/1 [===================>] loss: 0.151371066338093 ; acc: 0.6\n",
            "Epoch 446 / 1000\n",
            "1/1 [===================>] loss: 0.151314380178354 ; acc: 0.6\n",
            "Epoch 447 / 1000\n",
            "1/1 [===================>] loss: 0.15125800101940926 ; acc: 0.6\n",
            "Epoch 448 / 1000\n",
            "1/1 [===================>] loss: 0.15120192491312845 ; acc: 0.6\n",
            "Epoch 449 / 1000\n",
            "1/1 [===================>] loss: 0.15114614796620288 ; acc: 0.6\n",
            "Epoch 450 / 1000\n",
            "1/1 [===================>] loss: 0.15109066633932616 ; acc: 0.6\n",
            "Epoch 451 / 1000\n",
            "1/1 [===================>] loss: 0.15103547624638797 ; acc: 0.6\n",
            "Epoch 452 / 1000\n",
            "1/1 [===================>] loss: 0.1509805739536787 ; acc: 0.6\n",
            "Epoch 453 / 1000\n",
            "1/1 [===================>] loss: 0.15092595577910722 ; acc: 0.6\n",
            "Epoch 454 / 1000\n",
            "1/1 [===================>] loss: 0.15087161809142907 ; acc: 0.6\n",
            "Epoch 455 / 1000\n",
            "1/1 [===================>] loss: 0.15081755730948693 ; acc: 0.6\n",
            "Epoch 456 / 1000\n",
            "1/1 [===================>] loss: 0.15076376990146229 ; acc: 0.6\n",
            "Epoch 457 / 1000\n",
            "1/1 [===================>] loss: 0.1507102523841381 ; acc: 0.6\n",
            "Epoch 458 / 1000\n",
            "1/1 [===================>] loss: 0.1506570013221727 ; acc: 0.6\n",
            "Epoch 459 / 1000\n",
            "1/1 [===================>] loss: 0.15060401332738466 ; acc: 0.6\n",
            "Epoch 460 / 1000\n",
            "1/1 [===================>] loss: 0.1505512850580481 ; acc: 0.6\n",
            "Epoch 461 / 1000\n",
            "1/1 [===================>] loss: 0.15049881321819883 ; acc: 0.6\n",
            "Epoch 462 / 1000\n",
            "1/1 [===================>] loss: 0.1504465945569507 ; acc: 0.6\n",
            "Epoch 463 / 1000\n",
            "1/1 [===================>] loss: 0.1503946258678227 ; acc: 0.6\n",
            "Epoch 464 / 1000\n",
            "1/1 [===================>] loss: 0.15034290398807573 ; acc: 0.6\n",
            "Epoch 465 / 1000\n",
            "1/1 [===================>] loss: 0.15029142579805946 ; acc: 0.6\n",
            "Epoch 466 / 1000\n",
            "1/1 [===================>] loss: 0.1502401882205693 ; acc: 0.6\n",
            "Epoch 467 / 1000\n",
            "1/1 [===================>] loss: 0.15018918822021296 ; acc: 0.6\n",
            "Epoch 468 / 1000\n",
            "1/1 [===================>] loss: 0.1501384228027865 ; acc: 0.6\n",
            "Epoch 469 / 1000\n",
            "1/1 [===================>] loss: 0.15008788901465986 ; acc: 0.6\n",
            "Epoch 470 / 1000\n",
            "1/1 [===================>] loss: 0.15003758394217198 ; acc: 0.6\n",
            "Epoch 471 / 1000\n",
            "1/1 [===================>] loss: 0.14998750471103467 ; acc: 0.6\n",
            "Epoch 472 / 1000\n",
            "1/1 [===================>] loss: 0.14993764848574584 ; acc: 0.6\n",
            "Epoch 473 / 1000\n",
            "1/1 [===================>] loss: 0.14988801246901157 ; acc: 0.6\n",
            "Epoch 474 / 1000\n",
            "1/1 [===================>] loss: 0.14983859390117704 ; acc: 0.6\n",
            "Epoch 475 / 1000\n",
            "1/1 [===================>] loss: 0.14978939005966568 ; acc: 0.6\n",
            "Epoch 476 / 1000\n",
            "1/1 [===================>] loss: 0.14974039825842778 ; acc: 0.6\n",
            "Epoch 477 / 1000\n",
            "1/1 [===================>] loss: 0.1496916158473964 ; acc: 0.6\n",
            "Epoch 478 / 1000\n",
            "1/1 [===================>] loss: 0.14964304021195268 ; acc: 0.6\n",
            "Epoch 479 / 1000\n",
            "1/1 [===================>] loss: 0.1495946687723983 ; acc: 0.6\n",
            "Epoch 480 / 1000\n",
            "1/1 [===================>] loss: 0.14954649898343664 ; acc: 0.6\n",
            "Epoch 481 / 1000\n",
            "1/1 [===================>] loss: 0.14949852833366187 ; acc: 0.6\n",
            "Epoch 482 / 1000\n",
            "1/1 [===================>] loss: 0.14945075434505545 ; acc: 0.6\n",
            "Epoch 483 / 1000\n",
            "1/1 [===================>] loss: 0.14940317457249047 ; acc: 0.6\n",
            "Epoch 484 / 1000\n",
            "1/1 [===================>] loss: 0.1493557866032442 ; acc: 0.6\n",
            "Epoch 485 / 1000\n",
            "1/1 [===================>] loss: 0.14930858805651698 ; acc: 0.6\n",
            "Epoch 486 / 1000\n",
            "1/1 [===================>] loss: 0.14926157658295933 ; acc: 0.6\n",
            "Epoch 487 / 1000\n",
            "1/1 [===================>] loss: 0.14921474986420627 ; acc: 0.6\n",
            "Epoch 488 / 1000\n",
            "1/1 [===================>] loss: 0.14916810561241817 ; acc: 0.6\n",
            "Epoch 489 / 1000\n",
            "1/1 [===================>] loss: 0.14912164156982938 ; acc: 0.6\n",
            "Epoch 490 / 1000\n",
            "1/1 [===================>] loss: 0.1490753555083033 ; acc: 0.6\n",
            "Epoch 491 / 1000\n",
            "1/1 [===================>] loss: 0.1490292452288946 ; acc: 0.6\n",
            "Epoch 492 / 1000\n",
            "1/1 [===================>] loss: 0.14898330856141773 ; acc: 0.6\n",
            "Epoch 493 / 1000\n",
            "1/1 [===================>] loss: 0.14893754336402265 ; acc: 0.6\n",
            "Epoch 494 / 1000\n",
            "1/1 [===================>] loss: 0.1488919475227766 ; acc: 0.6\n",
            "Epoch 495 / 1000\n",
            "1/1 [===================>] loss: 0.1488465189512526 ; acc: 0.6\n",
            "Epoch 496 / 1000\n",
            "1/1 [===================>] loss: 0.14880125559012397 ; acc: 0.6\n",
            "Epoch 497 / 1000\n",
            "1/1 [===================>] loss: 0.14875615540676543 ; acc: 0.6\n",
            "Epoch 498 / 1000\n",
            "1/1 [===================>] loss: 0.14871121639486 ; acc: 0.6\n",
            "Epoch 499 / 1000\n",
            "1/1 [===================>] loss: 0.14866643657401252 ; acc: 0.6\n",
            "Epoch 500 / 1000\n",
            "1/1 [===================>] loss: 0.14862181398936808 ; acc: 0.6\n",
            "Epoch 501 / 1000\n",
            "1/1 [===================>] loss: 0.14857734671123773 ; acc: 0.6\n",
            "Epoch 502 / 1000\n",
            "1/1 [===================>] loss: 0.14853303283472852 ; acc: 0.6\n",
            "Epoch 503 / 1000\n",
            "1/1 [===================>] loss: 0.1484888704793802 ; acc: 0.6\n",
            "Epoch 504 / 1000\n",
            "1/1 [===================>] loss: 0.14844485778880737 ; acc: 0.6\n",
            "Epoch 505 / 1000\n",
            "1/1 [===================>] loss: 0.14840099293034673 ; acc: 0.6\n",
            "Epoch 506 / 1000\n",
            "1/1 [===================>] loss: 0.14835727409471006 ; acc: 0.6\n",
            "Epoch 507 / 1000\n",
            "1/1 [===================>] loss: 0.1483136994956427 ; acc: 0.6\n",
            "Epoch 508 / 1000\n",
            "1/1 [===================>] loss: 0.14827026736958682 ; acc: 0.6\n",
            "Epoch 509 / 1000\n",
            "1/1 [===================>] loss: 0.14822697597535048 ; acc: 0.6\n",
            "Epoch 510 / 1000\n",
            "1/1 [===================>] loss: 0.1481838235937811 ; acc: 0.6\n",
            "Epoch 511 / 1000\n",
            "1/1 [===================>] loss: 0.1481408085274446 ; acc: 0.6\n",
            "Epoch 512 / 1000\n",
            "1/1 [===================>] loss: 0.14809792910030903 ; acc: 0.6\n",
            "Epoch 513 / 1000\n",
            "1/1 [===================>] loss: 0.14805518365743334 ; acc: 0.6\n",
            "Epoch 514 / 1000\n",
            "1/1 [===================>] loss: 0.14801257056466083 ; acc: 0.6\n",
            "Epoch 515 / 1000\n",
            "1/1 [===================>] loss: 0.14797008820831714 ; acc: 0.6\n",
            "Epoch 516 / 1000\n",
            "1/1 [===================>] loss: 0.14792773499491346 ; acc: 0.6\n",
            "Epoch 517 / 1000\n",
            "1/1 [===================>] loss: 0.14788550935085348 ; acc: 0.6\n",
            "Epoch 518 / 1000\n",
            "1/1 [===================>] loss: 0.14784340972214566 ; acc: 0.6\n",
            "Epoch 519 / 1000\n",
            "1/1 [===================>] loss: 0.14780143457411937 ; acc: 0.6\n",
            "Epoch 520 / 1000\n",
            "1/1 [===================>] loss: 0.14775958239114553 ; acc: 0.6\n",
            "Epoch 521 / 1000\n",
            "1/1 [===================>] loss: 0.1477178516763618 ; acc: 0.6\n",
            "Epoch 522 / 1000\n",
            "1/1 [===================>] loss: 0.1476762409514017 ; acc: 0.6\n",
            "Epoch 523 / 1000\n",
            "1/1 [===================>] loss: 0.1476347487561279 ; acc: 0.6\n",
            "Epoch 524 / 1000\n",
            "1/1 [===================>] loss: 0.14759337364836977 ; acc: 0.6\n",
            "Epoch 525 / 1000\n",
            "1/1 [===================>] loss: 0.14755211420366504 ; acc: 0.6\n",
            "Epoch 526 / 1000\n",
            "1/1 [===================>] loss: 0.14751096901500493 ; acc: 0.6\n",
            "Epoch 527 / 1000\n",
            "1/1 [===================>] loss: 0.1474699366925838 ; acc: 0.6\n",
            "Epoch 528 / 1000\n",
            "1/1 [===================>] loss: 0.14742901586355245 ; acc: 0.6\n",
            "Epoch 529 / 1000\n",
            "1/1 [===================>] loss: 0.14738820517177484 ; acc: 0.6\n",
            "Epoch 530 / 1000\n",
            "1/1 [===================>] loss: 0.14734750327758914 ; acc: 0.6\n",
            "Epoch 531 / 1000\n",
            "1/1 [===================>] loss: 0.14730690885757192 ; acc: 0.6\n",
            "Epoch 532 / 1000\n",
            "1/1 [===================>] loss: 0.14726642060430628 ; acc: 0.6\n",
            "Epoch 533 / 1000\n",
            "1/1 [===================>] loss: 0.14722603722615354 ; acc: 0.6\n",
            "Epoch 534 / 1000\n",
            "1/1 [===================>] loss: 0.14718575744702803 ; acc: 0.6\n",
            "Epoch 535 / 1000\n",
            "1/1 [===================>] loss: 0.1471455800061759 ; acc: 0.6\n",
            "Epoch 536 / 1000\n",
            "1/1 [===================>] loss: 0.14710550365795688 ; acc: 0.6\n",
            "Epoch 537 / 1000\n",
            "1/1 [===================>] loss: 0.14706552717162952 ; acc: 0.6\n",
            "Epoch 538 / 1000\n",
            "1/1 [===================>] loss: 0.14702564933113998 ; acc: 0.6\n",
            "Epoch 539 / 1000\n",
            "1/1 [===================>] loss: 0.14698586893491322 ; acc: 0.6\n",
            "Epoch 540 / 1000\n",
            "1/1 [===================>] loss: 0.1469461847956487 ; acc: 0.6\n",
            "Epoch 541 / 1000\n",
            "1/1 [===================>] loss: 0.14690659574011775 ; acc: 0.6\n",
            "Epoch 542 / 1000\n",
            "1/1 [===================>] loss: 0.14686710060896532 ; acc: 0.6\n",
            "Epoch 543 / 1000\n",
            "1/1 [===================>] loss: 0.14682769825651365 ; acc: 0.6\n",
            "Epoch 544 / 1000\n",
            "1/1 [===================>] loss: 0.14678838755056997 ; acc: 0.6\n",
            "Epoch 545 / 1000\n",
            "1/1 [===================>] loss: 0.1467491673722362 ; acc: 0.6\n",
            "Epoch 546 / 1000\n",
            "1/1 [===================>] loss: 0.14671003661572213 ; acc: 0.6\n",
            "Epoch 547 / 1000\n",
            "1/1 [===================>] loss: 0.14667099418816124 ; acc: 0.6\n",
            "Epoch 548 / 1000\n",
            "1/1 [===================>] loss: 0.14663203900942962 ; acc: 0.6\n",
            "Epoch 549 / 1000\n",
            "1/1 [===================>] loss: 0.1465931700119668 ; acc: 0.6\n",
            "Epoch 550 / 1000\n",
            "1/1 [===================>] loss: 0.14655438614060057 ; acc: 0.6\n",
            "Epoch 551 / 1000\n",
            "1/1 [===================>] loss: 0.14651568635237333 ; acc: 0.6\n",
            "Epoch 552 / 1000\n",
            "1/1 [===================>] loss: 0.14647706961637166 ; acc: 0.6\n",
            "Epoch 553 / 1000\n",
            "1/1 [===================>] loss: 0.1464385349135584 ; acc: 0.6\n",
            "Epoch 554 / 1000\n",
            "1/1 [===================>] loss: 0.14640008123660714 ; acc: 0.6\n",
            "Epoch 555 / 1000\n",
            "1/1 [===================>] loss: 0.1463617075897394 ; acc: 0.6\n",
            "Epoch 556 / 1000\n",
            "1/1 [===================>] loss: 0.14632341298856408 ; acc: 0.6\n",
            "Epoch 557 / 1000\n",
            "1/1 [===================>] loss: 0.14628519645991958 ; acc: 0.6\n",
            "Epoch 558 / 1000\n",
            "1/1 [===================>] loss: 0.14624705704171795 ; acc: 0.6\n",
            "Epoch 559 / 1000\n",
            "1/1 [===================>] loss: 0.14620899378279179 ; acc: 0.6\n",
            "Epoch 560 / 1000\n",
            "1/1 [===================>] loss: 0.1461710057427434 ; acc: 0.6\n",
            "Epoch 561 / 1000\n",
            "1/1 [===================>] loss: 0.14613309199179575 ; acc: 0.6\n",
            "Epoch 562 / 1000\n",
            "1/1 [===================>] loss: 0.1460952516106464 ; acc: 0.6\n",
            "Epoch 563 / 1000\n",
            "1/1 [===================>] loss: 0.14605748369032312 ; acc: 0.6\n",
            "Epoch 564 / 1000\n",
            "1/1 [===================>] loss: 0.14601978733204193 ; acc: 0.6\n",
            "Epoch 565 / 1000\n",
            "1/1 [===================>] loss: 0.14598216164706695 ; acc: 0.6\n",
            "Epoch 566 / 1000\n",
            "1/1 [===================>] loss: 0.14594460575657295 ; acc: 0.6\n",
            "Epoch 567 / 1000\n",
            "1/1 [===================>] loss: 0.14590711879150944 ; acc: 0.6\n",
            "Epoch 568 / 1000\n",
            "1/1 [===================>] loss: 0.14586969989246698 ; acc: 0.6\n",
            "Epoch 569 / 1000\n",
            "1/1 [===================>] loss: 0.14583234820954555 ; acc: 0.6\n",
            "Epoch 570 / 1000\n",
            "1/1 [===================>] loss: 0.14579506290222488 ; acc: 0.6\n",
            "Epoch 571 / 1000\n",
            "1/1 [===================>] loss: 0.14575784313923662 ; acc: 0.6\n",
            "Epoch 572 / 1000\n",
            "1/1 [===================>] loss: 0.14572068809843858 ; acc: 0.6\n",
            "Epoch 573 / 1000\n",
            "1/1 [===================>] loss: 0.1456835969666908 ; acc: 0.6\n",
            "Epoch 574 / 1000\n",
            "1/1 [===================>] loss: 0.14564656893973343 ; acc: 0.6\n",
            "Epoch 575 / 1000\n",
            "1/1 [===================>] loss: 0.1456096032220663 ; acc: 0.6\n",
            "Epoch 576 / 1000\n",
            "1/1 [===================>] loss: 0.14557269902683093 ; acc: 0.6\n",
            "Epoch 577 / 1000\n",
            "1/1 [===================>] loss: 0.1455358555756933 ; acc: 0.6\n",
            "Epoch 578 / 1000\n",
            "1/1 [===================>] loss: 0.14549907209872928 ; acc: 0.6\n",
            "Epoch 579 / 1000\n",
            "1/1 [===================>] loss: 0.14546234783431122 ; acc: 0.6\n",
            "Epoch 580 / 1000\n",
            "1/1 [===================>] loss: 0.1454256820289965 ; acc: 0.6\n",
            "Epoch 581 / 1000\n",
            "1/1 [===================>] loss: 0.14538907393741757 ; acc: 0.6\n",
            "Epoch 582 / 1000\n",
            "1/1 [===================>] loss: 0.14535252282217379 ; acc: 0.6\n",
            "Epoch 583 / 1000\n",
            "1/1 [===================>] loss: 0.14531602795372478 ; acc: 0.6\n",
            "Epoch 584 / 1000\n",
            "1/1 [===================>] loss: 0.1452795886102854 ; acc: 0.6\n",
            "Epoch 585 / 1000\n",
            "1/1 [===================>] loss: 0.14524320407772223 ; acc: 0.6\n",
            "Epoch 586 / 1000\n",
            "1/1 [===================>] loss: 0.14520687364945156 ; acc: 0.6\n",
            "Epoch 587 / 1000\n",
            "1/1 [===================>] loss: 0.1451705966263393 ; acc: 0.6\n",
            "Epoch 588 / 1000\n",
            "1/1 [===================>] loss: 0.1451343723166016 ; acc: 0.6\n",
            "Epoch 589 / 1000\n",
            "1/1 [===================>] loss: 0.14509820003570761 ; acc: 0.6\n",
            "Epoch 590 / 1000\n",
            "1/1 [===================>] loss: 0.1450620791062835 ; acc: 0.6\n",
            "Epoch 591 / 1000\n",
            "1/1 [===================>] loss: 0.14502600885801775 ; acc: 0.6\n",
            "Epoch 592 / 1000\n",
            "1/1 [===================>] loss: 0.14498998862756807 ; acc: 0.6\n",
            "Epoch 593 / 1000\n",
            "1/1 [===================>] loss: 0.14495401775846936 ; acc: 0.6\n",
            "Epoch 594 / 1000\n",
            "1/1 [===================>] loss: 0.14491809560104355 ; acc: 0.6\n",
            "Epoch 595 / 1000\n",
            "1/1 [===================>] loss: 0.14488222151231017 ; acc: 0.6\n",
            "Epoch 596 / 1000\n",
            "1/1 [===================>] loss: 0.1448463948558988 ; acc: 0.6\n",
            "Epoch 597 / 1000\n",
            "1/1 [===================>] loss: 0.14481061500196232 ; acc: 0.6\n",
            "Epoch 598 / 1000\n",
            "1/1 [===================>] loss: 0.14477488132709188 ; acc: 0.6\n",
            "Epoch 599 / 1000\n",
            "1/1 [===================>] loss: 0.14473919321423265 ; acc: 0.6\n",
            "Epoch 600 / 1000\n",
            "1/1 [===================>] loss: 0.14470355005260135 ; acc: 0.6\n",
            "Epoch 601 / 1000\n",
            "1/1 [===================>] loss: 0.14466795123760468 ; acc: 0.6\n",
            "Epoch 602 / 1000\n",
            "1/1 [===================>] loss: 0.14463239617075865 ; acc: 0.6\n",
            "Epoch 603 / 1000\n",
            "1/1 [===================>] loss: 0.14459688425960984 ; acc: 0.6\n",
            "Epoch 604 / 1000\n",
            "1/1 [===================>] loss: 0.14456141491765723 ; acc: 0.6\n",
            "Epoch 605 / 1000\n",
            "1/1 [===================>] loss: 0.14452598756427526 ; acc: 0.6\n",
            "Epoch 606 / 1000\n",
            "1/1 [===================>] loss: 0.14449060162463828 ; acc: 0.6\n",
            "Epoch 607 / 1000\n",
            "1/1 [===================>] loss: 0.1444552565296458 ; acc: 0.6\n",
            "Epoch 608 / 1000\n",
            "1/1 [===================>] loss: 0.14441995171584915 ; acc: 0.6\n",
            "Epoch 609 / 1000\n",
            "1/1 [===================>] loss: 0.1443846866253788 ; acc: 0.6\n",
            "Epoch 610 / 1000\n",
            "1/1 [===================>] loss: 0.14434946070587293 ; acc: 0.6\n",
            "Epoch 611 / 1000\n",
            "1/1 [===================>] loss: 0.14431427341040753 ; acc: 0.6\n",
            "Epoch 612 / 1000\n",
            "1/1 [===================>] loss: 0.14427912419742656 ; acc: 0.6\n",
            "Epoch 613 / 1000\n",
            "1/1 [===================>] loss: 0.14424401253067407 ; acc: 0.6\n",
            "Epoch 614 / 1000\n",
            "1/1 [===================>] loss: 0.14420893787912642 ; acc: 0.6\n",
            "Epoch 615 / 1000\n",
            "1/1 [===================>] loss: 0.14417389971692646 ; acc: 0.6\n",
            "Epoch 616 / 1000\n",
            "1/1 [===================>] loss: 0.14413889752331777 ; acc: 0.6\n",
            "Epoch 617 / 1000\n",
            "1/1 [===================>] loss: 0.14410393078258044 ; acc: 0.6\n",
            "Epoch 618 / 1000\n",
            "1/1 [===================>] loss: 0.14406899898396755 ; acc: 0.6\n",
            "Epoch 619 / 1000\n",
            "1/1 [===================>] loss: 0.14403410162164254 ; acc: 0.6\n",
            "Epoch 620 / 1000\n",
            "1/1 [===================>] loss: 0.1439992381946178 ; acc: 0.6\n",
            "Epoch 621 / 1000\n",
            "1/1 [===================>] loss: 0.14397276689430102 ; acc: 0.6\n",
            "Epoch 622 / 1000\n",
            "1/1 [===================>] loss: 0.1439544178912804 ; acc: 0.6\n",
            "Epoch 623 / 1000\n",
            "1/1 [===================>] loss: 0.14393612803024916 ; acc: 0.6\n",
            "Epoch 624 / 1000\n",
            "1/1 [===================>] loss: 0.14391789683862527 ; acc: 0.6\n",
            "Epoch 625 / 1000\n",
            "1/1 [===================>] loss: 0.1438997238490371 ; acc: 0.6\n",
            "Epoch 626 / 1000\n",
            "1/1 [===================>] loss: 0.14388160859925636 ; acc: 0.6\n",
            "Epoch 627 / 1000\n",
            "1/1 [===================>] loss: 0.14386355063213216 ; acc: 0.6\n",
            "Epoch 628 / 1000\n",
            "1/1 [===================>] loss: 0.14384554949552586 ; acc: 0.6\n",
            "Epoch 629 / 1000\n",
            "1/1 [===================>] loss: 0.14382760474224696 ; acc: 0.6\n",
            "Epoch 630 / 1000\n",
            "1/1 [===================>] loss: 0.14380971592998962 ; acc: 0.6\n",
            "Epoch 631 / 1000\n",
            "1/1 [===================>] loss: 0.14379188262127024 ; acc: 0.6\n",
            "Epoch 632 / 1000\n",
            "1/1 [===================>] loss: 0.14377410438336607 ; acc: 0.6\n",
            "Epoch 633 / 1000\n",
            "1/1 [===================>] loss: 0.1437563807882542 ; acc: 0.6\n",
            "Epoch 634 / 1000\n",
            "1/1 [===================>] loss: 0.14373871141255173 ; acc: 0.6\n",
            "Epoch 635 / 1000\n",
            "1/1 [===================>] loss: 0.1437210958374569 ; acc: 0.6\n",
            "Epoch 636 / 1000\n",
            "1/1 [===================>] loss: 0.14370353364869048 ; acc: 0.6\n",
            "Epoch 637 / 1000\n",
            "1/1 [===================>] loss: 0.1436860244364385 ; acc: 0.6\n",
            "Epoch 638 / 1000\n",
            "1/1 [===================>] loss: 0.14366856779529552 ; acc: 0.6\n",
            "Epoch 639 / 1000\n",
            "1/1 [===================>] loss: 0.1436511633242088 ; acc: 0.6\n",
            "Epoch 640 / 1000\n",
            "1/1 [===================>] loss: 0.14363381062642308 ; acc: 0.6\n",
            "Epoch 641 / 1000\n",
            "1/1 [===================>] loss: 0.14361650930942618 ; acc: 0.6\n",
            "Epoch 642 / 1000\n",
            "1/1 [===================>] loss: 0.14359925898489545 ; acc: 0.6\n",
            "Epoch 643 / 1000\n",
            "1/1 [===================>] loss: 0.14358205926864473 ; acc: 0.6\n",
            "Epoch 644 / 1000\n",
            "1/1 [===================>] loss: 0.1435649097805724 ; acc: 0.6\n",
            "Epoch 645 / 1000\n",
            "1/1 [===================>] loss: 0.14354781014460968 ; acc: 0.6\n",
            "Epoch 646 / 1000\n",
            "1/1 [===================>] loss: 0.14353075998867 ; acc: 0.6\n",
            "Epoch 647 / 1000\n",
            "1/1 [===================>] loss: 0.14351375894459878 ; acc: 0.6\n",
            "Epoch 648 / 1000\n",
            "1/1 [===================>] loss: 0.14349680664812448 ; acc: 0.6\n",
            "Epoch 649 / 1000\n",
            "1/1 [===================>] loss: 0.1434799027388092 ; acc: 0.6\n",
            "Epoch 650 / 1000\n",
            "1/1 [===================>] loss: 0.14346304686000125 ; acc: 0.6\n",
            "Epoch 651 / 1000\n",
            "1/1 [===================>] loss: 0.14344623865878733 ; acc: 0.6\n",
            "Epoch 652 / 1000\n",
            "1/1 [===================>] loss: 0.1434294777859462 ; acc: 0.6\n",
            "Epoch 653 / 1000\n",
            "1/1 [===================>] loss: 0.14341276389590205 ; acc: 0.6\n",
            "Epoch 654 / 1000\n",
            "1/1 [===================>] loss: 0.14339609664667954 ; acc: 0.6\n",
            "Epoch 655 / 1000\n",
            "1/1 [===================>] loss: 0.1433794756998585 ; acc: 0.6\n",
            "Epoch 656 / 1000\n",
            "1/1 [===================>] loss: 0.14336290072053004 ; acc: 0.6\n",
            "Epoch 657 / 1000\n",
            "1/1 [===================>] loss: 0.14334637137725253 ; acc: 0.6\n",
            "Epoch 658 / 1000\n",
            "1/1 [===================>] loss: 0.1433298873420088 ; acc: 0.6\n",
            "Epoch 659 / 1000\n",
            "1/1 [===================>] loss: 0.14331344829016365 ; acc: 0.6\n",
            "Epoch 660 / 1000\n",
            "1/1 [===================>] loss: 0.14329705390042177 ; acc: 0.6\n",
            "Epoch 661 / 1000\n",
            "1/1 [===================>] loss: 0.14328070385478647 ; acc: 0.6\n",
            "Epoch 662 / 1000\n",
            "1/1 [===================>] loss: 0.14326439783851913 ; acc: 0.6\n",
            "Epoch 663 / 1000\n",
            "1/1 [===================>] loss: 0.14324813554009846 ; acc: 0.6\n",
            "Epoch 664 / 1000\n",
            "1/1 [===================>] loss: 0.14323191665118146 ; acc: 0.6\n",
            "Epoch 665 / 1000\n",
            "1/1 [===================>] loss: 0.1432157408665637 ; acc: 0.6\n",
            "Epoch 666 / 1000\n",
            "1/1 [===================>] loss: 0.14319960788414102 ; acc: 0.6\n",
            "Epoch 667 / 1000\n",
            "1/1 [===================>] loss: 0.14318351740487129 ; acc: 0.6\n",
            "Epoch 668 / 1000\n",
            "1/1 [===================>] loss: 0.143167469132737 ; acc: 0.6\n",
            "Epoch 669 / 1000\n",
            "1/1 [===================>] loss: 0.1431514627747079 ; acc: 0.6\n",
            "Epoch 670 / 1000\n",
            "1/1 [===================>] loss: 0.14313549804070466 ; acc: 0.6\n",
            "Epoch 671 / 1000\n",
            "1/1 [===================>] loss: 0.14311957464356265 ; acc: 0.6\n",
            "Epoch 672 / 1000\n",
            "1/1 [===================>] loss: 0.1431036922989963 ; acc: 0.6\n",
            "Epoch 673 / 1000\n",
            "1/1 [===================>] loss: 0.14308785072556418 ; acc: 0.6\n",
            "Epoch 674 / 1000\n",
            "1/1 [===================>] loss: 0.14307204964463416 ; acc: 0.6\n",
            "Epoch 675 / 1000\n",
            "1/1 [===================>] loss: 0.14305628878034915 ; acc: 0.6\n",
            "Epoch 676 / 1000\n",
            "1/1 [===================>] loss: 0.1430405678595935 ; acc: 0.6\n",
            "Epoch 677 / 1000\n",
            "1/1 [===================>] loss: 0.1430248866119598 ; acc: 0.6\n",
            "Epoch 678 / 1000\n",
            "1/1 [===================>] loss: 0.1430092447697158 ; acc: 0.6\n",
            "Epoch 679 / 1000\n",
            "1/1 [===================>] loss: 0.14299364206777213 ; acc: 0.6\n",
            "Epoch 680 / 1000\n",
            "1/1 [===================>] loss: 0.14297807824365033 ; acc: 0.6\n",
            "Epoch 681 / 1000\n",
            "1/1 [===================>] loss: 0.14296255303745148 ; acc: 0.6\n",
            "Epoch 682 / 1000\n",
            "1/1 [===================>] loss: 0.14294706619182468 ; acc: 0.6\n",
            "Epoch 683 / 1000\n",
            "1/1 [===================>] loss: 0.14293161745193672 ; acc: 0.6\n",
            "Epoch 684 / 1000\n",
            "1/1 [===================>] loss: 0.1429162065654414 ; acc: 0.6\n",
            "Epoch 685 / 1000\n",
            "1/1 [===================>] loss: 0.14290083328245026 ; acc: 0.6\n",
            "Epoch 686 / 1000\n",
            "1/1 [===================>] loss: 0.14288549735550232 ; acc: 0.6\n",
            "Epoch 687 / 1000\n",
            "1/1 [===================>] loss: 0.14287019853953548 ; acc: 0.6\n",
            "Epoch 688 / 1000\n",
            "1/1 [===================>] loss: 0.14285493659185772 ; acc: 0.6\n",
            "Epoch 689 / 1000\n",
            "1/1 [===================>] loss: 0.1428397112721186 ; acc: 0.6\n",
            "Epoch 690 / 1000\n",
            "1/1 [===================>] loss: 0.1428245223422815 ; acc: 0.6\n",
            "Epoch 691 / 1000\n",
            "1/1 [===================>] loss: 0.14280936956659573 ; acc: 0.6\n",
            "Epoch 692 / 1000\n",
            "1/1 [===================>] loss: 0.1427942527115697 ; acc: 0.6\n",
            "Epoch 693 / 1000\n",
            "1/1 [===================>] loss: 0.14277917154594374 ; acc: 0.6\n",
            "Epoch 694 / 1000\n",
            "1/1 [===================>] loss: 0.14276412584066375 ; acc: 0.6\n",
            "Epoch 695 / 1000\n",
            "1/1 [===================>] loss: 0.14274911536885493 ; acc: 0.6\n",
            "Epoch 696 / 1000\n",
            "1/1 [===================>] loss: 0.14273413990579595 ; acc: 0.6\n",
            "Epoch 697 / 1000\n",
            "1/1 [===================>] loss: 0.14271919922889367 ; acc: 0.6\n",
            "Epoch 698 / 1000\n",
            "1/1 [===================>] loss: 0.14270429311765767 ; acc: 0.6\n",
            "Epoch 699 / 1000\n",
            "1/1 [===================>] loss: 0.14268942135367582 ; acc: 0.6\n",
            "Epoch 700 / 1000\n",
            "1/1 [===================>] loss: 0.14267458372058942 ; acc: 0.6\n",
            "Epoch 701 / 1000\n",
            "1/1 [===================>] loss: 0.1426597800040694 ; acc: 0.6\n",
            "Epoch 702 / 1000\n",
            "1/1 [===================>] loss: 0.14264500999179203 ; acc: 0.6\n",
            "Epoch 703 / 1000\n",
            "1/1 [===================>] loss: 0.14263027347341578 ; acc: 0.6\n",
            "Epoch 704 / 1000\n",
            "1/1 [===================>] loss: 0.14261557024055804 ; acc: 0.6\n",
            "Epoch 705 / 1000\n",
            "1/1 [===================>] loss: 0.14260090008677187 ; acc: 0.6\n",
            "Epoch 706 / 1000\n",
            "1/1 [===================>] loss: 0.14258626280752384 ; acc: 0.6\n",
            "Epoch 707 / 1000\n",
            "1/1 [===================>] loss: 0.14257165820017131 ; acc: 0.6\n",
            "Epoch 708 / 1000\n",
            "1/1 [===================>] loss: 0.14255708606394066 ; acc: 0.6\n",
            "Epoch 709 / 1000\n",
            "1/1 [===================>] loss: 0.1425425461999053 ; acc: 0.6\n",
            "Epoch 710 / 1000\n",
            "1/1 [===================>] loss: 0.1425280384109646 ; acc: 0.6\n",
            "Epoch 711 / 1000\n",
            "1/1 [===================>] loss: 0.14251356250182218 ; acc: 0.6\n",
            "Epoch 712 / 1000\n",
            "1/1 [===================>] loss: 0.14249911827896536 ; acc: 0.6\n",
            "Epoch 713 / 1000\n",
            "1/1 [===================>] loss: 0.14248470555064452 ; acc: 0.6\n",
            "Epoch 714 / 1000\n",
            "1/1 [===================>] loss: 0.1424703241268525 ; acc: 0.6\n",
            "Epoch 715 / 1000\n",
            "1/1 [===================>] loss: 0.1424559738193048 ; acc: 0.6\n",
            "Epoch 716 / 1000\n",
            "1/1 [===================>] loss: 0.14244165444141937 ; acc: 0.6\n",
            "Epoch 717 / 1000\n",
            "1/1 [===================>] loss: 0.14242736580829732 ; acc: 0.6\n",
            "Epoch 718 / 1000\n",
            "1/1 [===================>] loss: 0.1424131077367036 ; acc: 0.6\n",
            "Epoch 719 / 1000\n",
            "1/1 [===================>] loss: 0.14239888004504758 ; acc: 0.6\n",
            "Epoch 720 / 1000\n",
            "1/1 [===================>] loss: 0.14238468255336467 ; acc: 0.6\n",
            "Epoch 721 / 1000\n",
            "1/1 [===================>] loss: 0.1423705150832973 ; acc: 0.6\n",
            "Epoch 722 / 1000\n",
            "1/1 [===================>] loss: 0.1423563774580769 ; acc: 0.6\n",
            "Epoch 723 / 1000\n",
            "1/1 [===================>] loss: 0.14234226950250553 ; acc: 0.6\n",
            "Epoch 724 / 1000\n",
            "1/1 [===================>] loss: 0.1423281910429382 ; acc: 0.6\n",
            "Epoch 725 / 1000\n",
            "1/1 [===================>] loss: 0.1423141419072651 ; acc: 0.6\n",
            "Epoch 726 / 1000\n",
            "1/1 [===================>] loss: 0.1423001219248942 ; acc: 0.6\n",
            "Epoch 727 / 1000\n",
            "1/1 [===================>] loss: 0.14228613092673398 ; acc: 0.6\n",
            "Epoch 728 / 1000\n",
            "1/1 [===================>] loss: 0.14227216874517662 ; acc: 0.6\n",
            "Epoch 729 / 1000\n",
            "1/1 [===================>] loss: 0.14225823521408104 ; acc: 0.6\n",
            "Epoch 730 / 1000\n",
            "1/1 [===================>] loss: 0.1422443301687565 ; acc: 0.6\n",
            "Epoch 731 / 1000\n",
            "1/1 [===================>] loss: 0.14223045344594615 ; acc: 0.6\n",
            "Epoch 732 / 1000\n",
            "1/1 [===================>] loss: 0.14221660488381105 ; acc: 0.6\n",
            "Epoch 733 / 1000\n",
            "1/1 [===================>] loss: 0.14220278432191408 ; acc: 0.6\n",
            "Epoch 734 / 1000\n",
            "1/1 [===================>] loss: 0.14218899160120418 ; acc: 0.6\n",
            "Epoch 735 / 1000\n",
            "1/1 [===================>] loss: 0.1421752265640012 ; acc: 0.6\n",
            "Epoch 736 / 1000\n",
            "1/1 [===================>] loss: 0.14216148905397996 ; acc: 0.6\n",
            "Epoch 737 / 1000\n",
            "1/1 [===================>] loss: 0.1421477789161559 ; acc: 0.6\n",
            "Epoch 738 / 1000\n",
            "1/1 [===================>] loss: 0.14213409599686946 ; acc: 0.6\n",
            "Epoch 739 / 1000\n",
            "1/1 [===================>] loss: 0.14212044014377162 ; acc: 0.6\n",
            "Epoch 740 / 1000\n",
            "1/1 [===================>] loss: 0.14210681120580942 ; acc: 0.6\n",
            "Epoch 741 / 1000\n",
            "1/1 [===================>] loss: 0.1420932090332115 ; acc: 0.6\n",
            "Epoch 742 / 1000\n",
            "1/1 [===================>] loss: 0.14207963347747382 ; acc: 0.6\n",
            "Epoch 743 / 1000\n",
            "1/1 [===================>] loss: 0.14206608439134577 ; acc: 0.6\n",
            "Epoch 744 / 1000\n",
            "1/1 [===================>] loss: 0.14205256162881627 ; acc: 0.6\n",
            "Epoch 745 / 1000\n",
            "1/1 [===================>] loss: 0.14203906504510022 ; acc: 0.6\n",
            "Epoch 746 / 1000\n",
            "1/1 [===================>] loss: 0.1420255944966249 ; acc: 0.6\n",
            "Epoch 747 / 1000\n",
            "1/1 [===================>] loss: 0.14201214984101668 ; acc: 0.6\n",
            "Epoch 748 / 1000\n",
            "1/1 [===================>] loss: 0.14199873093708787 ; acc: 0.6\n",
            "Epoch 749 / 1000\n",
            "1/1 [===================>] loss: 0.14198533764482385 ; acc: 0.6\n",
            "Epoch 750 / 1000\n",
            "1/1 [===================>] loss: 0.14197196982537003 ; acc: 0.6\n",
            "Epoch 751 / 1000\n",
            "1/1 [===================>] loss: 0.14195862734101933 ; acc: 0.6\n",
            "Epoch 752 / 1000\n",
            "1/1 [===================>] loss: 0.1419453100551997 ; acc: 0.6\n",
            "Epoch 753 / 1000\n",
            "1/1 [===================>] loss: 0.14193201783246176 ; acc: 0.6\n",
            "Epoch 754 / 1000\n",
            "1/1 [===================>] loss: 0.14191875053846653 ; acc: 0.6\n",
            "Epoch 755 / 1000\n",
            "1/1 [===================>] loss: 0.14190550803997348 ; acc: 0.6\n",
            "Epoch 756 / 1000\n",
            "1/1 [===================>] loss: 0.1418922902048287 ; acc: 0.6\n",
            "Epoch 757 / 1000\n",
            "1/1 [===================>] loss: 0.14187909690195297 ; acc: 0.6\n",
            "Epoch 758 / 1000\n",
            "1/1 [===================>] loss: 0.1418659280013303 ; acc: 0.6\n",
            "Epoch 759 / 1000\n",
            "1/1 [===================>] loss: 0.14185278337399654 ; acc: 0.6\n",
            "Epoch 760 / 1000\n",
            "1/1 [===================>] loss: 0.14183966289202793 ; acc: 0.6\n",
            "Epoch 761 / 1000\n",
            "1/1 [===================>] loss: 0.14182656642853012 ; acc: 0.6\n",
            "Epoch 762 / 1000\n",
            "1/1 [===================>] loss: 0.14181349385762687 ; acc: 0.6\n",
            "Epoch 763 / 1000\n",
            "1/1 [===================>] loss: 0.14180044505444953 ; acc: 0.6\n",
            "Epoch 764 / 1000\n",
            "1/1 [===================>] loss: 0.1417874198951259 ; acc: 0.6\n",
            "Epoch 765 / 1000\n",
            "1/1 [===================>] loss: 0.14177441825677 ; acc: 0.6\n",
            "Epoch 766 / 1000\n",
            "1/1 [===================>] loss: 0.1417614400174712 ; acc: 0.6\n",
            "Epoch 767 / 1000\n",
            "1/1 [===================>] loss: 0.14174848505628418 ; acc: 0.6\n",
            "Epoch 768 / 1000\n",
            "1/1 [===================>] loss: 0.14173555325321846 ; acc: 0.6\n",
            "Epoch 769 / 1000\n",
            "1/1 [===================>] loss: 0.14172264448922856 ; acc: 0.6\n",
            "Epoch 770 / 1000\n",
            "1/1 [===================>] loss: 0.14170975864620367 ; acc: 0.6\n",
            "Epoch 771 / 1000\n",
            "1/1 [===================>] loss: 0.14169689560695814 ; acc: 0.6\n",
            "Epoch 772 / 1000\n",
            "1/1 [===================>] loss: 0.1416840552552216 ; acc: 0.6\n",
            "Epoch 773 / 1000\n",
            "1/1 [===================>] loss: 0.1416712374756292 ; acc: 0.6\n",
            "Epoch 774 / 1000\n",
            "1/1 [===================>] loss: 0.14165844215371243 ; acc: 0.6\n",
            "Epoch 775 / 1000\n",
            "1/1 [===================>] loss: 0.14164566917588944 ; acc: 0.6\n",
            "Epoch 776 / 1000\n",
            "1/1 [===================>] loss: 0.14163291842945597 ; acc: 0.6\n",
            "Epoch 777 / 1000\n",
            "1/1 [===================>] loss: 0.1416201898025761 ; acc: 0.6\n",
            "Epoch 778 / 1000\n",
            "1/1 [===================>] loss: 0.14160748318427327 ; acc: 0.6\n",
            "Epoch 779 / 1000\n",
            "1/1 [===================>] loss: 0.14159479846442125 ; acc: 0.6\n",
            "Epoch 780 / 1000\n",
            "1/1 [===================>] loss: 0.1415821355337355 ; acc: 0.6\n",
            "Epoch 781 / 1000\n",
            "1/1 [===================>] loss: 0.1415694942837641 ; acc: 0.6\n",
            "Epoch 782 / 1000\n",
            "1/1 [===================>] loss: 0.14155687460687985 ; acc: 0.6\n",
            "Epoch 783 / 1000\n",
            "1/1 [===================>] loss: 0.14154427639627085 ; acc: 0.6\n",
            "Epoch 784 / 1000\n",
            "1/1 [===================>] loss: 0.14153169954593275 ; acc: 0.6\n",
            "Epoch 785 / 1000\n",
            "1/1 [===================>] loss: 0.1415191439506602 ; acc: 0.6\n",
            "Epoch 786 / 1000\n",
            "1/1 [===================>] loss: 0.14150660950603888 ; acc: 0.6\n",
            "Epoch 787 / 1000\n",
            "1/1 [===================>] loss: 0.14149409610843705 ; acc: 0.6\n",
            "Epoch 788 / 1000\n",
            "1/1 [===================>] loss: 0.14148160365499782 ; acc: 0.6\n",
            "Epoch 789 / 1000\n",
            "1/1 [===================>] loss: 0.14146913204363112 ; acc: 0.6\n",
            "Epoch 790 / 1000\n",
            "1/1 [===================>] loss: 0.14145668117300605 ; acc: 0.6\n",
            "Epoch 791 / 1000\n",
            "1/1 [===================>] loss: 0.14144425094254293 ; acc: 0.6\n",
            "Epoch 792 / 1000\n",
            "1/1 [===================>] loss: 0.14143184125240574 ; acc: 0.6\n",
            "Epoch 793 / 1000\n",
            "1/1 [===================>] loss: 0.14141945200349498 ; acc: 0.6\n",
            "Epoch 794 / 1000\n",
            "1/1 [===================>] loss: 0.1414070830974396 ; acc: 0.6\n",
            "Epoch 795 / 1000\n",
            "1/1 [===================>] loss: 0.14139473443659006 ; acc: 0.6\n",
            "Epoch 796 / 1000\n",
            "1/1 [===================>] loss: 0.14138240592401097 ; acc: 0.6\n",
            "Epoch 797 / 1000\n",
            "1/1 [===================>] loss: 0.14137009746347398 ; acc: 0.6\n",
            "Epoch 798 / 1000\n",
            "1/1 [===================>] loss: 0.14135780895945052 ; acc: 0.6\n",
            "Epoch 799 / 1000\n",
            "1/1 [===================>] loss: 0.14134554031710497 ; acc: 0.6\n",
            "Epoch 800 / 1000\n",
            "1/1 [===================>] loss: 0.14133329144228765 ; acc: 0.6\n",
            "Epoch 801 / 1000\n",
            "1/1 [===================>] loss: 0.14132106224152813 ; acc: 0.6\n",
            "Epoch 802 / 1000\n",
            "1/1 [===================>] loss: 0.1413088526220281 ; acc: 0.6\n",
            "Epoch 803 / 1000\n",
            "1/1 [===================>] loss: 0.14129666249165523 ; acc: 0.6\n",
            "Epoch 804 / 1000\n",
            "1/1 [===================>] loss: 0.1412844917589361 ; acc: 0.6\n",
            "Epoch 805 / 1000\n",
            "1/1 [===================>] loss: 0.14127234033305 ; acc: 0.6\n",
            "Epoch 806 / 1000\n",
            "1/1 [===================>] loss: 0.14126020812382228 ; acc: 0.6\n",
            "Epoch 807 / 1000\n",
            "1/1 [===================>] loss: 0.14124809504171815 ; acc: 0.6\n",
            "Epoch 808 / 1000\n",
            "1/1 [===================>] loss: 0.14123600099783623 ; acc: 0.6\n",
            "Epoch 809 / 1000\n",
            "1/1 [===================>] loss: 0.14122392590390248 ; acc: 0.6\n",
            "Epoch 810 / 1000\n",
            "1/1 [===================>] loss: 0.14121186967226404 ; acc: 0.6\n",
            "Epoch 811 / 1000\n",
            "1/1 [===================>] loss: 0.14119983221588295 ; acc: 0.6\n",
            "Epoch 812 / 1000\n",
            "1/1 [===================>] loss: 0.14118781344833056 ; acc: 0.6\n",
            "Epoch 813 / 1000\n",
            "1/1 [===================>] loss: 0.1411758132837812 ; acc: 0.6\n",
            "Epoch 814 / 1000\n",
            "1/1 [===================>] loss: 0.14116383163700655 ; acc: 0.6\n",
            "Epoch 815 / 1000\n",
            "1/1 [===================>] loss: 0.1411518684233698 ; acc: 0.6\n",
            "Epoch 816 / 1000\n",
            "1/1 [===================>] loss: 0.14113992355881982 ; acc: 0.6\n",
            "Epoch 817 / 1000\n",
            "1/1 [===================>] loss: 0.14112799695988582 ; acc: 0.6\n",
            "Epoch 818 / 1000\n",
            "1/1 [===================>] loss: 0.14111608854367136 ; acc: 0.6\n",
            "Epoch 819 / 1000\n",
            "1/1 [===================>] loss: 0.14110419822784906 ; acc: 0.6\n",
            "Epoch 820 / 1000\n",
            "1/1 [===================>] loss: 0.1410923259306551 ; acc: 0.6\n",
            "Epoch 821 / 1000\n",
            "1/1 [===================>] loss: 0.14108047157088394 ; acc: 0.6\n",
            "Epoch 822 / 1000\n",
            "1/1 [===================>] loss: 0.14106863506788278 ; acc: 0.6\n",
            "Epoch 823 / 1000\n",
            "1/1 [===================>] loss: 0.1410568163415463 ; acc: 0.6\n",
            "Epoch 824 / 1000\n",
            "1/1 [===================>] loss: 0.14104501531231164 ; acc: 0.6\n",
            "Epoch 825 / 1000\n",
            "1/1 [===================>] loss: 0.14103323190115322 ; acc: 0.6\n",
            "Epoch 826 / 1000\n",
            "1/1 [===================>] loss: 0.14102146602957738 ; acc: 0.6\n",
            "Epoch 827 / 1000\n",
            "1/1 [===================>] loss: 0.14100971761961775 ; acc: 0.6\n",
            "Epoch 828 / 1000\n",
            "1/1 [===================>] loss: 0.14099798659382995 ; acc: 0.6\n",
            "Epoch 829 / 1000\n",
            "1/1 [===================>] loss: 0.14098627287528692 ; acc: 0.6\n",
            "Epoch 830 / 1000\n",
            "1/1 [===================>] loss: 0.14097457638757377 ; acc: 0.6\n",
            "Epoch 831 / 1000\n",
            "1/1 [===================>] loss: 0.14096289705478332 ; acc: 0.6\n",
            "Epoch 832 / 1000\n",
            "1/1 [===================>] loss: 0.1409512348015111 ; acc: 0.6\n",
            "Epoch 833 / 1000\n",
            "1/1 [===================>] loss: 0.1409395895528508 ; acc: 0.6\n",
            "Epoch 834 / 1000\n",
            "1/1 [===================>] loss: 0.1409279612343895 ; acc: 0.6\n",
            "Epoch 835 / 1000\n",
            "1/1 [===================>] loss: 0.1409163497722033 ; acc: 0.6\n",
            "Epoch 836 / 1000\n",
            "1/1 [===================>] loss: 0.14090475509285244 ; acc: 0.6\n",
            "Epoch 837 / 1000\n",
            "1/1 [===================>] loss: 0.1408931771233772 ; acc: 0.6\n",
            "Epoch 838 / 1000\n",
            "1/1 [===================>] loss: 0.14088161579129327 ; acc: 0.6\n",
            "Epoch 839 / 1000\n",
            "1/1 [===================>] loss: 0.14087007102458732 ; acc: 0.6\n",
            "Epoch 840 / 1000\n",
            "1/1 [===================>] loss: 0.14085854275171283 ; acc: 0.6\n",
            "Epoch 841 / 1000\n",
            "1/1 [===================>] loss: 0.14084703090158573 ; acc: 0.6\n",
            "Epoch 842 / 1000\n",
            "1/1 [===================>] loss: 0.14083553540358001 ; acc: 0.6\n",
            "Epoch 843 / 1000\n",
            "1/1 [===================>] loss: 0.1408240561875239 ; acc: 0.6\n",
            "Epoch 844 / 1000\n",
            "1/1 [===================>] loss: 0.14081259318369538 ; acc: 0.6\n",
            "Epoch 845 / 1000\n",
            "1/1 [===================>] loss: 0.14080114632281826 ; acc: 0.6\n",
            "Epoch 846 / 1000\n",
            "1/1 [===================>] loss: 0.14078971553605818 ; acc: 0.6\n",
            "Epoch 847 / 1000\n",
            "1/1 [===================>] loss: 0.14077830075501846 ; acc: 0.6\n",
            "Epoch 848 / 1000\n",
            "1/1 [===================>] loss: 0.14076690191173627 ; acc: 0.6\n",
            "Epoch 849 / 1000\n",
            "1/1 [===================>] loss: 0.1407555189386786 ; acc: 0.6\n",
            "Epoch 850 / 1000\n",
            "1/1 [===================>] loss: 0.14074415176873858 ; acc: 0.6\n",
            "Epoch 851 / 1000\n",
            "1/1 [===================>] loss: 0.14073280033523142 ; acc: 0.6\n",
            "Epoch 852 / 1000\n",
            "1/1 [===================>] loss: 0.14072146457189086 ; acc: 0.6\n",
            "Epoch 853 / 1000\n",
            "1/1 [===================>] loss: 0.14071014441286525 ; acc: 0.6\n",
            "Epoch 854 / 1000\n",
            "1/1 [===================>] loss: 0.14069883979271408 ; acc: 0.6\n",
            "Epoch 855 / 1000\n",
            "1/1 [===================>] loss: 0.14068755064640404 ; acc: 0.6\n",
            "Epoch 856 / 1000\n",
            "1/1 [===================>] loss: 0.14067627690930554 ; acc: 0.6\n",
            "Epoch 857 / 1000\n",
            "1/1 [===================>] loss: 0.1406650185171892 ; acc: 0.6\n",
            "Epoch 858 / 1000\n",
            "1/1 [===================>] loss: 0.14065377540622231 ; acc: 0.6\n",
            "Epoch 859 / 1000\n",
            "1/1 [===================>] loss: 0.14064254751296518 ; acc: 0.6\n",
            "Epoch 860 / 1000\n",
            "1/1 [===================>] loss: 0.1406313347743678 ; acc: 0.6\n",
            "Epoch 861 / 1000\n",
            "1/1 [===================>] loss: 0.14062013712776647 ; acc: 0.6\n",
            "Epoch 862 / 1000\n",
            "1/1 [===================>] loss: 0.14060895451088032 ; acc: 0.6\n",
            "Epoch 863 / 1000\n",
            "1/1 [===================>] loss: 0.14059778686180802 ; acc: 0.6\n",
            "Epoch 864 / 1000\n",
            "1/1 [===================>] loss: 0.1405866341190244 ; acc: 0.6\n",
            "Epoch 865 / 1000\n",
            "1/1 [===================>] loss: 0.1405754962213774 ; acc: 0.6\n",
            "Epoch 866 / 1000\n",
            "1/1 [===================>] loss: 0.14056437310808445 ; acc: 0.6\n",
            "Epoch 867 / 1000\n",
            "1/1 [===================>] loss: 0.14055326471872961 ; acc: 0.6\n",
            "Epoch 868 / 1000\n",
            "1/1 [===================>] loss: 0.14054217099326033 ; acc: 0.6\n",
            "Epoch 869 / 1000\n",
            "1/1 [===================>] loss: 0.14053109187198426 ; acc: 0.6\n",
            "Epoch 870 / 1000\n",
            "1/1 [===================>] loss: 0.14052002729556615 ; acc: 0.6\n",
            "Epoch 871 / 1000\n",
            "1/1 [===================>] loss: 0.14050897720502478 ; acc: 0.6\n",
            "Epoch 872 / 1000\n",
            "1/1 [===================>] loss: 0.14049794154173004 ; acc: 0.6\n",
            "Epoch 873 / 1000\n",
            "1/1 [===================>] loss: 0.1404869202473998 ; acc: 0.6\n",
            "Epoch 874 / 1000\n",
            "1/1 [===================>] loss: 0.14047591326409714 ; acc: 0.6\n",
            "Epoch 875 / 1000\n",
            "1/1 [===================>] loss: 0.14046492053422704 ; acc: 0.6\n",
            "Epoch 876 / 1000\n",
            "1/1 [===================>] loss: 0.14045394200053402 ; acc: 0.6\n",
            "Epoch 877 / 1000\n",
            "1/1 [===================>] loss: 0.14044297760609872 ; acc: 0.6\n",
            "Epoch 878 / 1000\n",
            "1/1 [===================>] loss: 0.14043202729433546 ; acc: 0.6\n",
            "Epoch 879 / 1000\n",
            "1/1 [===================>] loss: 0.1404210910089893 ; acc: 0.6\n",
            "Epoch 880 / 1000\n",
            "1/1 [===================>] loss: 0.14041016869413325 ; acc: 0.6\n",
            "Epoch 881 / 1000\n",
            "1/1 [===================>] loss: 0.14039926029416544 ; acc: 0.6\n",
            "Epoch 882 / 1000\n",
            "1/1 [===================>] loss: 0.1403883657538067 ; acc: 0.6\n",
            "Epoch 883 / 1000\n",
            "1/1 [===================>] loss: 0.1403774850180974 ; acc: 0.6\n",
            "Epoch 884 / 1000\n",
            "1/1 [===================>] loss: 0.1403666180323953 ; acc: 0.6\n",
            "Epoch 885 / 1000\n",
            "1/1 [===================>] loss: 0.1403557647423725 ; acc: 0.6\n",
            "Epoch 886 / 1000\n",
            "1/1 [===================>] loss: 0.14034492509401314 ; acc: 0.6\n",
            "Epoch 887 / 1000\n",
            "1/1 [===================>] loss: 0.1403340990336106 ; acc: 0.6\n",
            "Epoch 888 / 1000\n",
            "1/1 [===================>] loss: 0.14032328650776515 ; acc: 0.6\n",
            "Epoch 889 / 1000\n",
            "1/1 [===================>] loss: 0.14031248746338115 ; acc: 0.6\n",
            "Epoch 890 / 1000\n",
            "1/1 [===================>] loss: 0.14030170184766497 ; acc: 0.6\n",
            "Epoch 891 / 1000\n",
            "1/1 [===================>] loss: 0.14029092960812212 ; acc: 0.6\n",
            "Epoch 892 / 1000\n",
            "1/1 [===================>] loss: 0.140280170692555 ; acc: 0.6\n",
            "Epoch 893 / 1000\n",
            "1/1 [===================>] loss: 0.14026942504906045 ; acc: 0.6\n",
            "Epoch 894 / 1000\n",
            "1/1 [===================>] loss: 0.1402586926260275 ; acc: 0.6\n",
            "Epoch 895 / 1000\n",
            "1/1 [===================>] loss: 0.14024797337213463 ; acc: 0.6\n",
            "Epoch 896 / 1000\n",
            "1/1 [===================>] loss: 0.14023726723634783 ; acc: 0.6\n",
            "Epoch 897 / 1000\n",
            "1/1 [===================>] loss: 0.14022657416791812 ; acc: 0.6\n",
            "Epoch 898 / 1000\n",
            "1/1 [===================>] loss: 0.1402158941163792 ; acc: 0.6\n",
            "Epoch 899 / 1000\n",
            "1/1 [===================>] loss: 0.14020522703154517 ; acc: 0.6\n",
            "Epoch 900 / 1000\n",
            "1/1 [===================>] loss: 0.14019457286350845 ; acc: 0.6\n",
            "Epoch 901 / 1000\n",
            "1/1 [===================>] loss: 0.14018393156263756 ; acc: 0.6\n",
            "Epoch 902 / 1000\n",
            "1/1 [===================>] loss: 0.1401733030795746 ; acc: 0.6\n",
            "Epoch 903 / 1000\n",
            "1/1 [===================>] loss: 0.14016268736523343 ; acc: 0.6\n",
            "Epoch 904 / 1000\n",
            "1/1 [===================>] loss: 0.14015208437079735 ; acc: 0.6\n",
            "Epoch 905 / 1000\n",
            "1/1 [===================>] loss: 0.14014149404771692 ; acc: 0.6\n",
            "Epoch 906 / 1000\n",
            "1/1 [===================>] loss: 0.14013091634770808 ; acc: 0.6\n",
            "Epoch 907 / 1000\n",
            "1/1 [===================>] loss: 0.14012035122274985 ; acc: 0.6\n",
            "Epoch 908 / 1000\n",
            "1/1 [===================>] loss: 0.14010979862508227 ; acc: 0.6\n",
            "Epoch 909 / 1000\n",
            "1/1 [===================>] loss: 0.14009925850720448 ; acc: 0.6\n",
            "Epoch 910 / 1000\n",
            "1/1 [===================>] loss: 0.14008873082187256 ; acc: 0.6\n",
            "Epoch 911 / 1000\n",
            "1/1 [===================>] loss: 0.1400782155220977 ; acc: 0.6\n",
            "Epoch 912 / 1000\n",
            "1/1 [===================>] loss: 0.1400677125611439 ; acc: 0.6\n",
            "Epoch 913 / 1000\n",
            "1/1 [===================>] loss: 0.1400572218925265 ; acc: 0.6\n",
            "Epoch 914 / 1000\n",
            "1/1 [===================>] loss: 0.1400467434700097 ; acc: 0.6\n",
            "Epoch 915 / 1000\n",
            "1/1 [===================>] loss: 0.14003627724760498 ; acc: 0.6\n",
            "Epoch 916 / 1000\n",
            "1/1 [===================>] loss: 0.1400258231795692 ; acc: 0.6\n",
            "Epoch 917 / 1000\n",
            "1/1 [===================>] loss: 0.1400153812204024 ; acc: 0.6\n",
            "Epoch 918 / 1000\n",
            "1/1 [===================>] loss: 0.14000495132484644 ; acc: 0.6\n",
            "Epoch 919 / 1000\n",
            "1/1 [===================>] loss: 0.13999453344788257 ; acc: 0.6\n",
            "Epoch 920 / 1000\n",
            "1/1 [===================>] loss: 0.13998412754473014 ; acc: 0.6\n",
            "Epoch 921 / 1000\n",
            "1/1 [===================>] loss: 0.13997373357084442 ; acc: 0.6\n",
            "Epoch 922 / 1000\n",
            "1/1 [===================>] loss: 0.1399633514819149 ; acc: 0.6\n",
            "Epoch 923 / 1000\n",
            "1/1 [===================>] loss: 0.13995298123386365 ; acc: 0.6\n",
            "Epoch 924 / 1000\n",
            "1/1 [===================>] loss: 0.13994262278284347 ; acc: 0.6\n",
            "Epoch 925 / 1000\n",
            "1/1 [===================>] loss: 0.139932276085236 ; acc: 0.6\n",
            "Epoch 926 / 1000\n",
            "1/1 [===================>] loss: 0.13992194109765035 ; acc: 0.6\n",
            "Epoch 927 / 1000\n",
            "1/1 [===================>] loss: 0.13991161777692102 ; acc: 0.6\n",
            "Epoch 928 / 1000\n",
            "1/1 [===================>] loss: 0.13990130608010648 ; acc: 0.6\n",
            "Epoch 929 / 1000\n",
            "1/1 [===================>] loss: 0.1398910059644873 ; acc: 0.6\n",
            "Epoch 930 / 1000\n",
            "1/1 [===================>] loss: 0.13988071738756483 ; acc: 0.6\n",
            "Epoch 931 / 1000\n",
            "1/1 [===================>] loss: 0.139870440307059 ; acc: 0.6\n",
            "Epoch 932 / 1000\n",
            "1/1 [===================>] loss: 0.13986017468090722 ; acc: 0.6\n",
            "Epoch 933 / 1000\n",
            "1/1 [===================>] loss: 0.13984992046726263 ; acc: 0.6\n",
            "Epoch 934 / 1000\n",
            "1/1 [===================>] loss: 0.13983967762449231 ; acc: 0.6\n",
            "Epoch 935 / 1000\n",
            "1/1 [===================>] loss: 0.13982944611117595 ; acc: 0.6\n",
            "Epoch 936 / 1000\n",
            "1/1 [===================>] loss: 0.13981922588610424 ; acc: 0.6\n",
            "Epoch 937 / 1000\n",
            "1/1 [===================>] loss: 0.13980901690827707 ; acc: 0.6\n",
            "Epoch 938 / 1000\n",
            "1/1 [===================>] loss: 0.1397988191369025 ; acc: 0.6\n",
            "Epoch 939 / 1000\n",
            "1/1 [===================>] loss: 0.13978863253139476 ; acc: 0.6\n",
            "Epoch 940 / 1000\n",
            "1/1 [===================>] loss: 0.13977845705137296 ; acc: 0.6\n",
            "Epoch 941 / 1000\n",
            "1/1 [===================>] loss: 0.13976829265665977 ; acc: 0.6\n",
            "Epoch 942 / 1000\n",
            "1/1 [===================>] loss: 0.13975813930727948 ; acc: 0.6\n",
            "Epoch 943 / 1000\n",
            "1/1 [===================>] loss: 0.13974799696345713 ; acc: 0.6\n",
            "Epoch 944 / 1000\n",
            "1/1 [===================>] loss: 0.13973786558561663 ; acc: 0.6\n",
            "Epoch 945 / 1000\n",
            "1/1 [===================>] loss: 0.13972774513437936 ; acc: 0.6\n",
            "Epoch 946 / 1000\n",
            "1/1 [===================>] loss: 0.13971763557056313 ; acc: 0.6\n",
            "Epoch 947 / 1000\n",
            "1/1 [===================>] loss: 0.1397075368551804 ; acc: 0.6\n",
            "Epoch 948 / 1000\n",
            "1/1 [===================>] loss: 0.13969744894943684 ; acc: 0.6\n",
            "Epoch 949 / 1000\n",
            "1/1 [===================>] loss: 0.1396873718147304 ; acc: 0.6\n",
            "Epoch 950 / 1000\n",
            "1/1 [===================>] loss: 0.1396773054126495 ; acc: 0.6\n",
            "Epoch 951 / 1000\n",
            "1/1 [===================>] loss: 0.139667249704972 ; acc: 0.6\n",
            "Epoch 952 / 1000\n",
            "1/1 [===================>] loss: 0.13965720465366355 ; acc: 0.6\n",
            "Epoch 953 / 1000\n",
            "1/1 [===================>] loss: 0.13964717022087644 ; acc: 0.6\n",
            "Epoch 954 / 1000\n",
            "1/1 [===================>] loss: 0.13963714636894847 ; acc: 0.6\n",
            "Epoch 955 / 1000\n",
            "1/1 [===================>] loss: 0.13962713306040134 ; acc: 0.6\n",
            "Epoch 956 / 1000\n",
            "1/1 [===================>] loss: 0.13961713025793943 ; acc: 0.6\n",
            "Epoch 957 / 1000\n",
            "1/1 [===================>] loss: 0.13960713792444868 ; acc: 0.6\n",
            "Epoch 958 / 1000\n",
            "1/1 [===================>] loss: 0.13959715602299516 ; acc: 0.6\n",
            "Epoch 959 / 1000\n",
            "1/1 [===================>] loss: 0.139587184516824 ; acc: 0.6\n",
            "Epoch 960 / 1000\n",
            "1/1 [===================>] loss: 0.13957722336935796 ; acc: 0.6\n",
            "Epoch 961 / 1000\n",
            "1/1 [===================>] loss: 0.1395672725441963 ; acc: 0.6\n",
            "Epoch 962 / 1000\n",
            "1/1 [===================>] loss: 0.1395573320051135 ; acc: 0.6\n",
            "Epoch 963 / 1000\n",
            "1/1 [===================>] loss: 0.13954740171605817 ; acc: 0.6\n",
            "Epoch 964 / 1000\n",
            "1/1 [===================>] loss: 0.1395374816411518 ; acc: 0.6\n",
            "Epoch 965 / 1000\n",
            "1/1 [===================>] loss: 0.13952757174468747 ; acc: 0.6\n",
            "Epoch 966 / 1000\n",
            "1/1 [===================>] loss: 0.1395176719911289 ; acc: 0.6\n",
            "Epoch 967 / 1000\n",
            "1/1 [===================>] loss: 0.13950778234510908 ; acc: 0.6\n",
            "Epoch 968 / 1000\n",
            "1/1 [===================>] loss: 0.13949790277142923 ; acc: 0.6\n",
            "Epoch 969 / 1000\n",
            "1/1 [===================>] loss: 0.13948803323505765 ; acc: 0.6\n",
            "Epoch 970 / 1000\n",
            "1/1 [===================>] loss: 0.13947817370112853 ; acc: 0.6\n",
            "Epoch 971 / 1000\n",
            "1/1 [===================>] loss: 0.1394683241349409 ; acc: 0.6\n",
            "Epoch 972 / 1000\n",
            "1/1 [===================>] loss: 0.13945848450195758 ; acc: 0.6\n",
            "Epoch 973 / 1000\n",
            "1/1 [===================>] loss: 0.13944865476780385 ; acc: 0.6\n",
            "Epoch 974 / 1000\n",
            "1/1 [===================>] loss: 0.13943883489826656 ; acc: 0.6\n",
            "Epoch 975 / 1000\n",
            "1/1 [===================>] loss: 0.13942902485929315 ; acc: 0.6\n",
            "Epoch 976 / 1000\n",
            "1/1 [===================>] loss: 0.1394192246169901 ; acc: 0.6\n",
            "Epoch 977 / 1000\n",
            "1/1 [===================>] loss: 0.1394094341376226 ; acc: 0.6\n",
            "Epoch 978 / 1000\n",
            "1/1 [===================>] loss: 0.13939965338761273 ; acc: 0.6\n",
            "Epoch 979 / 1000\n",
            "1/1 [===================>] loss: 0.13938988233353913 ; acc: 0.6\n",
            "Epoch 980 / 1000\n",
            "1/1 [===================>] loss: 0.1393801209421354 ; acc: 0.6\n",
            "Epoch 981 / 1000\n",
            "1/1 [===================>] loss: 0.13937036918028936 ; acc: 0.6\n",
            "Epoch 982 / 1000\n",
            "1/1 [===================>] loss: 0.13936062701504204 ; acc: 0.6\n",
            "Epoch 983 / 1000\n",
            "1/1 [===================>] loss: 0.13935089441358656 ; acc: 0.6\n",
            "Epoch 984 / 1000\n",
            "1/1 [===================>] loss: 0.13934117134326723 ; acc: 0.6\n",
            "Epoch 985 / 1000\n",
            "1/1 [===================>] loss: 0.13933145777157852 ; acc: 0.6\n",
            "Epoch 986 / 1000\n",
            "1/1 [===================>] loss: 0.13932175366616406 ; acc: 0.6\n",
            "Epoch 987 / 1000\n",
            "1/1 [===================>] loss: 0.1393120589948158 ; acc: 0.6\n",
            "Epoch 988 / 1000\n",
            "1/1 [===================>] loss: 0.13930237372547283 ; acc: 0.6\n",
            "Epoch 989 / 1000\n",
            "1/1 [===================>] loss: 0.13929269782622064 ; acc: 0.6\n",
            "Epoch 990 / 1000\n",
            "1/1 [===================>] loss: 0.13928303126529012 ; acc: 0.6\n",
            "Epoch 991 / 1000\n",
            "1/1 [===================>] loss: 0.13927337401105647 ; acc: 0.6\n",
            "Epoch 992 / 1000\n",
            "1/1 [===================>] loss: 0.13926372603203846 ; acc: 0.6\n",
            "Epoch 993 / 1000\n",
            "1/1 [===================>] loss: 0.13925408729689745 ; acc: 0.6\n",
            "Epoch 994 / 1000\n",
            "1/1 [===================>] loss: 0.1392444577744365 ; acc: 0.6\n",
            "Epoch 995 / 1000\n",
            "1/1 [===================>] loss: 0.1392348374335994 ; acc: 0.6\n",
            "Epoch 996 / 1000\n",
            "1/1 [===================>] loss: 0.13922522624346992 ; acc: 0.6\n",
            "Epoch 997 / 1000\n",
            "1/1 [===================>] loss: 0.13921562417327057 ; acc: 0.6\n",
            "Epoch 998 / 1000\n",
            "1/1 [===================>] loss: 0.13920603119236227 ; acc: 0.6\n",
            "Epoch 999 / 1000\n",
            "1/1 [===================>] loss: 0.13919644727024305 ; acc: 0.6\n",
            "Epoch 1000 / 1000\n",
            "1/1 [===================>] loss: 0.13918687237654725 ; acc: 0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz6IxudRQDtj"
      },
      "source": [
        "Crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flChPQSnP8Gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02f0dd0-b0fa-42b2-985a-ee8faabeb7a4"
      },
      "source": [
        "input = np.array([[1, -1, 1],\n",
        "                  [2, 1, 4],\n",
        "                  [-1, 2, 1],\n",
        "                  [2, 2, 1],\n",
        "                  [4, 1, -4]])\n",
        "\n",
        "target = np.array([[0, 0, 1, 0],\n",
        "                   [0, 1, 0, 0],\n",
        "                   [1, 0, 0, 0],\n",
        "                   [1, 0, 0, 0],\n",
        "                   [0, 0, 1, 0]])\n",
        "\n",
        "print('#### BUILDING TESTS ####')\n",
        "model = Model()\n",
        "model.add(Dense(2, activation=Relu()))\n",
        "model.add(Dense(4, activation=Softmax()))\n",
        "\n",
        "model.build(input[0].shape, SGD(), CategoricalCrossentropy())\n",
        "\n",
        "print('\\n\\n#### FORWARD TESTS ####')\n",
        "model.weights = [np.array([[0, 1],\n",
        "                          [2, 1],\n",
        "                          [-1, 0.5]]),\n",
        "                np.array([[1, -1, 2, 1],\n",
        "                          [0, -2, -1, 0.5]])]\n",
        "\n",
        "model.biases = [np.array([1, 0]), np.array([0.5, -1, 0, 2])]\n",
        "\n",
        "output = model(input)\n",
        "print('\\noutput :\\n', output)\n",
        "print('\\n loss:', model.loss(output, target))\n",
        "print('\\n eval:', model.evaluate(input, target))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n### BACKWARD TESTS ###\\n\")\n",
        "\n",
        "dloss_dact = model.loss.derivative(output, target)\n",
        "print(dloss_dact)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#### BUILDING TESTS ####\n",
            "\n",
            "\n",
            "#### FORWARD TESTS ####\n",
            "\n",
            "output :\n",
            " [[1.38800845e-01 1.13934671e-02 5.10619771e-02 7.98743711e-01]\n",
            " [1.79848847e-02 1.82188565e-07 7.35001131e-05 9.81941433e-01]\n",
            " [5.59384366e-02 2.08463152e-07 4.13332246e-01 5.30729109e-01]\n",
            " [2.27847725e-02 2.10473189e-10 8.38204936e-03 9.68833178e-01]\n",
            " [1.84498479e-02 8.48518721e-12 6.10975051e-01 3.70575101e-01]]\n",
            "\n",
            " loss: [ 2.97471515 15.51822361  2.88350354  3.78166284  0.49269915]\n",
            "\n",
            " eval: (5.130160858770255, 0.2)\n",
            "\n",
            "\n",
            "\n",
            "### BACKWARD TESTS ###\n",
            "\n",
            "[[ 0.00000000e+00  0.00000000e+00 -1.95840439e+01  0.00000000e+00]\n",
            " [ 0.00000000e+00 -5.48881868e+06  0.00000000e+00  0.00000000e+00]\n",
            " [-1.78767957e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-4.38889615e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00 -1.63672804e+00  0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMxTUbGwtxlF"
      },
      "source": [
        "### Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Q-tHq5txlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b26383c3-27e4-4e8d-afde-818d7c79486a"
      },
      "source": [
        "inputs = np.array([[[1, 0, 0, 1],\n",
        "                   [1, 1, 1, 0],\n",
        "                   [1, 1, 0, 0],\n",
        "                   [1, 1, 0, 1]],\n",
        "                  [[1, 0, 0, 0],\n",
        "                  [0, 0, 1, 1],\n",
        "                  [0, 0, 1, 0],\n",
        "                  [0, 1, 0, 1]]])\n",
        "\n",
        "target = np.array([[0, 0, 1, 0],\n",
        "                   [0, 1, 0, 0]])\n",
        "\n",
        "print('#### BUILDING TESTS ####')\n",
        "model = Model()\n",
        "model.add(Conv2D(3, (2, 2), activation=Relu()))\n",
        "model.add(Conv2D(4, (2, 2), activation=Relu()))\n",
        "model.add(Dense(4, activation=Softmax()))\n",
        "\n",
        "model.summary()\n",
        "model.build(inputs[0].shape, SGD(), MSE())\n",
        "model.summary()\n",
        "\n",
        "print('weights :\\n', model.weights)\n",
        "print('biases :\\n', model.biases)\n",
        "\n",
        "\n",
        "\n",
        "print('\\n\\n#### FORWARD TESTS ####')\n",
        "model.weights = [np.array([[[[1, 1], [1, 0]]],\n",
        "                          [[[0, 1], [1, 2]]],\n",
        "                          [[[2, 0], [1, 1]]]]),\n",
        "                 np.array([[[[1, 1], [1, 0]],\n",
        "                            [[-1, 0], [0, 0]],\n",
        "                            [[1, 1], [2, -2]]],\n",
        "                           [[[0, 1], [1, 0]],\n",
        "                            [[2, 0], [1, 1]],\n",
        "                            [[-1, 1], [2, 2]]],\n",
        "                           [[[1, 1], [1, 1]],\n",
        "                            [[1, 1], [0, 1]],\n",
        "                            [[1, 2], [-2, 2]]],\n",
        "                           [[[1, 1], [1, 2]],\n",
        "                            [[0, 0], [0, 1]],\n",
        "                            [[1, 1], [2, 2]]]]),\n",
        "                 np.array([[1, 1, 1, -1],\n",
        "                          [1, 1, 1, -1],\n",
        "                          [2, 1, 1, -1],\n",
        "                          [1, -2, 1, -1],\n",
        "                          [1, 1, 1, -1],\n",
        "                          [-1, -1, 1, -1],\n",
        "                          [2, 2, 1, -1],\n",
        "                          [1, 2, 1, -1],\n",
        "                          [2, 2, 2, -1],\n",
        "                          [1, 2, 1, -1],\n",
        "                          [1, 1, 1, 1],\n",
        "                          [1, 0, 1, 2],\n",
        "                          [1, 1, 1, 0],\n",
        "                          [1, 1, 1, 1],\n",
        "                          [1, 0, 1, 1],\n",
        "                          [1, 1, 1, 1]])]\n",
        "                          \n",
        "\n",
        "model.biases = [np.array([1, 0, 0]), np.array([1, -1, 0, 1]), np.array([1, 1, 1, 0])]\n",
        "print('\\nweights :\\n', model.weights)\n",
        "print('biases :\\n', model.biases)\n",
        "\n",
        "output = model(inputs)\n",
        "\n",
        "bs = inputs.shape[0]\n",
        "for i in range(len(model.layers)):\n",
        "    print('\\nLayer', i+1)\n",
        "    if model.layers[i].type_name == 'Conv2D':\n",
        "        print('z:', np.reshape(model.layers[i].z, (bs, *model.layers[i].output_shape)))\n",
        "        print('activation:', np.reshape(model.layers[i].out, (bs, *model.layers[i].output_shape)))\n",
        "    else:\n",
        "        print('z:', model.layers[i].z)\n",
        "        print('activation:', model.layers[i].out)\n",
        "\n",
        "print('\\noutput :\\n', output)\n",
        "print('\\n loss:', model.loss(output, target))\n",
        "print('\\n eval:', model.evaluate(inputs, target))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n### BACKWARD TESTS ###\\n\")\n",
        "\n",
        "model.fit(inputs, target, epochs=1, verbose='debug')\n",
        "\n",
        "print(model.biases)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#### BUILDING TESTS ####\n",
            "MODEL UNBUILT\n",
            "layer    |\n",
            "----------\n",
            "Conv2D   |\n",
            "----------\n",
            "Conv2D   |\n",
            "----------\n",
            "Dense    |\n",
            "\n",
            "\n",
            "layer | input shape | output shape | nb_param |\n",
            "-----------------------------------------------\n",
            "Conv2D| (4, 4)      | (3, 3, 3)    | 15       |\n",
            "-----------------------------------------------\n",
            "Conv2D| (3, 3, 3)   | (4, 2, 2)    | 52       |\n",
            "-----------------------------------------------\n",
            "Dense | (4, 2, 2)   | (4,)         | 68       |\n",
            "\n",
            "\n",
            "weights :\n",
            " [array([[[[-0.11254056,  0.05279407],\n",
            "         [-0.15371227, -0.5420581 ]]],\n",
            "\n",
            "\n",
            "       [[[-0.32647085, -0.17883377],\n",
            "         [-0.22291275,  0.9972099 ]]],\n",
            "\n",
            "\n",
            "       [[[ 0.68085006, -0.09189219],\n",
            "         [ 0.12763752,  0.18045348]]]]), array([[[[ 0.03778367,  0.46696359],\n",
            "         [-0.13264557,  0.17687973]],\n",
            "\n",
            "        [[ 0.54934818,  0.03572331],\n",
            "         [-0.13299696,  0.15365858]],\n",
            "\n",
            "        [[ 0.12703576, -0.02233472],\n",
            "         [ 0.26935876, -0.26143466]]],\n",
            "\n",
            "\n",
            "       [[[-0.12379367,  0.25293387],\n",
            "         [-0.25859583,  0.02211457]],\n",
            "\n",
            "        [[ 0.41181588,  0.44632774],\n",
            "         [-0.29939428, -0.28346311]],\n",
            "\n",
            "        [[-0.00466447, -0.15411871],\n",
            "         [-0.15577479, -0.14728231]]],\n",
            "\n",
            "\n",
            "       [[[ 0.0562495 , -0.16985361],\n",
            "         [-0.15067061,  0.23855853]],\n",
            "\n",
            "        [[-0.31309489,  0.14824008],\n",
            "         [-0.02145298, -0.1784747 ]],\n",
            "\n",
            "        [[-0.04298968, -0.10690195],\n",
            "         [ 0.23536065, -0.29053683]]],\n",
            "\n",
            "\n",
            "       [[[ 0.09211759, -0.09003218],\n",
            "         [-0.19214118,  0.310951  ]],\n",
            "\n",
            "        [[ 0.1002962 ,  0.18991899],\n",
            "         [ 0.13087408, -0.26063989]],\n",
            "\n",
            "        [[-0.37070743, -0.28103295],\n",
            "         [ 0.16924146,  0.13532512]]]]), array([[-0.24456352,  0.06313532,  0.46490178,  0.18152695],\n",
            "       [-0.13262498, -0.03548755, -0.10297999, -0.02805906],\n",
            "       [ 0.17491041,  0.03348312,  0.13588018, -0.04051479],\n",
            "       [ 0.18817924, -0.04316893,  0.18495575, -0.04747881],\n",
            "       [-0.10535206,  0.06414164,  0.00824449, -0.10585008],\n",
            "       [-0.1902915 , -0.32472063,  0.24200251,  0.1567791 ],\n",
            "       [ 0.25943829,  0.14322161,  0.0525352 ,  0.16275182],\n",
            "       [-0.01911369,  0.0192502 , -0.04331197, -0.06480858],\n",
            "       [-0.04981955, -0.07532603, -0.1503793 ,  0.32873759],\n",
            "       [-0.02028604,  0.25846797, -0.05956254,  0.01892851],\n",
            "       [ 0.12140606,  0.39042356,  0.00647199, -0.11901232],\n",
            "       [-0.08065814, -0.28644433, -0.11939752, -0.04154426],\n",
            "       [ 0.05356148, -0.30495109, -0.34597265,  0.01173449],\n",
            "       [ 0.24377206,  0.34219365, -0.11066565,  0.10006675],\n",
            "       [ 0.45386745, -0.02677494,  0.07362155, -0.00720603],\n",
            "       [-0.35903423, -0.01022226, -0.26240258, -0.03698085]])]\n",
            "biases :\n",
            " [array([ 0.04694938,  0.26068778, -0.01503797]), array([ 0.17564214,  0.06562917,  0.31417783, -0.08888538]), array([ 0.00871446,  0.15263114,  0.1093156 , -0.00846048])]\n",
            "\n",
            "\n",
            "#### FORWARD TESTS ####\n",
            "\n",
            "weights :\n",
            " [array([[[[1, 1],\n",
            "         [1, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 1],\n",
            "         [1, 2]]],\n",
            "\n",
            "\n",
            "       [[[2, 0],\n",
            "         [1, 1]]]]), array([[[[ 1,  1],\n",
            "         [ 1,  0]],\n",
            "\n",
            "        [[-1,  0],\n",
            "         [ 0,  0]],\n",
            "\n",
            "        [[ 1,  1],\n",
            "         [ 2, -2]]],\n",
            "\n",
            "\n",
            "       [[[ 0,  1],\n",
            "         [ 1,  0]],\n",
            "\n",
            "        [[ 2,  0],\n",
            "         [ 1,  1]],\n",
            "\n",
            "        [[-1,  1],\n",
            "         [ 2,  2]]],\n",
            "\n",
            "\n",
            "       [[[ 1,  1],\n",
            "         [ 1,  1]],\n",
            "\n",
            "        [[ 1,  1],\n",
            "         [ 0,  1]],\n",
            "\n",
            "        [[ 1,  2],\n",
            "         [-2,  2]]],\n",
            "\n",
            "\n",
            "       [[[ 1,  1],\n",
            "         [ 1,  2]],\n",
            "\n",
            "        [[ 0,  0],\n",
            "         [ 0,  1]],\n",
            "\n",
            "        [[ 1,  1],\n",
            "         [ 2,  2]]]]), array([[ 1,  1,  1, -1],\n",
            "       [ 1,  1,  1, -1],\n",
            "       [ 2,  1,  1, -1],\n",
            "       [ 1, -2,  1, -1],\n",
            "       [ 1,  1,  1, -1],\n",
            "       [-1, -1,  1, -1],\n",
            "       [ 2,  2,  1, -1],\n",
            "       [ 1,  2,  1, -1],\n",
            "       [ 2,  2,  2, -1],\n",
            "       [ 1,  2,  1, -1],\n",
            "       [ 1,  1,  1,  1],\n",
            "       [ 1,  0,  1,  2],\n",
            "       [ 1,  1,  1,  0],\n",
            "       [ 1,  1,  1,  1],\n",
            "       [ 1,  0,  1,  1],\n",
            "       [ 1,  1,  1,  1]])]\n",
            "biases :\n",
            " [array([1, 0, 0]), array([ 1, -1,  0,  1]), array([1, 1, 1, 0])]\n",
            "\n",
            "Layer 1\n",
            "z: [[[[3. 2. 3.]\n",
            "   [4. 4. 2.]\n",
            "   [4. 3. 1.]]\n",
            "\n",
            "  [[3. 3. 2.]\n",
            "   [4. 2. 0.]\n",
            "   [4. 1. 2.]]\n",
            "\n",
            "  [[4. 2. 1.]\n",
            "   [4. 3. 2.]\n",
            "   [4. 3. 1.]]]\n",
            "\n",
            "\n",
            " [[[2. 1. 2.]\n",
            "   [1. 2. 4.]\n",
            "   [1. 3. 2.]]\n",
            "\n",
            "  [[0. 2. 3.]\n",
            "   [0. 3. 2.]\n",
            "   [2. 2. 2.]]\n",
            "\n",
            "  [[2. 1. 2.]\n",
            "   [0. 1. 3.]\n",
            "   [1. 1. 3.]]]]\n",
            "activation: [[[[3. 2. 3.]\n",
            "   [4. 4. 2.]\n",
            "   [4. 3. 1.]]\n",
            "\n",
            "  [[3. 3. 2.]\n",
            "   [4. 2. 0.]\n",
            "   [4. 1. 2.]]\n",
            "\n",
            "  [[4. 2. 1.]\n",
            "   [4. 3. 2.]\n",
            "   [4. 3. 1.]]]\n",
            "\n",
            "\n",
            " [[[2. 1. 2.]\n",
            "   [1. 2. 4.]\n",
            "   [1. 3. 2.]]\n",
            "\n",
            "  [[0. 2. 3.]\n",
            "   [0. 3. 2.]\n",
            "   [2. 2. 2.]]\n",
            "\n",
            "  [[2. 1. 2.]\n",
            "   [0. 1. 3.]\n",
            "   [1. 1. 3.]]]]\n",
            "\n",
            "Layer 2\n",
            "z: [[[[15. 12.]\n",
            "   [18. 17.]]\n",
            "\n",
            "  [[29. 23.]\n",
            "   [33. 18.]]\n",
            "\n",
            "  [[27. 18.]\n",
            "   [30. 17.]]\n",
            "\n",
            "  [[40. 27.]\n",
            "   [41. 27.]]]\n",
            "\n",
            "\n",
            " [[[ 6.  3.]\n",
            "   [ 6.  7.]]\n",
            "\n",
            "  [[ 5. 21.]\n",
            "   [11. 26.]]\n",
            "\n",
            "  [[17. 25.]\n",
            "   [14. 29.]]\n",
            "\n",
            "  [[17. 27.]\n",
            "   [18. 28.]]]]\n",
            "activation: [[[[15. 12.]\n",
            "   [18. 17.]]\n",
            "\n",
            "  [[29. 23.]\n",
            "   [33. 18.]]\n",
            "\n",
            "  [[27. 18.]\n",
            "   [30. 17.]]\n",
            "\n",
            "  [[40. 27.]\n",
            "   [41. 27.]]]\n",
            "\n",
            "\n",
            " [[[ 6.  3.]\n",
            "   [ 6.  7.]]\n",
            "\n",
            "  [[ 5. 21.]\n",
            "   [11. 26.]]\n",
            "\n",
            "  [[17. 25.]\n",
            "   [14. 29.]]\n",
            "\n",
            "  [[17. 27.]\n",
            "   [18. 28.]]]]\n",
            "\n",
            "Layer 3\n",
            "z: [[425. 334. 420. -51.]\n",
            " [253. 230. 278.  18.]]\n",
            "activation: [[9.93307149e-001 2.99423380e-040 6.69285092e-003 1.87460674e-207]\n",
            " [1.38879439e-011 1.42516408e-021 1.00000000e+000 1.21181048e-113]]\n",
            "\n",
            "output :\n",
            " [[9.93307149e-001 2.99423380e-040 6.69285092e-003 1.87460674e-207]\n",
            " [1.38879439e-011 1.42516408e-021 1.00000000e+000 1.21181048e-113]]\n",
            "\n",
            " loss: [0.49332955 0.5       ]\n",
            "\n",
            " eval: (0.49666477309775925, 0.0)\n",
            "\n",
            "\n",
            "\n",
            "### BACKWARD TESTS ###\n",
            "\n",
            "Epoch 1 / 1\n",
            "\r1/1 [===================>] loss:  0.497\n",
            "dloss_dw\n",
            " [array([[[[[ 1.98106867e-02,  6.60356222e-03],\n",
            "          [ 6.60356222e-03,  1.98106867e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 6.60356222e-03, -6.99093561e-16],\n",
            "          [-1.98106867e-02, -1.32071244e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.32071244e-02, -1.60982339e-15],\n",
            "          [ 1.32071244e-02,  3.96213733e-02]]]],\n",
            "\n",
            "\n",
            "\n",
            "       [[[[ 4.05959738e-16,  8.40924847e-16],\n",
            "          [ 1.38884658e-11,  9.85900390e-16]]],\n",
            "\n",
            "\n",
            "        [[[ 1.38881468e-11,  2.08323797e-11],\n",
            "          [ 6.94423290e-12,  3.47203526e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 2.77763517e-11,  4.16648175e-11],\n",
            "          [ 4.16644695e-11, -1.38868130e-11]]]]]), array([[[[[ 2.64142489e-02,  2.64142489e-02],\n",
            "          [ 2.64142489e-02,  1.98106867e-02]],\n",
            "\n",
            "         [[ 2.64142489e-02,  1.32071244e-02],\n",
            "          [ 2.64142489e-02,  6.60356222e-03]],\n",
            "\n",
            "         [[ 2.64142489e-02,  1.98106867e-02],\n",
            "          [ 2.64142489e-02,  1.98106867e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.00828284e-16, -1.32071244e-02],\n",
            "          [-2.64142489e-02, -6.60356222e-03]],\n",
            "\n",
            "         [[-1.32071244e-02, -1.32071244e-02],\n",
            "          [-5.91540705e-16,  6.60356222e-03]],\n",
            "\n",
            "         [[-6.99093561e-16,  6.60356222e-03],\n",
            "          [-1.32071244e-02, -6.60356222e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.60422844e-16, -6.99093561e-16],\n",
            "          [-1.02175213e-15, -7.52869989e-16]],\n",
            "\n",
            "         [[-8.06646416e-16, -5.37764278e-16],\n",
            "          [-8.06646416e-16, -3.76434994e-16]],\n",
            "\n",
            "         [[-9.14199272e-16, -5.37764278e-16],\n",
            "          [-9.67975700e-16, -6.45317133e-16]]],\n",
            "\n",
            "\n",
            "        [[[-6.99093561e-16, -5.91540705e-16],\n",
            "          [-8.06646416e-16, -5.37764278e-16]],\n",
            "\n",
            "         [[-6.45317133e-16, -3.76434994e-16],\n",
            "          [-5.91540705e-16, -2.68882139e-16]],\n",
            "\n",
            "         [[-6.99093561e-16, -4.30211422e-16],\n",
            "          [-7.52869989e-16, -4.83987850e-16]]]],\n",
            "\n",
            "\n",
            "\n",
            "       [[[[-6.94379794e-12, -1.38876829e-11],\n",
            "          [-6.94376894e-12, -2.08315968e-11]],\n",
            "\n",
            "         [[ 1.44996920e-16, -2.08316258e-11],\n",
            "          [-1.38877409e-11, -1.38876829e-11]],\n",
            "\n",
            "         [[ 1.15991551e-16, -6.94376894e-12],\n",
            "          [-6.94388494e-12, -6.94373994e-12]]],\n",
            "\n",
            "\n",
            "        [[[ 6.94414591e-12,  1.38882048e-11],\n",
            "          [ 2.08321188e-11,  3.47201786e-11]],\n",
            "\n",
            "         [[ 2.77760327e-11,  2.08322058e-11],\n",
            "          [ 2.77760907e-11,  1.38882048e-11]],\n",
            "\n",
            "         [[ 1.38880599e-11,  2.08321188e-11],\n",
            "          [ 6.94405892e-12,  3.47200916e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 2.31975975e-16,  2.89971038e-16],\n",
            "          [ 2.31975975e-16,  3.76955794e-16]],\n",
            "\n",
            "         [[ 1.44985519e-16,  3.47960400e-16],\n",
            "          [ 2.02976306e-16,  3.47961825e-16]],\n",
            "\n",
            "         [[ 1.73980913e-16,  2.31975975e-16],\n",
            "          [ 8.69904564e-17,  2.60971369e-16]]],\n",
            "\n",
            "\n",
            "        [[[ 1.73982338e-16,  2.60974219e-16],\n",
            "          [ 2.02979157e-16,  3.18969282e-16]],\n",
            "\n",
            "         [[ 1.44984094e-16,  2.89972463e-16],\n",
            "          [ 2.02980582e-16,  2.60974219e-16]],\n",
            "\n",
            "         [[ 1.15987275e-16,  2.02979157e-16],\n",
            "          [ 8.69918815e-17,  2.31975975e-16]]]]]), array([[[ 9.90534333e-002, -2.20078662e-039, -9.90534333e-002,\n",
            "         -1.37785147e-206],\n",
            "        [ 7.92427466e-002, -1.76062930e-039, -7.92427466e-002,\n",
            "         -1.10228118e-206],\n",
            "        [ 1.18864120e-001, -2.64094395e-039, -1.18864120e-001,\n",
            "         -1.65342177e-206],\n",
            "        [ 1.12260558e-001, -2.49422484e-039, -1.12260558e-001,\n",
            "         -1.56156500e-206],\n",
            "        [ 1.91503304e-001, -4.25485414e-039, -1.91503304e-001,\n",
            "         -2.66384618e-206],\n",
            "        [ 1.51881931e-001, -3.37453949e-039, -1.51881931e-001,\n",
            "         -2.11270559e-206],\n",
            "        [ 2.17917553e-001, -4.84173057e-039, -2.17917553e-001,\n",
            "         -3.03127324e-206],\n",
            "        [ 1.18864120e-001, -2.64094395e-039, -1.18864120e-001,\n",
            "         -1.65342177e-206],\n",
            "        [ 1.78296180e-001, -3.96141592e-039, -1.78296180e-001,\n",
            "         -2.48013265e-206],\n",
            "        [ 1.18864120e-001, -2.64094395e-039, -1.18864120e-001,\n",
            "         -1.65342177e-206],\n",
            "        [ 1.98106867e-001, -4.40157325e-039, -1.98106867e-001,\n",
            "         -2.75570295e-206],\n",
            "        [ 1.12260558e-001, -2.49422484e-039, -1.12260558e-001,\n",
            "         -1.56156500e-206],\n",
            "        [ 2.64142489e-001, -5.86876433e-039, -2.64142489e-001,\n",
            "         -3.67427059e-206],\n",
            "        [ 1.78296180e-001, -3.96141592e-039, -1.78296180e-001,\n",
            "         -2.48013265e-206],\n",
            "        [ 2.70746051e-001, -6.01548344e-039, -2.70746051e-001,\n",
            "         -3.76612736e-206],\n",
            "        [ 1.78296180e-001, -3.96141592e-039, -1.78296180e-001,\n",
            "         -2.48013265e-206]],\n",
            "\n",
            "       [[-4.16638316e-011, -8.55098450e-021,  4.16640056e-011,\n",
            "         -3.63543145e-113],\n",
            "        [-2.08319158e-011, -4.27549225e-021,  2.08320028e-011,\n",
            "         -1.81771572e-113],\n",
            "        [-4.16638316e-011, -8.55098450e-021,  4.16640056e-011,\n",
            "         -3.63543145e-113],\n",
            "        [-4.86078035e-011, -9.97614858e-021,  4.86080065e-011,\n",
            "         -4.24133669e-113],\n",
            "        [-3.47198597e-011, -7.12582041e-021,  3.47200047e-011,\n",
            "         -3.02952621e-113],\n",
            "        [-1.45823411e-010, -2.99284457e-020,  1.45824020e-010,\n",
            "         -1.27240101e-112],\n",
            "        [-7.63836913e-011, -1.56768049e-020,  7.63840102e-011,\n",
            "         -6.66495766e-113],\n",
            "        [-1.80543270e-010, -3.70542662e-020,  1.80544024e-010,\n",
            "         -1.57535363e-112],\n",
            "        [-1.18047523e-010, -2.42277894e-020,  1.18048016e-010,\n",
            "         -1.03003891e-112],\n",
            "        [-1.73599298e-010, -3.56291021e-020,  1.73600023e-010,\n",
            "         -1.51476310e-112],\n",
            "        [-9.72156070e-011, -1.99522972e-020,  9.72160130e-011,\n",
            "         -8.48267338e-113],\n",
            "        [-2.01375186e-010, -4.13297584e-020,  2.01376027e-010,\n",
            "         -1.75712520e-112],\n",
            "        [-1.18047523e-010, -2.42277894e-020,  1.18048016e-010,\n",
            "         -1.03003891e-112],\n",
            "        [-1.87487242e-010, -3.84794302e-020,  1.87488025e-010,\n",
            "         -1.63594415e-112],\n",
            "        [-1.24991495e-010, -2.56529535e-020,  1.24992017e-010,\n",
            "         -1.09062943e-112],\n",
            "        [-1.94431214e-010, -3.99045943e-020,  1.94432026e-010,\n",
            "         -1.69653468e-112]]])]\n",
            "\n",
            "dloss_db\n",
            " [array([[ 6.60356222e-03, -1.98106867e-02, -1.32071244e-02],\n",
            "       [-6.94223210e-12,  4.16645565e-11,  1.38896547e-11]]), array([[ 6.60356222e-03, -6.60356222e-03, -2.68882139e-16,\n",
            "        -2.15105711e-16],\n",
            "       [-6.94385594e-12,  6.94408792e-12,  1.44984094e-16,\n",
            "         1.15988700e-16]]), array([[ 6.60356222e-003, -1.46719108e-040, -6.60356222e-003,\n",
            "        -9.18567649e-208],\n",
            "       [-6.94397193e-012, -1.42516408e-021,  6.94400093e-012,\n",
            "        -6.05905242e-114]])]\n",
            "\r1/1 [===================>] loss: 0.493227433953774 ; acc: 0.0\n",
            "[array([9.99966982e-01, 9.90534331e-05, 6.60356221e-05]), array([ 9.99966982e-01, -9.99966982e-01,  6.19490224e-19,  1.00000000e+00]), array([9.99966982e-001, 1.00000000e+000, 1.00003302e+000, 3.02952621e-116])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ_f4ZvrtxlI"
      },
      "source": [
        "Test stride"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWSAiTwXtxlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50acca20-7a7c-42d6-fbc6-221d41dd7888"
      },
      "source": [
        "inputs = np.array([[[1, 0, 0, 1],\n",
        "                   [1, 1, 1, 0],\n",
        "                   [1, 1, 0, 0],\n",
        "                   [1, 1, 0, 1]],\n",
        "                  [[1, 0, 0, 0],\n",
        "                  [0, 0, 1, 1],\n",
        "                  [0, 0, 1, 0],\n",
        "                  [0, 1, 0, 1]]])\n",
        "\n",
        "target = np.array([[0, 0, 1, 0],\n",
        "                   [0, 1, 0, 0]])\n",
        "\n",
        "print('#### BUILDING TESTS ####')\n",
        "model = Model()\n",
        "model.add(Conv2D(3, (2, 2), activation=Relu(), stride=(2, 2)))\n",
        "model.add(Conv2D(4, (2, 2), activation=Relu()))\n",
        "model.add(Dense(4, activation=Softmax()))\n",
        "\n",
        "model.summary()\n",
        "model.build(inputs[0].shape, SGD(), MSE())\n",
        "model.summary()\n",
        "\n",
        "print('weights :\\n', model.weights)\n",
        "print('biases :\\n', model.biases)\n",
        "\n",
        "\n",
        "\n",
        "print('\\n\\n#### FORWARD TESTS ####')\n",
        "model.weights = [np.array([[[[1, 1], [1, 0]]],\n",
        "                          [[[0, 1], [1, 2]]],\n",
        "                          [[[2, 0], [1, 1]]]]),\n",
        "                 np.array([[[[1, 1], [1, 0]],\n",
        "                            [[-1, 0], [0, 0]],\n",
        "                            [[1, 1], [2, -2]]],\n",
        "                           [[[0, 1], [1, 0]],\n",
        "                            [[2, 0], [1, 1]],\n",
        "                            [[-1, 1], [2, 2]]],\n",
        "                           [[[1, 1], [1, 1]],\n",
        "                            [[1, 1], [0, 1]],\n",
        "                            [[1, 2], [-2, 2]]],\n",
        "                           [[[1, 1], [1, 2]],\n",
        "                            [[0, 0], [0, 1]],\n",
        "                            [[1, 1], [2, 2]]]]),\n",
        "                 np.array([[1, 1, 1, -1],\n",
        "                          [1, 1, 1, -1],\n",
        "                          [2, 1, 1, -1],\n",
        "                          [1, -2, 1, -1]])]\n",
        "                          \n",
        "\n",
        "model.biases = [np.array([1, 0, 0]), np.array([1, -1, 0, 1]), np.array([1, 1, 1, 0])]\n",
        "print('\\nweights :\\n', model.weights)\n",
        "print('biases :\\n', model.biases)\n",
        "\n",
        "output = model(inputs)\n",
        "\n",
        "bs = inputs.shape[0]\n",
        "for i in range(len(model.layers)):\n",
        "    print('\\nLayer', i+1)\n",
        "    if model.layers[i].type_name == 'Conv2D':\n",
        "        print('z:', np.reshape(model.layers[i].z, (bs, *model.layers[i].output_shape)))\n",
        "        print('activation:', np.reshape(model.layers[i].out, (bs, *model.layers[i].output_shape)))\n",
        "    else:\n",
        "        print('z:', model.layers[i].z)\n",
        "        print('activation:', model.layers[i].out)\n",
        "\n",
        "print('\\noutput :\\n', output)\n",
        "print('\\n loss:', model.loss(output, target))\n",
        "print('\\n eval:', model.evaluate(inputs, target))\n",
        "\n",
        "\n",
        "print(\"\\n\\n\\n### BACKWARD TESTS ###\\n\")\n",
        "\n",
        "model.fit(inputs, target, epochs=1, verbose='debug')\n",
        "\n",
        "print(model.biases)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#### BUILDING TESTS ####\n",
            "MODEL UNBUILT\n",
            "layer    |\n",
            "----------\n",
            "Conv2D   |\n",
            "----------\n",
            "Conv2D   |\n",
            "----------\n",
            "Dense    |\n",
            "\n",
            "\n",
            "layer | input shape | output shape | nb_param |\n",
            "-----------------------------------------------\n",
            "Conv2D| (4, 4)      | (3, 2, 2)    | 15       |\n",
            "-----------------------------------------------\n",
            "Conv2D| (3, 2, 2)   | (4, 1, 1)    | 52       |\n",
            "-----------------------------------------------\n",
            "Dense | (4, 1, 1)   | (4,)         | 20       |\n",
            "\n",
            "\n",
            "weights :\n",
            " [array([[[[ 0.2184747 ,  0.43265017],\n",
            "         [ 0.02172403,  0.40529544]]],\n",
            "\n",
            "\n",
            "       [[[-0.23991677,  0.17781006],\n",
            "         [-0.54910038, -0.03093817]]],\n",
            "\n",
            "\n",
            "       [[[ 0.32930873, -0.10264767],\n",
            "         [-0.04072907,  0.32201755]]]]), array([[[[-0.26565349,  0.28105277],\n",
            "         [-0.40999466, -0.00688628]],\n",
            "\n",
            "        [[ 0.05170115, -0.06348626],\n",
            "         [ 0.07487241,  0.13129386]],\n",
            "\n",
            "        [[ 0.03173987,  0.00078636],\n",
            "         [-0.05959256, -0.1662526 ]]],\n",
            "\n",
            "\n",
            "       [[[ 0.1190331 , -0.14731489],\n",
            "         [ 0.12584461, -0.09916457]],\n",
            "\n",
            "        [[-0.24113813, -0.21730425],\n",
            "         [-0.03856527, -0.06180383]],\n",
            "\n",
            "        [[-0.04291084, -0.2241674 ],\n",
            "         [-0.4416428 , -0.20000239]]],\n",
            "\n",
            "\n",
            "       [[[ 0.21255545, -0.14854983],\n",
            "         [ 0.18331624,  0.1525388 ]],\n",
            "\n",
            "        [[ 0.14146142, -0.25103552],\n",
            "         [ 0.05946467, -0.02889081]],\n",
            "\n",
            "        [[ 0.2958861 , -0.03032258],\n",
            "         [-0.14913649,  0.1312906 ]]],\n",
            "\n",
            "\n",
            "       [[[-0.07589009,  0.06827069],\n",
            "         [-0.21284789, -0.04160405]],\n",
            "\n",
            "        [[ 0.34732008,  0.23239655],\n",
            "         [ 0.03246246, -0.05684812]],\n",
            "\n",
            "        [[ 0.02194417,  0.05088489],\n",
            "         [-0.38289264, -0.26704477]]]]), array([[ 1.97753495e-01,  1.59078396e-01, -1.50045360e-01,\n",
            "         2.91743977e-01],\n",
            "       [ 1.63831379e-02,  3.82201965e-01,  2.79144432e-01,\n",
            "        -1.57559101e-01],\n",
            "       [-5.72579606e-02, -3.78055685e-01,  1.85555216e-02,\n",
            "        -7.00388714e-01],\n",
            "       [-3.62269103e-01, -9.04008272e-01,  8.91740067e-08,\n",
            "         7.77195990e-03]])]\n",
            "biases :\n",
            " [array([-0.34648077, -0.23641358,  0.76787701]), array([ 0.0637824 , -0.09270703,  0.23248103,  0.11693169]), array([-0.20608935, -0.59101136, -0.06036479,  0.4780498 ])]\n",
            "\n",
            "\n",
            "#### FORWARD TESTS ####\n",
            "\n",
            "weights :\n",
            " [array([[[[1, 1],\n",
            "         [1, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 1],\n",
            "         [1, 2]]],\n",
            "\n",
            "\n",
            "       [[[2, 0],\n",
            "         [1, 1]]]]), array([[[[ 1,  1],\n",
            "         [ 1,  0]],\n",
            "\n",
            "        [[-1,  0],\n",
            "         [ 0,  0]],\n",
            "\n",
            "        [[ 1,  1],\n",
            "         [ 2, -2]]],\n",
            "\n",
            "\n",
            "       [[[ 0,  1],\n",
            "         [ 1,  0]],\n",
            "\n",
            "        [[ 2,  0],\n",
            "         [ 1,  1]],\n",
            "\n",
            "        [[-1,  1],\n",
            "         [ 2,  2]]],\n",
            "\n",
            "\n",
            "       [[[ 1,  1],\n",
            "         [ 1,  1]],\n",
            "\n",
            "        [[ 1,  1],\n",
            "         [ 0,  1]],\n",
            "\n",
            "        [[ 1,  2],\n",
            "         [-2,  2]]],\n",
            "\n",
            "\n",
            "       [[[ 1,  1],\n",
            "         [ 1,  2]],\n",
            "\n",
            "        [[ 0,  0],\n",
            "         [ 0,  1]],\n",
            "\n",
            "        [[ 1,  1],\n",
            "         [ 2,  2]]]]), array([[ 1,  1,  1, -1],\n",
            "       [ 1,  1,  1, -1],\n",
            "       [ 2,  1,  1, -1],\n",
            "       [ 1, -2,  1, -1]])]\n",
            "biases :\n",
            " [array([1, 0, 0]), array([ 1, -1,  0,  1]), array([1, 1, 1, 0])]\n",
            "\n",
            "Layer 1\n",
            "z: [[[[3. 3.]\n",
            "   [4. 1.]]\n",
            "\n",
            "  [[3. 2.]\n",
            "   [4. 2.]]\n",
            "\n",
            "  [[4. 1.]\n",
            "   [4. 1.]]]\n",
            "\n",
            "\n",
            " [[[2. 2.]\n",
            "   [1. 2.]]\n",
            "\n",
            "  [[0. 3.]\n",
            "   [2. 2.]]\n",
            "\n",
            "  [[2. 2.]\n",
            "   [1. 3.]]]]\n",
            "activation: [[[[3. 3.]\n",
            "   [4. 1.]]\n",
            "\n",
            "  [[3. 2.]\n",
            "   [4. 2.]]\n",
            "\n",
            "  [[4. 1.]\n",
            "   [4. 1.]]]\n",
            "\n",
            "\n",
            " [[[2. 2.]\n",
            "   [1. 2.]]\n",
            "\n",
            "  [[0. 3.]\n",
            "   [2. 2.]]\n",
            "\n",
            "  [[2. 2.]\n",
            "   [1. 3.]]]]\n",
            "\n",
            "Layer 2\n",
            "z: [[[[19.]]\n",
            "\n",
            "  [[25.]]\n",
            "\n",
            "  [[18.]]\n",
            "\n",
            "  [[30.]]]\n",
            "\n",
            "\n",
            " [[[ 6.]]\n",
            "\n",
            "  [[14.]]\n",
            "\n",
            "  [[22.]]\n",
            "\n",
            "  [[24.]]]]\n",
            "activation: [[[[19.]]\n",
            "\n",
            "  [[25.]]\n",
            "\n",
            "  [[18.]]\n",
            "\n",
            "  [[30.]]]\n",
            "\n",
            "\n",
            " [[[ 6.]]\n",
            "\n",
            "  [[14.]]\n",
            "\n",
            "  [[22.]]\n",
            "\n",
            "  [[24.]]]]\n",
            "\n",
            "Layer 3\n",
            "z: [[111.   3.  93. -92.]\n",
            " [ 89.  -5.  67. -66.]]\n",
            "activation: [[9.99999985e-01 1.24794644e-47 1.52299795e-08 6.89001499e-89]\n",
            " [1.00000000e+00 1.50078576e-41 2.78946809e-10 4.83454164e-68]]\n",
            "\n",
            "output :\n",
            " [[9.99999985e-01 1.24794644e-47 1.52299795e-08 6.89001499e-89]\n",
            " [1.00000000e+00 1.50078576e-41 2.78946809e-10 4.83454164e-68]]\n",
            "\n",
            " loss: [0.49999998 0.5       ]\n",
            "\n",
            " eval: (0.4999999923152736, 0.0)\n",
            "\n",
            "\n",
            "\n",
            "### BACKWARD TESTS ###\n",
            "\n",
            "Epoch 1 / 1\n",
            "\r1/1 [===================>] loss:  0.5\n",
            "dloss_dw\n",
            " [array([[[[[ 2.78947035e-10,  0.00000000e+00],\n",
            "          [ 1.39473546e-10,  4.18420609e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 1.39473517e-10,  0.00000000e+00],\n",
            "          [ 1.39473461e-10,  2.78947007e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 4.18420468e-10,  0.00000000e+00],\n",
            "          [ 2.78947007e-10,  2.78947233e-10]]]],\n",
            "\n",
            "\n",
            "\n",
            "       [[[[ 3.04599581e-08,  3.04599581e-08],\n",
            "          [ 4.56899371e-08,  4.56899371e-08]]],\n",
            "\n",
            "\n",
            "        [[[ 1.52299790e-08,  1.52299790e-08],\n",
            "          [ 3.04599581e-08,  3.04599581e-08]]],\n",
            "\n",
            "\n",
            "        [[[-1.52299791e-08, -1.38964953e-17],\n",
            "          [ 1.52299790e-08,  1.52299790e-08]]]]]), array([[[[[ 5.64748747e-17,  5.64748747e-17],\n",
            "          [ 2.82374373e-17,  5.64748747e-17]],\n",
            "\n",
            "         [[ 0.00000000e+00,  8.47123120e-17],\n",
            "          [ 5.64748747e-17,  5.64748747e-17]],\n",
            "\n",
            "         [[ 5.64748747e-17,  5.64748747e-17],\n",
            "          [ 2.82374373e-17,  8.47123120e-17]]],\n",
            "\n",
            "\n",
            "        [[[ 5.64748747e-17,  5.64748747e-17],\n",
            "          [ 2.82374373e-17,  5.64748747e-17]],\n",
            "\n",
            "         [[ 0.00000000e+00,  8.47123120e-17],\n",
            "          [ 5.64748747e-17,  5.64748747e-17]],\n",
            "\n",
            "         [[ 5.64748747e-17,  5.64748747e-17],\n",
            "          [ 2.82374373e-17,  8.47123120e-17]]],\n",
            "\n",
            "\n",
            "        [[[ 2.78946922e-10,  2.78946922e-10],\n",
            "          [ 1.39473461e-10,  2.78946922e-10]],\n",
            "\n",
            "         [[ 0.00000000e+00,  4.18420383e-10],\n",
            "          [ 2.78946922e-10,  2.78946922e-10]],\n",
            "\n",
            "         [[ 2.78946922e-10,  2.78946922e-10],\n",
            "          [ 1.39473461e-10,  4.18420383e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 5.64748747e-17,  5.64748747e-17],\n",
            "          [ 2.82374373e-17,  5.64748747e-17]],\n",
            "\n",
            "         [[ 0.00000000e+00,  8.47123120e-17],\n",
            "          [ 5.64748747e-17,  5.64748747e-17]],\n",
            "\n",
            "         [[ 5.64748747e-17,  5.64748747e-17],\n",
            "          [ 2.82374373e-17,  8.47123120e-17]]]],\n",
            "\n",
            "\n",
            "\n",
            "       [[[[-4.63216511e-18, -4.63216511e-18],\n",
            "          [-6.17622014e-18, -1.54405504e-18]],\n",
            "\n",
            "         [[-4.63216511e-18, -3.08811007e-18],\n",
            "          [-6.17622014e-18, -3.08811007e-18]],\n",
            "\n",
            "         [[-6.17622014e-18, -1.54405504e-18],\n",
            "          [-6.17622014e-18, -1.54405504e-18]]],\n",
            "\n",
            "\n",
            "        [[[-4.63216511e-18, -4.63216511e-18],\n",
            "          [-6.17622014e-18, -1.54405504e-18]],\n",
            "\n",
            "         [[-4.63216511e-18, -3.08811007e-18],\n",
            "          [-6.17622014e-18, -3.08811007e-18]],\n",
            "\n",
            "         [[-6.17622014e-18, -1.54405504e-18],\n",
            "          [-6.17622014e-18, -1.54405504e-18]]],\n",
            "\n",
            "\n",
            "        [[[ 4.56899371e-08,  4.56899371e-08],\n",
            "          [ 6.09199162e-08,  1.52299790e-08]],\n",
            "\n",
            "         [[ 4.56899371e-08,  3.04599581e-08],\n",
            "          [ 6.09199162e-08,  3.04599581e-08]],\n",
            "\n",
            "         [[ 6.09199162e-08,  1.52299790e-08],\n",
            "          [ 6.09199162e-08,  1.52299790e-08]]],\n",
            "\n",
            "\n",
            "        [[[-4.63216511e-18, -4.63216511e-18],\n",
            "          [-6.17622014e-18, -1.54405504e-18]],\n",
            "\n",
            "         [[-4.63216511e-18, -3.08811007e-18],\n",
            "          [-6.17622014e-18, -3.08811007e-18]],\n",
            "\n",
            "         [[-6.17622014e-18, -1.54405504e-18],\n",
            "          [-6.17622014e-18, -1.54405504e-18]]]]]), array([[[ 8.36840596e-10, -9.00471457e-41, -8.36840427e-10,\n",
            "         -1.45036249e-67],\n",
            "        [ 1.95262806e-09, -2.10110007e-40, -1.95262766e-09,\n",
            "         -3.38417914e-67],\n",
            "        [ 3.06841552e-09, -3.30172868e-40, -3.06841490e-09,\n",
            "         -5.31799580e-67],\n",
            "        [ 3.34736239e-09, -3.60188583e-40, -3.34736171e-09,\n",
            "         -5.80144996e-67]],\n",
            "\n",
            "       [[ 2.89369602e-07, -1.18554907e-46, -2.89369602e-07,\n",
            "         -6.54551395e-88],\n",
            "        [ 3.80749476e-07, -1.55993298e-46, -3.80749476e-07,\n",
            "         -8.61251835e-88],\n",
            "        [ 2.74139623e-07, -1.12315175e-46, -2.74139623e-07,\n",
            "         -6.20101321e-88],\n",
            "        [ 4.56899371e-07, -1.87191958e-46, -4.56899371e-07,\n",
            "         -1.03350220e-87]]])]\n",
            "\n",
            "dloss_db\n",
            " [array([[5.57894126e-10, 2.78947007e-10, 4.18420722e-10],\n",
            "       [6.09199162e-08, 4.56899371e-08, 4.56899371e-08]]), array([[ 2.82374373e-17,  2.82374373e-17,  1.39473461e-10,\n",
            "         2.82374373e-17],\n",
            "       [-1.54405504e-18, -1.54405504e-18,  1.52299790e-08,\n",
            "        -1.54405504e-18]]), array([[ 1.39473433e-10, -1.50078576e-41, -1.39473404e-10,\n",
            "        -2.41727082e-68],\n",
            "       [ 1.52299790e-08, -6.23973193e-48, -1.52299790e-08,\n",
            "        -3.44500734e-89]])]\n",
            "\r1/1 [===================>] loss: 0.4999999923152709 ; acc: 0.0\n",
            "[array([ 1.00000000e+00, -2.29844421e-10, -2.30541789e-10]), array([ 1.00000000e+00, -1.00000000e+00, -7.68472625e-11,  1.00000000e+00]), array([1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.20863541e-70])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov7yeMsLuq6i"
      },
      "source": [
        "## **Tests MNIST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcGPdylHu-4Z"
      },
      "source": [
        "### Fully connected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqKc6XQ7ZMnZ"
      },
      "source": [
        "#### Models Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRj-Nbfq4xPc"
      },
      "source": [
        "Création model numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffQZq6UFuu8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd54483-8c53-4918-90c8-e23e5b332bb5"
      },
      "source": [
        "from keras.datasets import mnist as db\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "(x, y), (x_test_ori, y_test_ori) = db.load_data()\n",
        "\n",
        "model = Model()\n",
        "model.add(Dense(64, activation=Relu()))\n",
        "model.add(Dense(32, activation=Relu()))\n",
        "model.add(Dense(10, activation=Softmax()))\n",
        "\n",
        "x_flat = x.reshape(x.shape[0], -1) / 255\n",
        "model.build(x_flat[0].shape, SGD(learning_rate=1e3), MSE())\n",
        "\n",
        "target = to_categorical(y)\n",
        "\n",
        "print(x_flat.shape, target.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784) (60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MegRE9l42XL"
      },
      "source": [
        "Création modèle torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU5ys_Lr4vlP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "torch_x, torch_y = map(torch.tensor, (x_flat[0:6], target[0:6]))\n",
        "\n",
        "class Torch_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Torch_Model, self).__init__()\n",
        "\n",
        "        # Couches complètement connectées\n",
        "        self.fc1 = nn.Linear(784, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float() # Prevent RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #3 'mat1' in call to _th_addmm_\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.softmax(self.fc3(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def fit(self, data, targets, epochs, opt, loss_func, batch_size, validation_split=None, debug=False, print_debug=\"No print specified\"):\n",
        "        train_dl, valid_dl = self.get_train_valid_data(data, targets, batch_size, validation_split)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train() # Tell the model that training begins (important for some specific operations as Dropout)\n",
        "\n",
        "            for xb, yb in train_dl:\n",
        "                pred = self.forward(xb)\n",
        "                loss = loss_func(pred.float(), yb.float())\n",
        "\n",
        "                # Gradient descent\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                if debug:\n",
        "                    print(\"\\rEpoch : {} -> \".format(epoch), end=\"\")\n",
        "                    print(self.fc3.bias.grad)\n",
        "\n",
        "                opt.zero_grad() # Reset gradient, otherwise future gradients will be added to the current one.\n",
        "\n",
        "            self.eval() #Tell the model that evaluation begins\n",
        "            with torch.no_grad():\n",
        "                valid_loss = sum(loss_func(self(xb), yb) for xb, yb in valid_dl)\n",
        "            if validation_split is not None:\n",
        "                print(\"\\rEpoch : {} -> \".format(epoch), end=\"\")\n",
        "                print(\"  valid loss : {}\".format(valid_loss / len(valid_dl)))\n",
        "\n",
        "    def get_train_valid_data(self, data, targets, batch_size, validation_split):\n",
        "        if validation_split is not None:\n",
        "            data_train, data_valid, labels_train, labels_valid = train_test_split(data, targets, test_size=validation_split, random_state=31, stratify=targets)\n",
        "        else:\n",
        "            data_train, data_valid, labels_train, labels_valid = data, torch.empty(0, *data[0].shape), targets, torch.empty(0, *targets[0].shape)\n",
        "\n",
        "\n",
        "        train_ds = TensorDataset(data_train, labels_train)\n",
        "        train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
        "\n",
        "        valid_ds = TensorDataset(data_valid, labels_valid)\n",
        "        valid_dl = DataLoader(valid_ds, batch_size=2 * batch_size) # double batch for better performance\n",
        "\n",
        "        return train_dl, valid_dl\n",
        "\n",
        "model_torch = Torch_Model()\n",
        "\n",
        "W = [model_torch.fc1, model_torch.fc2, model_torch.fc3]\n",
        "for i in range(len(model.layers)):\n",
        "    W[i].weight.data = torch.Tensor(model.weights[i].T)\n",
        "    W[i].bias.data = torch.Tensor(model.biases[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joGAJoFU47Ha"
      },
      "source": [
        "#### Weights and gradients comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2qSNOoYKWWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e32f3de-87cf-48b8-e040-f2d25d96c651"
      },
      "source": [
        "reset=True\n",
        "\n",
        "np.random.seed(0)\n",
        "if reset:\n",
        "    model = Model()\n",
        "    model.add(Dense(64, activation=Relu()))\n",
        "    model.add(Dense(32, activation=Relu()))\n",
        "    model.add(Dense(10, activation=Softmax()))\n",
        "    model.build(x_flat[0].shape, SGD(learning_rate=1e2), MSE())\n",
        "\n",
        "    model_torch = Torch_Model()\n",
        "    W = [model_torch.fc1, model_torch.fc2, model_torch.fc3]\n",
        "    for i in range(len(model.layers)):\n",
        "        W[i].weight.data = torch.Tensor(model.weights[i].T)\n",
        "        W[i].bias.data = torch.Tensor(model.biases[i])\n",
        "\n",
        "print('\\033[4m' + 'Modèle Torch:' + '\\033[0m')\n",
        "model_torch.fit(torch_x, torch_y, opt=optim.SGD(params=model_torch.parameters(), lr=1e2), loss_func=nn.MSELoss(), batch_size=3, epochs=2, debug=True)\n",
        "\n",
        "\n",
        "print('\\n\\n\\n')\n",
        "\n",
        "print('\\033[4m' + 'Modèle perso:' + '\\033[0m')\n",
        "model.fit(x_flat[0:6], target[0:6], batch_size=3, epochs=2, shuffle=False)\n",
        "print(np.mean(model.dloss_db[2], axis=0))\n",
        "\n",
        "\n",
        "print('\\n\\n\\n')\n",
        "\n",
        "print('\\033[31mTorch biases after training:\\n', model_torch.fc3.bias)\n",
        "print('\\033[32m\\nModel biases after training:\\n', model.biases[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[4mModèle Torch:\u001b[0m\n",
            "\rEpoch : 0 -> tensor([-0.0048,  0.0015,  0.0028,  0.0017, -0.0048, -0.0046,  0.0021,  0.0018,\n",
            "         0.0024,  0.0018])\n",
            "\rEpoch : 0 -> tensor([ 0.0084, -0.0039, -0.0018, -0.0008,  0.0055, -0.0006, -0.0008, -0.0009,\n",
            "        -0.0009, -0.0042])\n",
            "\rEpoch : 1 -> tensor([-7.7176e-06, -5.5411e-03, -2.8895e-03, -5.3693e-03, -3.0289e-04,\n",
            "        -7.7922e-03, -8.5926e-04, -1.1771e-03, -1.4431e-03,  2.5382e-02])\n",
            "\rEpoch : 1 -> tensor([-3.6020e-09, -2.2135e-03, -4.8729e-08, -1.0583e-03, -2.9424e-10,\n",
            "         3.2718e-03, -7.2836e-11, -3.7218e-10, -3.3013e-10, -5.0353e-38])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[4mModèle perso:\u001b[0m\n",
            "Epoch 1 / 2\n",
            "\r1/2 [=========>..........] loss:  0.091\r2/2 [===================>] loss:  0.109\r2/2 [===================>] loss: 0.11471892171147251 ; acc: 0.16666666666666666\n",
            "Epoch 2 / 2\n",
            "\r1/2 [=========>..........] loss:  0.147\r2/2 [===================>] loss:  0.197\r2/2 [===================>] loss: 0.16666641367163867 ; acc: 0.16666666666666666\n",
            "[-3.60209892e-09 -2.21350255e-03 -4.87301983e-08 -1.05829509e-03\n",
            " -2.94243680e-10  3.27185104e-03 -7.28379628e-11 -3.72188766e-10\n",
            " -3.30131368e-10 -5.03562933e-38]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[31mTorch biases after training:\n",
            " Parameter containing:\n",
            "tensor([-0.3453,  0.9126,  0.3509,  0.4812, -0.0335,  0.8738, -0.0346, -0.0409,\n",
            "         0.0550, -2.2935], requires_grad=True)\n",
            "\u001b[32m\n",
            "Model biases after training:\n",
            " [-0.34524984  0.91261038  0.35089335  0.48124958 -0.03352649  0.87381462\n",
            " -0.03455198 -0.04087653  0.05503672 -2.2934898 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGnaOB0rWvgI"
      },
      "source": [
        "#### Test with different optimizers: SGD, RMSprop, Adam *(loss=MSE, layers=Dense)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMJscUA8ac5H"
      },
      "source": [
        "SGD with momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjxCQjBZc_AF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d91a15-79ec-4d6d-a4ba-c5de855cf8c4"
      },
      "source": [
        "from keras.datasets import mnist as db\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "reset=True\n",
        "\n",
        "np.random.seed(0)\n",
        "if reset:\n",
        "    model = Model()\n",
        "    model.add(Dense(64, activation=Relu()))\n",
        "    model.add(Dense(32, activation=Relu()))\n",
        "    model.add(Dense(10, activation=Softmax()))\n",
        "    model.build(x_flat[0].shape, SGD(learning_rate=1e-2, momentum=0.9, nesterov=True), MSE())\n",
        "\n",
        "    model_torch = Torch_Model()\n",
        "    W = [model_torch.fc1, model_torch.fc2, model_torch.fc3]\n",
        "    for i in range(len(model.layers)):\n",
        "        W[i].weight.data = torch.Tensor(model.weights[i].T)\n",
        "        W[i].bias.data = torch.Tensor(model.biases[i])\n",
        "\n",
        "\n",
        "(x, y), (x_test_ori, y_test_ori) = db.load_data()\n",
        "x_flat = x.reshape(x.shape[0], -1) / 255\n",
        "target = to_categorical(y)\n",
        "\n",
        "torch_x, torch_y = map(torch.tensor, (x_flat, target))\n",
        "opt = optim.SGD(model_torch.parameters(), lr=1e-2, momentum=0.9, nesterov=True)\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "model_torch.fit(torch_x, torch_y, 10, opt, loss_func, 32, validation_split=0.2)\n",
        "\n",
        "print('\\n\\n')\n",
        "model.fit(x_flat, target, batch_size=32, epochs=10, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0 ->   valid loss : 0.07985232025384903\n",
            "Epoch : 1 ->   valid loss : 0.03507859259843826\n",
            "Epoch : 2 ->   valid loss : 0.020565800368785858\n",
            "Epoch : 3 ->   valid loss : 0.016691530123353004\n",
            "Epoch : 4 ->   valid loss : 0.015024054795503616\n",
            "Epoch : 5 ->   valid loss : 0.01399229932576418\n",
            "Epoch : 6 ->   valid loss : 0.01325113233178854\n",
            "Epoch : 7 ->   valid loss : 0.012665868736803532\n",
            "Epoch : 8 ->   valid loss : 0.012179220095276833\n",
            "Epoch : 9 ->   valid loss : 0.011748121120035648\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1 / 10\n",
            "1500/1500 [===================>] loss: 0.08031369906980175 ; acc: 0.5409166666666667 ; val_loss: 0.08035272188383143 ; val_acc: 0.5396666666666666\n",
            "Epoch 2 / 10\n",
            "1500/1500 [===================>] loss: 0.0354616990223877 ; acc: 0.7824375 ; val_loss: 0.03564199464924612 ; val_acc: 0.7778333333333334\n",
            "Epoch 3 / 10\n",
            "1500/1500 [===================>] loss: 0.020063161534635752 ; acc: 0.8809583333333333 ; val_loss: 0.020338931871585704 ; val_acc: 0.8773333333333333\n",
            "Epoch 4 / 10\n",
            "1500/1500 [===================>] loss: 0.016209217711997743 ; acc: 0.9004791666666667 ; val_loss: 0.016573054437732486 ; val_acc: 0.8956666666666667\n",
            "Epoch 5 / 10\n",
            "1500/1500 [===================>] loss: 0.014485622332380294 ; acc: 0.9082708333333334 ; val_loss: 0.014901559102399427 ; val_acc: 0.9024166666666666\n",
            "Epoch 6 / 10\n",
            "1500/1500 [===================>] loss: 0.013498531060108314 ; acc: 0.91375 ; val_loss: 0.013885067756809198 ; val_acc: 0.90875\n",
            "Epoch 7 / 10\n",
            "1500/1500 [===================>] loss: 0.012692934791403137 ; acc: 0.9196458333333334 ; val_loss: 0.013151025052418212 ; val_acc: 0.9160833333333334\n",
            "Epoch 8 / 10\n",
            "1500/1500 [===================>] loss: 0.012252619332588874 ; acc: 0.9214791666666666 ; val_loss: 0.012763931909420425 ; val_acc: 0.9173333333333333\n",
            "Epoch 9 / 10\n",
            "1500/1500 [===================>] loss: 0.011659250852908367 ; acc: 0.92575 ; val_loss: 0.01213135887058454 ; val_acc: 0.9216666666666666\n",
            "Epoch 10 / 10\n",
            "1500/1500 [===================>] loss: 0.01131702564326929 ; acc: 0.9279375 ; val_loss: 0.011882402780658519 ; val_acc: 0.9233333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': [0.5409166666666667,\n",
              "  0.7824375,\n",
              "  0.8809583333333333,\n",
              "  0.9004791666666667,\n",
              "  0.9082708333333334,\n",
              "  0.91375,\n",
              "  0.9196458333333334,\n",
              "  0.9214791666666666,\n",
              "  0.92575,\n",
              "  0.9279375],\n",
              " 'loss': [0.08031369906980175,\n",
              "  0.0354616990223877,\n",
              "  0.020063161534635752,\n",
              "  0.016209217711997743,\n",
              "  0.014485622332380294,\n",
              "  0.013498531060108314,\n",
              "  0.012692934791403137,\n",
              "  0.012252619332588874,\n",
              "  0.011659250852908367,\n",
              "  0.01131702564326929],\n",
              " 'val_acc': [0.5396666666666666,\n",
              "  0.7778333333333334,\n",
              "  0.8773333333333333,\n",
              "  0.8956666666666667,\n",
              "  0.9024166666666666,\n",
              "  0.90875,\n",
              "  0.9160833333333334,\n",
              "  0.9173333333333333,\n",
              "  0.9216666666666666,\n",
              "  0.9233333333333333],\n",
              " 'val_loss': [0.08035272188383143,\n",
              "  0.03564199464924612,\n",
              "  0.020338931871585704,\n",
              "  0.016573054437732486,\n",
              "  0.014901559102399427,\n",
              "  0.013885067756809198,\n",
              "  0.013151025052418212,\n",
              "  0.012763931909420425,\n",
              "  0.01213135887058454,\n",
              "  0.011882402780658519]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlNM05Vwaf6a"
      },
      "source": [
        "RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f-1wm0jDUEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0308fd0e-fa25-4584-a622-ab60c70d9aec"
      },
      "source": [
        "from keras.datasets import mnist as db\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "reset=True\n",
        "\n",
        "np.random.seed(0)\n",
        "if reset:\n",
        "    model = Model()\n",
        "    model.add(Dense(64, activation=Relu()))\n",
        "    model.add(Dense(32, activation=Relu()))\n",
        "    model.add(Dense(10, activation=Softmax()))\n",
        "    model.build(x_flat[0].shape, RMSProp(learning_rate=1e-2, rho=0.9), MSE())\n",
        "\n",
        "    model_torch = Torch_Model()\n",
        "    W = [model_torch.fc1, model_torch.fc2, model_torch.fc3]\n",
        "    for i in range(len(model.layers)):\n",
        "        W[i].weight.data = torch.Tensor(model.weights[i].T)\n",
        "        W[i].bias.data = torch.Tensor(model.biases[i])\n",
        "\n",
        "\n",
        "(x, y), (x_test_ori, y_test_ori) = db.load_data()\n",
        "x_flat = x.reshape(x.shape[0], -1) / 255\n",
        "target = to_categorical(y)\n",
        "\n",
        "torch_x, torch_y = map(torch.tensor, (x_flat, target))\n",
        "opt = optim.RMSprop(model_torch.parameters(), lr=1e-2, alpha=0.9, eps=1e-7)\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "model_torch.fit(torch_x, torch_y, 10, opt, loss_func, 32, validation_split=0.2)\n",
        "\n",
        "\n",
        "print('\\n\\n')\n",
        "model.fit(x_flat, target, batch_size=32, epochs=10, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1 ->   valid loss : 0.009461630135774612\n",
            "Epoch : 2 ->   valid loss : 0.009961865842342377\n",
            "Epoch : 3 ->   valid loss : 0.009508982300758362\n",
            "Epoch : 4 ->   valid loss : 0.009421045891940594\n",
            "Epoch : 5 ->   valid loss : 0.01046204287558794\n",
            "Epoch : 6 ->   valid loss : 0.01166702713817358\n",
            "Epoch : 7 ->   valid loss : 0.011157970875501633\n",
            "Epoch : 8 ->   valid loss : 0.010938932187855244\n",
            "Epoch : 9 ->   valid loss : 0.00858311727643013\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1 / 10\n",
            "1500/1500 [===================>] loss: 0.009284994106108627 ; acc: 0.9427291666666666 ; val_loss: 0.00999487455804005 ; val_acc: 0.9396666666666667\n",
            "Epoch 2 / 10\n",
            "1500/1500 [===================>] loss: 0.00816418799773018 ; acc: 0.9501666666666667 ; val_loss: 0.009039026327388278 ; val_acc: 0.9439166666666666\n",
            "Epoch 3 / 10\n",
            "1500/1500 [===================>] loss: 0.00787672052824159 ; acc: 0.9520208333333333 ; val_loss: 0.0084595947178186 ; val_acc: 0.9493333333333334\n",
            "Epoch 4 / 10\n",
            "1500/1500 [===================>] loss: 0.008328451219446057 ; acc: 0.9536041666666667 ; val_loss: 0.009806585210980816 ; val_acc: 0.94475\n",
            "Epoch 5 / 10\n",
            "1500/1500 [===================>] loss: 0.008362729142673764 ; acc: 0.95425 ; val_loss: 0.009298338742890589 ; val_acc: 0.9486666666666667\n",
            "Epoch 6 / 10\n",
            "1500/1500 [===================>] loss: 0.0074572630032309194 ; acc: 0.9595625 ; val_loss: 0.00884804422455337 ; val_acc: 0.95225\n",
            "Epoch 7 / 10\n",
            "1500/1500 [===================>] loss: 0.007970491480494786 ; acc: 0.95675 ; val_loss: 0.009380050374081511 ; val_acc: 0.9484166666666667\n",
            "Epoch 8 / 10\n",
            "1500/1500 [===================>] loss: 0.007803162636766614 ; acc: 0.9582708333333333 ; val_loss: 0.009233081982079751 ; val_acc: 0.95\n",
            "Epoch 9 / 10\n",
            "1500/1500 [===================>] loss: 0.007675581280830335 ; acc: 0.9590416666666667 ; val_loss: 0.009447714894157253 ; val_acc: 0.9495833333333333\n",
            "Epoch 10 / 10\n",
            "1500/1500 [===================>] loss: 0.007430626391343687 ; acc: 0.9610625 ; val_loss: 0.008990808663455498 ; val_acc: 0.9523333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': [0.9427291666666666,\n",
              "  0.9501666666666667,\n",
              "  0.9520208333333333,\n",
              "  0.9536041666666667,\n",
              "  0.95425,\n",
              "  0.9595625,\n",
              "  0.95675,\n",
              "  0.9582708333333333,\n",
              "  0.9590416666666667,\n",
              "  0.9610625],\n",
              " 'loss': [0.009284994106108627,\n",
              "  0.00816418799773018,\n",
              "  0.00787672052824159,\n",
              "  0.008328451219446057,\n",
              "  0.008362729142673764,\n",
              "  0.0074572630032309194,\n",
              "  0.007970491480494786,\n",
              "  0.007803162636766614,\n",
              "  0.007675581280830335,\n",
              "  0.007430626391343687],\n",
              " 'val_acc': [0.9396666666666667,\n",
              "  0.9439166666666666,\n",
              "  0.9493333333333334,\n",
              "  0.94475,\n",
              "  0.9486666666666667,\n",
              "  0.95225,\n",
              "  0.9484166666666667,\n",
              "  0.95,\n",
              "  0.9495833333333333,\n",
              "  0.9523333333333334],\n",
              " 'val_loss': [0.00999487455804005,\n",
              "  0.009039026327388278,\n",
              "  0.0084595947178186,\n",
              "  0.009806585210980816,\n",
              "  0.009298338742890589,\n",
              "  0.00884804422455337,\n",
              "  0.009380050374081511,\n",
              "  0.009233081982079751,\n",
              "  0.009447714894157253,\n",
              "  0.008990808663455498]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0W8JrbXaljA"
      },
      "source": [
        "Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rwoGQKhrItQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45eedae7-c496-4fb1-c3e9-66165c78f1cf"
      },
      "source": [
        "from keras.datasets import mnist as db\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "reset=True\n",
        "\n",
        "np.random.seed(0)\n",
        "if reset:\n",
        "    model = Model()\n",
        "    model.add(Dense(64, activation=Relu()))\n",
        "    model.add(Dense(32, activation=Relu()))\n",
        "    model.add(Dense(10, activation=Softmax()))\n",
        "    model.build(x_flat[0].shape, Adam(), MSE())\n",
        "\n",
        "    model_torch = Torch_Model()\n",
        "    W = [model_torch.fc1, model_torch.fc2, model_torch.fc3]\n",
        "    for i in range(len(model.layers)):\n",
        "        W[i].weight.data = torch.Tensor(model.weights[i].T)\n",
        "        W[i].bias.data = torch.Tensor(model.biases[i])\n",
        "\n",
        "\n",
        "(x, y), (x_test_ori, y_test_ori) = db.load_data()\n",
        "x_flat = x.reshape(x.shape[0], -1) / 255\n",
        "target = to_categorical(y)\n",
        "\n",
        "torch_x, torch_y = map(torch.tensor, (x_flat, target))\n",
        "opt = optim.Adam(model_torch.parameters(), eps=1e-7)\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "model_torch.fit(torch_x, torch_y, 10, opt, loss_func, 32, validation_split=0.2)\n",
        "\n",
        "print('\\n\\n')\n",
        "model.fit(x_flat, target, batch_size=32, epochs=10, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0 ->   valid loss : 0.010441718623042107\n",
            "Epoch : 1 ->   valid loss : 0.00850758422166109\n",
            "Epoch : 2 ->   valid loss : 0.006298528052866459\n",
            "Epoch : 3 ->   valid loss : 0.00586102157831192\n",
            "Epoch : 4 ->   valid loss : 0.00538274459540844\n",
            "Epoch : 5 ->   valid loss : 0.004819015972316265\n",
            "Epoch : 6 ->   valid loss : 0.004964479710906744\n",
            "Epoch : 7 ->   valid loss : 0.00511875981464982\n",
            "Epoch : 8 ->   valid loss : 0.005127946846187115\n",
            "Epoch : 9 ->   valid loss : 0.004757270682603121\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1 / 10\n",
            "1500/1500 [===================>] loss: 0.009649704995136668 ; acc: 0.9387083333333334 ; val_loss: 0.010162384294815809 ; val_acc: 0.9354166666666667\n",
            "Epoch 2 / 10\n",
            "1500/1500 [===================>] loss: 0.006248801552844896 ; acc: 0.96125 ; val_loss: 0.006908236830858858 ; val_acc: 0.9561666666666667\n",
            "Epoch 3 / 10\n",
            "1500/1500 [===================>] loss: 0.004658047847108141 ; acc: 0.9713541666666666 ; val_loss: 0.005861256997749946 ; val_acc: 0.96125\n",
            "Epoch 4 / 10\n",
            "1500/1500 [===================>] loss: 0.0039605144820486155 ; acc: 0.9756041666666667 ; val_loss: 0.005245693705862712 ; val_acc: 0.9665\n",
            "Epoch 5 / 10\n",
            "1500/1500 [===================>] loss: 0.004583744783797805 ; acc: 0.9715 ; val_loss: 0.006096147702271335 ; val_acc: 0.9606666666666667\n",
            "Epoch 6 / 10\n",
            "1500/1500 [===================>] loss: 0.0039312892851325115 ; acc: 0.9753125 ; val_loss: 0.005846090735484642 ; val_acc: 0.96275\n",
            "Epoch 7 / 10\n",
            "1500/1500 [===================>] loss: 0.003331136459504414 ; acc: 0.9797083333333333 ; val_loss: 0.005421346135729039 ; val_acc: 0.9665\n",
            "Epoch 8 / 10\n",
            "1500/1500 [===================>] loss: 0.0024773987884121724 ; acc: 0.9850625 ; val_loss: 0.004754468531897233 ; val_acc: 0.9699166666666666\n",
            "Epoch 9 / 10\n",
            "1500/1500 [===================>] loss: 0.0021874198933776617 ; acc: 0.9868541666666667 ; val_loss: 0.004318116315585321 ; val_acc: 0.9730833333333333\n",
            "Epoch 10 / 10\n",
            "1500/1500 [===================>] loss: 0.002064684934489073 ; acc: 0.9878958333333333 ; val_loss: 0.004495118669840775 ; val_acc: 0.9714166666666667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': [0.9387083333333334,\n",
              "  0.96125,\n",
              "  0.9713541666666666,\n",
              "  0.9756041666666667,\n",
              "  0.9715,\n",
              "  0.9753125,\n",
              "  0.9797083333333333,\n",
              "  0.9850625,\n",
              "  0.9868541666666667,\n",
              "  0.9878958333333333],\n",
              " 'loss': [0.009649704995136668,\n",
              "  0.006248801552844896,\n",
              "  0.004658047847108141,\n",
              "  0.0039605144820486155,\n",
              "  0.004583744783797805,\n",
              "  0.0039312892851325115,\n",
              "  0.003331136459504414,\n",
              "  0.0024773987884121724,\n",
              "  0.0021874198933776617,\n",
              "  0.002064684934489073],\n",
              " 'val_acc': [0.9354166666666667,\n",
              "  0.9561666666666667,\n",
              "  0.96125,\n",
              "  0.9665,\n",
              "  0.9606666666666667,\n",
              "  0.96275,\n",
              "  0.9665,\n",
              "  0.9699166666666666,\n",
              "  0.9730833333333333,\n",
              "  0.9714166666666667],\n",
              " 'val_loss': [0.010162384294815809,\n",
              "  0.006908236830858858,\n",
              "  0.005861256997749946,\n",
              "  0.005245693705862712,\n",
              "  0.006096147702271335,\n",
              "  0.005846090735484642,\n",
              "  0.005421346135729039,\n",
              "  0.004754468531897233,\n",
              "  0.004318116315585321,\n",
              "  0.004495118669840775]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNXOUX_Z8cpz"
      },
      "source": [
        "#### Test with Crossentropy loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doujRXHo8gO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10afc0de-25a9-4e26-9700-7df2ed100478"
      },
      "source": [
        "from keras.datasets import mnist as db\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "reset=True\n",
        "\n",
        "np.random.seed(0)\n",
        "if reset:\n",
        "    model = Model()\n",
        "    model.add(Dense(64, activation=Relu()))\n",
        "    model.add(Dense(32, activation=Relu()))\n",
        "    model.add(Dense(10, activation=Softmax()))\n",
        "    model.build(x_flat[0].shape, Adam(), CategoricalCrossentropy())\n",
        "\n",
        "    model_torch = Torch_Model()\n",
        "    W = [model_torch.fc1, model_torch.fc2, model_torch.fc3]\n",
        "    for i in range(len(model.layers)):\n",
        "        W[i].weight.data = torch.Tensor(model.weights[i].T)\n",
        "        W[i].bias.data = torch.Tensor(model.biases[i])\n",
        "\n",
        "\n",
        "(x, y), (x_test_ori, y_test_ori) = db.load_data()\n",
        "x_flat = x.reshape(x.shape[0], -1) / 255\n",
        "target = to_categorical(y)\n",
        "\n",
        "model.fit(x_flat, target, batch_size=32, epochs=10, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 10\n",
            "1500/1500 [===================>] loss: 0.18599681664680726 ; acc: 0.9469583333333333 ; val_loss: 0.19534211113577477 ; val_acc: 0.9428333333333333\n",
            "Epoch 2 / 10\n",
            "1500/1500 [===================>] loss: 0.1275125155110217 ; acc: 0.9630833333333333 ; val_loss: 0.14693358748812727 ; val_acc: 0.9575\n",
            "Epoch 3 / 10\n",
            "1500/1500 [===================>] loss: 0.09665136023961003 ; acc: 0.9710416666666667 ; val_loss: 0.12694034218090486 ; val_acc: 0.9623333333333334\n",
            "Epoch 4 / 10\n",
            "1500/1500 [===================>] loss: 0.07032215290004781 ; acc: 0.9782083333333333 ; val_loss: 0.10902708831202618 ; val_acc: 0.9685833333333334\n",
            "Epoch 5 / 10\n",
            "1500/1500 [===================>] loss: 0.0661972863158912 ; acc: 0.9785833333333334 ; val_loss: 0.10904985999588356 ; val_acc: 0.968\n",
            "Epoch 6 / 10\n",
            "1500/1500 [===================>] loss: 0.04770082863233563 ; acc: 0.9853541666666666 ; val_loss: 0.10290455023792977 ; val_acc: 0.9698333333333333\n",
            "Epoch 7 / 10\n",
            "1500/1500 [===================>] loss: 0.041585438796825386 ; acc: 0.9871041666666667 ; val_loss: 0.10452949597340595 ; val_acc: 0.9700833333333333\n",
            "Epoch 8 / 10\n",
            "1500/1500 [===================>] loss: 0.03273594335532735 ; acc: 0.9900416666666667 ; val_loss: 0.09948765722796336 ; val_acc: 0.97225\n",
            "Epoch 9 / 10\n",
            "1500/1500 [===================>] loss: 0.027800180620587412 ; acc: 0.991625 ; val_loss: 0.09524671833367082 ; val_acc: 0.97225\n",
            "Epoch 10 / 10\n",
            "1500/1500 [===================>] loss: 0.031399127772178466 ; acc: 0.9896875 ; val_loss: 0.1062253472052106 ; val_acc: 0.9700833333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': [0.9469583333333333,\n",
              "  0.9630833333333333,\n",
              "  0.9710416666666667,\n",
              "  0.9782083333333333,\n",
              "  0.9785833333333334,\n",
              "  0.9853541666666666,\n",
              "  0.9871041666666667,\n",
              "  0.9900416666666667,\n",
              "  0.991625,\n",
              "  0.9896875],\n",
              " 'loss': [0.18599681664680726,\n",
              "  0.1275125155110217,\n",
              "  0.09665136023961003,\n",
              "  0.07032215290004781,\n",
              "  0.0661972863158912,\n",
              "  0.04770082863233563,\n",
              "  0.041585438796825386,\n",
              "  0.03273594335532735,\n",
              "  0.027800180620587412,\n",
              "  0.031399127772178466],\n",
              " 'val_acc': [0.9428333333333333,\n",
              "  0.9575,\n",
              "  0.9623333333333334,\n",
              "  0.9685833333333334,\n",
              "  0.968,\n",
              "  0.9698333333333333,\n",
              "  0.9700833333333333,\n",
              "  0.97225,\n",
              "  0.97225,\n",
              "  0.9700833333333333],\n",
              " 'val_loss': [0.19534211113577477,\n",
              "  0.14693358748812727,\n",
              "  0.12694034218090486,\n",
              "  0.10902708831202618,\n",
              "  0.10904985999588356,\n",
              "  0.10290455023792977,\n",
              "  0.10452949597340595,\n",
              "  0.09948765722796336,\n",
              "  0.09524671833367082,\n",
              "  0.1062253472052106]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ig8J0pKvE4A"
      },
      "source": [
        "### Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUJ3e-OavKLS"
      },
      "source": [
        "#### Models Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geRRnONy9HRL"
      },
      "source": [
        "Création convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0oA4XS7txlh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fba43f64-f1a6-4b1d-e586-3f5ac6d6d321"
      },
      "source": [
        "from keras.datasets import mnist as db\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "(x, y), (x_test_ori, y_test_ori) = db.load_data()\n",
        "\n",
        "model = Model()\n",
        "model.add(Conv2D(32, (5, 5), stride=(2, 2), activation=Relu()))\n",
        "model.add(Conv2D(64, (3, 3), activation=Relu()))\n",
        "model.add(Dense(64, activation=Relu()))\n",
        "model.add(Dense(10, activation=Softmax()))\n",
        "\n",
        "model.build(x[0].shape, SGD(learning_rate=1e-3), MSE())\n",
        "\n",
        "target = to_categorical(y)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "layer | input shape | output shape | nb_param |\n",
            "-----------------------------------------------\n",
            "Conv2D| (28, 28)    | (32, 12, 12) | 832      |\n",
            "-----------------------------------------------\n",
            "Conv2D| (32, 12, 12)| (64, 10, 10) | 18496    |\n",
            "-----------------------------------------------\n",
            "Dense | (64, 10, 10)| (64,)        | 409664   |\n",
            "-----------------------------------------------\n",
            "Dense | (64,)       | (10,)        | 650      |\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoQ-bHl8txlj"
      },
      "source": [
        "Création convolution Torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of7pTNHCtxlj"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "x_reshaped = np.expand_dims(x, 1)\n",
        "torch_x, torch_y = map(torch.tensor, (x_reshaped[0:6]/255, target[0:6]))\n",
        "\n",
        "class Torch_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Torch_Model, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 16, (5, 5), stride=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(16, 32, (3, 3))\n",
        "        \n",
        "        self.fc1 = nn.Linear(32 * 10 * 10, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x, debug=False):\n",
        "        bs = x.shape[0]\n",
        "\n",
        "        x = x.float() # Prevent RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #3 'mat1' in call to _th_addmm_\n",
        "        \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        \n",
        "        x = x.view(bs, -1)\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        if debug:\n",
        "            x = self.fc2(x) # To see gradient\n",
        "        else:\n",
        "            x = F.softmax(self.fc2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def fit(self, data, targets, epochs, opt, loss_func, batch_size, validation_split=None, debug=False):\n",
        "        train_dl, valid_dl = self.get_train_valid_data(data, targets, batch_size, validation_split)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train() # Tell the model that training begins (important for some specific operations as Dropout)\n",
        "\n",
        "            for xb, yb in train_dl:\n",
        "                pred = self.forward(xb)\n",
        "                loss = loss_func(pred.float(), yb.float())\n",
        "\n",
        "                # Gradient descent\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                if debug:\n",
        "                    print(\"\\rEpoch : {} -> \".format(epoch), end=\"\")\n",
        "                    print(self.fc2.bias.grad)\n",
        "\n",
        "                opt.zero_grad() # Reset gradient, otherwise future gradients will be added to the current one.\n",
        "                    \n",
        "\n",
        "            self.eval() #Tell the model that evaluation begins\n",
        "            with torch.no_grad():\n",
        "                valid_loss = sum(loss_func(self(xb), yb) for xb, yb in valid_dl)\n",
        "            if validation_split is not None:\n",
        "                print(\"\\rEpoch : {} -> \".format(epoch), end=\"\")\n",
        "                print(\"  valid loss : {}\".format(valid_loss / len(valid_dl)))\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    train_loss = sum(loss_func(self(xb), yb) for xb, yb in train_dl)\n",
        "                print(\"\\rEpoch : {} -> \".format(epoch), end=\"\")\n",
        "                print(\"  loss : {}\".format(train_loss / len(train_dl)))\n",
        "\n",
        "    def get_train_valid_data(self, data, targets, batch_size, validation_split):\n",
        "        if validation_split is not None:\n",
        "            data_train, data_valid, labels_train, labels_valid = train_test_split(data, targets, test_size=validation_split, random_state=0, stratify=targets)\n",
        "        else:\n",
        "            data_train, data_valid, labels_train, labels_valid = data, torch.empty(0, *data[0].shape), targets, torch.empty(0, *targets[0].shape)\n",
        "\n",
        "\n",
        "        train_ds = TensorDataset(data_train, labels_train)\n",
        "        train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
        "\n",
        "        valid_ds = TensorDataset(data_valid, labels_valid)\n",
        "        valid_dl = DataLoader(valid_ds, batch_size=2 * batch_size) # double batch for better performance\n",
        "\n",
        "        return train_dl, valid_dl\n",
        "\n",
        "model_torch = Torch_Model()\n",
        "\n",
        "W = [model_torch.conv1, model_torch.conv2, model_torch.fc1, model_torch.fc2]\n",
        "for i in range(len(model.layers)):\n",
        "    if model.layers[i].type_name == 'Conv2D':\n",
        "        W[i].weight.data = torch.Tensor(model.weights[i])\n",
        "    else:\n",
        "        W[i].weight.data = torch.Tensor(model.weights[i].T)\n",
        "    W[i].bias.data = torch.Tensor(model.biases[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go3y5bAXtxll"
      },
      "source": [
        "#### Weights and gradients comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J70fJxrclo9F"
      },
      "source": [
        "Step by step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bhc6XBZllcL",
        "outputId": "a36f8396-da44-4288-b6d2-48f50ff070a7"
      },
      "source": [
        "np.random.seed(0)\n",
        "reset = True\n",
        "if reset:\n",
        "    model = Model()\n",
        "    model.add(Conv2D(16, (5, 5), stride=(2, 2), activation=Relu()))\n",
        "    model.add(Conv2D(32, (3, 3), activation=Relu()))\n",
        "    model.add(Dense(64, activation=Relu()))\n",
        "    model.add(Dense(10, activation=Softmax()))\n",
        "    model.build(x[0].shape, SGD(learning_rate=1e2), MSE())\n",
        "\n",
        "    model_torch = Torch_Model()\n",
        "    W = [model_torch.conv1, model_torch.conv2, model_torch.fc1, model_torch.fc2]\n",
        "    for i in range(len(model.layers)):\n",
        "        if model.layers[i].type_name == 'Conv2D':\n",
        "            W[i].weight.data = torch.Tensor(model.weights[i])\n",
        "        else:\n",
        "            W[i].weight.data = torch.Tensor(model.weights[i].T)\n",
        "        W[i].bias.data = torch.Tensor(model.biases[i])\n",
        "\n",
        "\n",
        "### Step by step\n",
        "# Torch\n",
        "torch_x, torch_y = map(torch.tensor, (np.expand_dims(x[0:6], 1)/255, target[0:6]))\n",
        "pred_torch = model_torch.forward(torch_x, debug=False)\n",
        "loss_func = nn.MSELoss()\n",
        "loss_torch = loss_func(pred_torch.float(), torch_y.float())\n",
        "loss_torch.backward()\n",
        "opt = optim.SGD(model_torch.parameters(), lr=1e2, momentum=0.0)\n",
        "opt.step()\n",
        "\n",
        "# Numpy\n",
        "output = model(x[0:6]/255)\n",
        "loss = np.mean(model.loss(output, target[0:6]))\n",
        "model.fit(x[0:6] / 255, target[0:6], batch_size=6, epochs=1)\n",
        "\n",
        "# print\n",
        "print('pred:\\n', output)\n",
        "print('\\npred_torch:\\n', pred_torch)\n",
        "\n",
        "print('\\n\\nloss:', loss)\n",
        "print('loss_torch', loss_torch)\n",
        "\n",
        "print('\\n\\ngradient:\\n', np.mean(model.dloss_db[1], axis=0))\n",
        "print('\\ngradient torch:\\n', model_torch.conv2.bias.grad)\n",
        "\n",
        "print('\\n\\n\\033[32mModel biases after backward:\\n', model.biases[0])\n",
        "print('\\033[31mTorch biases after step:\\n', model_torch.conv1.bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 1\n",
            "1/1 [===================>] loss: 0.07253840091546328 ; acc: 0.5\n",
            "pred:\n",
            " [[0.08703774 0.10198284 0.09616278 0.10318064 0.12153796 0.09577594\n",
            "  0.08182547 0.10356675 0.10220511 0.10672475]\n",
            " [0.08502192 0.10510033 0.09715579 0.1067434  0.1198817  0.09339803\n",
            "  0.0814874  0.10181985 0.10051626 0.10887531]\n",
            " [0.08400857 0.10165665 0.10484243 0.10448844 0.11376131 0.09752665\n",
            "  0.08441966 0.10130325 0.10115998 0.10683306]\n",
            " [0.08588052 0.10522487 0.10197052 0.10365024 0.11582657 0.09680488\n",
            "  0.08416125 0.10200282 0.10148975 0.10298858]\n",
            " [0.08658191 0.1041941  0.10140924 0.10360541 0.11274054 0.09720112\n",
            "  0.08754627 0.09973386 0.09861468 0.10837288]\n",
            " [0.08847483 0.10329268 0.10175962 0.10156762 0.11620692 0.10000738\n",
            "  0.08438389 0.09827838 0.09914488 0.10688381]]\n",
            "\n",
            "pred_torch:\n",
            " tensor([[0.0870, 0.1020, 0.0962, 0.1032, 0.1215, 0.0958, 0.0818, 0.1036, 0.1022,\n",
            "         0.1067],\n",
            "        [0.0850, 0.1051, 0.0972, 0.1067, 0.1199, 0.0934, 0.0815, 0.1018, 0.1005,\n",
            "         0.1089],\n",
            "        [0.0840, 0.1017, 0.1048, 0.1045, 0.1138, 0.0975, 0.0844, 0.1013, 0.1012,\n",
            "         0.1068],\n",
            "        [0.0859, 0.1052, 0.1020, 0.1037, 0.1158, 0.0968, 0.0842, 0.1020, 0.1015,\n",
            "         0.1030],\n",
            "        [0.0866, 0.1042, 0.1014, 0.1036, 0.1127, 0.0972, 0.0875, 0.0997, 0.0986,\n",
            "         0.1084],\n",
            "        [0.0885, 0.1033, 0.1018, 0.1016, 0.1162, 0.1000, 0.0844, 0.0983, 0.0991,\n",
            "         0.1069]], grad_fn=<SoftmaxBackward>)\n",
            "\n",
            "\n",
            "loss: 0.08975473067525196\n",
            "loss_torch tensor(0.0898, grad_fn=<MseLossBackward>)\n",
            "\n",
            "\n",
            "gradient:\n",
            " [-2.38610894e-04 -1.72768223e-05 -2.80717437e-05 -1.66913056e-04\n",
            " -4.34115039e-04  6.64727322e-04  1.42814220e-04  3.49048862e-04\n",
            "  5.66894945e-05  5.19822327e-06 -1.81853832e-04 -3.05160675e-04\n",
            " -2.32735484e-04  2.87799001e-04 -1.33663364e-04  0.00000000e+00\n",
            "  1.96868610e-05  8.70992025e-04 -6.36766442e-05 -4.87152404e-05\n",
            "  4.96326830e-04 -1.70554957e-04  3.34091896e-04 -1.37975683e-04\n",
            "  2.46798513e-04  7.40998984e-05 -2.30280926e-04 -5.20910138e-05\n",
            "  4.22292713e-04 -2.33491980e-04 -1.66086541e-05 -1.09625141e-04]\n",
            "\n",
            "gradient torch:\n",
            " tensor([-2.3861e-04, -1.7277e-05, -2.8072e-05, -1.6691e-04, -4.3412e-04,\n",
            "         6.6473e-04,  1.4281e-04,  3.4905e-04,  5.6690e-05,  5.1982e-06,\n",
            "        -1.8185e-04, -3.0516e-04, -2.3274e-04,  2.8780e-04, -1.3366e-04,\n",
            "         0.0000e+00,  1.9687e-05,  8.7099e-04, -6.3677e-05, -4.8715e-05,\n",
            "         4.9633e-04, -1.7056e-04,  3.3409e-04, -1.3798e-04,  2.4680e-04,\n",
            "         7.4100e-05, -2.3028e-04, -5.2091e-05,  4.2229e-04, -2.3349e-04,\n",
            "        -1.6609e-05, -1.0963e-04])\n",
            "\n",
            "\n",
            "\u001b[32mModel biases after backward:\n",
            " [-0.07826194 -0.15169836  0.08797693  0.01170093 -0.23817744  0.06321301\n",
            "  0.08426084  0.01876541 -0.00533795 -0.11421874 -0.20348518  0.12673118\n",
            " -0.04546479 -0.04115315  0.23887297  0.09962619]\n",
            "\u001b[31mTorch biases after step:\n",
            " Parameter containing:\n",
            "tensor([-0.0783, -0.1517,  0.0880,  0.0117, -0.2382,  0.0632,  0.0843,  0.0188,\n",
            "        -0.0053, -0.1142, -0.2035,  0.1267, -0.0455, -0.0412,  0.2389,  0.0996],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44DLbOoLltvY"
      },
      "source": [
        "Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psNQ3Juktxll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace925e2-54e3-47b8-8ed5-82f57c3bdcf8"
      },
      "source": [
        "np.random.seed(0)\n",
        "reset = True\n",
        "if reset:\n",
        "    model = Model()\n",
        "    model.add(Conv2D(16, (5, 5), stride=(2, 2), activation=Relu()))\n",
        "    model.add(Conv2D(32, (3, 3), activation=Relu()))\n",
        "    model.add(Dense(64, activation=Relu()))\n",
        "    model.add(Dense(10, activation=Softmax()))\n",
        "    #model.build(x[0].shape, SGD(learning_rate=1e1), MSE())\n",
        "    model.build(x[0].shape, Adam(learning_rate=1e-3), MSE())\n",
        "\n",
        "    model_torch = Torch_Model()\n",
        "    W = [model_torch.conv1, model_torch.conv2, model_torch.fc1, model_torch.fc2]\n",
        "    for i in range(len(model.layers)):\n",
        "        if model.layers[i].type_name == 'Conv2D':\n",
        "            W[i].weight.data = torch.Tensor(model.weights[i])\n",
        "        else:\n",
        "            W[i].weight.data = torch.Tensor(model.weights[i].T)\n",
        "        W[i].bias.data = torch.Tensor(model.biases[i])\n",
        "\n",
        "### Fit\n",
        "print('Before training:')\n",
        "print('\\033[31mTorch biases before step:\\n', model_torch.conv2.weight.detach().numpy()[0:2,0])\n",
        "print('\\033[32mModel biases before backward:\\n', model.weights[1][0:2,0])\n",
        "\n",
        "torch_x, torch_y = map(torch.tensor, (np.expand_dims(x[0:1000], 1)/255, target[0:1000]))\n",
        "opt = optim.Adam(model_torch.parameters())\n",
        "loss_func = nn.MSELoss()\n",
        "model_torch.fit(torch_x, torch_y, 3, opt, loss_func, 32)\n",
        "\n",
        "model.fit(x[0:1000]/255, target[0:1000], batch_size=32, epochs=3, shuffle=False)\n",
        "\n",
        "print('\\nAfter training:')\n",
        "print('\\033[31mTorch biases after step:\\n', model_torch.conv2.weight.detach().numpy()[0:2,0])\n",
        "print('\\033[32mModel biases after backward:\\n', model.weights[1][0:2, 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before training:\n",
            "\u001b[31mTorch biases before step:\n",
            " [[[ 0.02185107  0.00837107  0.08956656]\n",
            "  [ 0.10132777  0.05477161  0.03430791]\n",
            "  [-0.12342567  0.00729038 -0.00766663]]\n",
            "\n",
            " [[ 0.11851443 -0.00262781  0.01149459]\n",
            "  [-0.1049796  -0.04295937  0.01158226]\n",
            "  [ 0.0209043   0.03635039  0.0005084 ]]]\n",
            "\u001b[32mModel biases before backward:\n",
            " [[[ 0.02185107  0.00837107  0.08956656]\n",
            "  [ 0.10132777  0.05477161  0.03430791]\n",
            "  [-0.12342567  0.00729038 -0.00766663]]\n",
            "\n",
            " [[ 0.11851444 -0.00262781  0.01149459]\n",
            "  [-0.1049796  -0.04295937  0.01158226]\n",
            "  [ 0.0209043   0.03635039  0.0005084 ]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0 ->   loss : 0.03038010746240616\n",
            "Epoch : 1 ->   loss : 0.018230507150292397\n",
            "Epoch : 2 ->   loss : 0.012022891081869602\n",
            "Epoch 1 / 3\n",
            "32/32 [===================>] loss: 0.030833028422243412 ; acc: 0.795\n",
            "Epoch 2 / 3\n",
            "32/32 [===================>] loss: 0.018865459066098284 ; acc: 0.874\n",
            "Epoch 3 / 3\n",
            "32/32 [===================>] loss: 0.013197536737338865 ; acc: 0.916\n",
            "\n",
            "After training:\n",
            "\u001b[31mTorch biases after step:\n",
            " [[[ 0.05097174  0.04354589  0.12376972]\n",
            "  [ 0.13105668  0.09378874  0.07115868]\n",
            "  [-0.11383     0.02368421  0.01637953]]\n",
            "\n",
            " [[ 0.14092134  0.01167447  0.0285505 ]\n",
            "  [-0.10264634 -0.06674211 -0.01634342]\n",
            "  [ 0.0286122   0.03320579  0.00622831]]]\n",
            "\u001b[32mModel biases after backward:\n",
            " [[[ 0.0518132   0.04463733  0.12420274]\n",
            "  [ 0.13239241  0.09464379  0.07077545]\n",
            "  [-0.11193715  0.02539659  0.01643408]]\n",
            "\n",
            " [[ 0.14208556  0.01237022  0.02756373]\n",
            "  [-0.10344194 -0.06687652 -0.01619571]\n",
            "  [ 0.02630244  0.03266892  0.00650498]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFHlO0-6o0U_"
      },
      "source": [
        "#### Final test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z6kE8T4SZOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1e881c-a1e4-4417-d21a-5b31f58be72d"
      },
      "source": [
        "from keras.datasets import mnist as db\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "reset=True\n",
        "\n",
        "np.random.seed(0)\n",
        "if reset:\n",
        "    model = Model()\n",
        "    model.add(Conv2D(16, (5, 5), stride=(2, 2), activation=Relu()))\n",
        "    model.add(Conv2D(32, (3, 3), activation=Relu()))\n",
        "    model.add(Dense(64, activation=Relu()))\n",
        "    model.add(Dense(10, activation=Softmax()))\n",
        "    model.build(x[0].shape, Adam(), CategoricalCrossentropy())\n",
        "\n",
        "(x, y), (x_test_ori, y_test_ori) = db.load_data()\n",
        "target = to_categorical(y)\n",
        "\n",
        "np.random.seed(293)\n",
        "model.fit(x/ 255, target, batch_size=32, epochs=4, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 4\n",
            "1500/1500 [===================>] loss: 0.06094920705960944 ; acc: 0.98075 ; val_loss: 0.0704177378765918 ; val_acc: 0.9780833333333333\n",
            "Epoch 2 / 4\n",
            "1500/1500 [===================>] loss: 0.040677762868495594 ; acc: 0.9875 ; val_loss: 0.06050314471876044 ; val_acc: 0.9828333333333333\n",
            "Epoch 3 / 4\n",
            "1500/1500 [===================>] loss: 0.026613825190110654 ; acc: 0.9917916666666666 ; val_loss: 0.04686432838715824 ; val_acc: 0.9864166666666667\n",
            "Epoch 4 / 4\n",
            "1500/1500 [===================>] loss: 0.01936644721573445 ; acc: 0.993875 ; val_loss: 0.04767690732682733 ; val_acc: 0.9863333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': [0.98075, 0.9875, 0.9917916666666666, 0.993875],\n",
              " 'loss': [0.06094920705960944,\n",
              "  0.040677762868495594,\n",
              "  0.026613825190110654,\n",
              "  0.01936644721573445],\n",
              " 'val_acc': [0.9780833333333333,\n",
              "  0.9828333333333333,\n",
              "  0.9864166666666667,\n",
              "  0.9863333333333333],\n",
              " 'val_loss': [0.0704177378765918,\n",
              "  0.06050314471876044,\n",
              "  0.04686432838715824,\n",
              "  0.04767690732682733]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}